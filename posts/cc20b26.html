<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="mask-icon" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Grand+Hotel:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=JetBrains+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":true,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="The essay reviews methods for analyzing sentiments tied to specific aspects or features of entities, such as product components. It introduces Aspect-Based Sentiment Classification (ABSC), a fine-grai">
<meta property="og:type" content="article">
<meta property="og:title" content="A Survey on Aspect-Based Sentiment Classification">
<meta property="og:url" content="http://example.com/posts/cc20b26.html">
<meta property="og:site_name" content="Joey">
<meta property="og:description" content="The essay reviews methods for analyzing sentiments tied to specific aspects or features of entities, such as product components. It introduces Aspect-Based Sentiment Classification (ABSC), a fine-grai">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-10-10T16:26:21.000Z">
<meta property="article:modified_time" content="2025-02-15T09:31:36.416Z">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="paper">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/posts/cc20b26.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/posts/cc20b26.html","path":"posts/cc20b26.html","title":"A Survey on Aspect-Based Sentiment Classification"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>A Survey on Aspect-Based Sentiment Classification | Joey</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Joey</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">6</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">44</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#a-survey-on-aspect-based-sentiment-classification"><span class="nav-text">A Survey on
Aspect-Based Sentiment Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#abstract"><span class="nav-text">Abstract</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#input-representation"><span class="nav-text">2. Input Representation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#context"><span class="nav-text">2.1 context</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dimensionality"><span class="nav-text">2.2 Dimensionality</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#feature-types"><span class="nav-text">2.3 Feature Types</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#performance-evaluation"><span class="nav-text">3. Performance Evaluation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sentiment-classification"><span class="nav-text">4. Sentiment Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#knowledge-based"><span class="nav-text">4.1 knowledge-based</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dictionary-based"><span class="nav-text">4.1.1 Dictionary-Based</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ontology-based"><span class="nav-text">4.1.2 Ontology-Based</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#discourse-based"><span class="nav-text">4.1.3 Discourse-Based</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#machine-learning"><span class="nav-text">4.2 Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#support-vector-machines"><span class="nav-text">4.2.1 Support Vector Machines</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tree-based"><span class="nav-text">4.2.2 Tree-Based</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deep-learning"><span class="nav-text">4.2.3 Deep Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-based-deep-learning"><span class="nav-text">4.2.4 Attention-Based Deep
Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hybrid"><span class="nav-text">4.3 Hybrid</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dictionary-enhanced-machine-learning"><span class="nav-text">4.3.1 Dictionary-Enhanced
Machine Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ontology-enhanced-machine-learning"><span class="nav-text">4.3.2 Ontology-Enhanced
Machine Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#discourse-enhanced-machine-learning"><span class="nav-text">4.3.3 Discourse-Enhanced
Machine Learning</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#related-topics"><span class="nav-text">5. RELATED TOPICS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#aspect-detection-aggregation"><span class="nav-text">5.1 Aspect Detection &amp;
Aggregation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sarcasm-thwarting"><span class="nav-text">5.2 Sarcasm &amp; Thwarting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#emotions"><span class="nav-text">5.3 Emotions</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#conclusion"><span class="nav-text">6. CONCLUSION</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joey"
      src="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <p class="site-author-name" itemprop="name">Joey</p>
  <div class="site-description" itemprop="description">A Humble Apprentice</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hlYWRtYXN0ZXJFZ2d5" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HeadmasterEggy"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmhlYWRtYXN0ZXJlZ2d5QGdtYWlsLmNvbQ==" title="E-Mail → mailto:headmastereggy@gmail.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly94LmNvbS9IZWFkbWFzdGVyRWdneQ==" title="X → https:&#x2F;&#x2F;x.com&#x2F;HeadmasterEggy"><i class="fab fa-x-twitter fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9pbnN0YWdyYW0uY29tL2hhYml0YXR1bmljb3Ju" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;habitatunicorn"><i class="fab fa-instagram fa-fw"></i></span>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/cc20b26.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
      <meta itemprop="name" content="Joey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joey">
      <meta itemprop="description" content="A Humble Apprentice">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="A Survey on Aspect-Based Sentiment Classification | Joey">
      <meta itemprop="description" content="The essay reviews methods for analyzing sentiments tied to specific aspects or features of entities, such as product components. It introduces Aspect-Based Sentiment Classification (ABSC), a fine-grained approach to sentiment analysis that focuses on identifying and classifying sentiments about specific aspects. The paper categorizes ABSC models into three groups: knowledge-based models, machine learning models (including SVMs and deep learning), and hybrid approaches that combine both.The essay also discusses key challenges, such as handling implicit aspects, processing sentences with multiple aspects, and dealing with complex language structures. Recent advances in deep learning and transformer models are highlighted as major contributors to improving performance in ABSC tasks. Finally, the essay points to future directions, suggesting a focus on better aspect detection, handling implicit aspects more effectively, and improving the scalability of ABSC models.">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Survey on Aspect-Based Sentiment Classification
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-10-10 16:26:21" itemprop="dateCreated datePublished" datetime="2024-10-10T16:26:21Z">2024-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-02-15 09:31:36" itemprop="dateModified" datetime="2025-02-15T09:31:36Z">2025-02-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>23k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>1:25</span>
    </span>
</div>

            <div class="post-description">The essay reviews methods for analyzing sentiments tied to specific aspects or features of entities, such as product components. It introduces Aspect-Based Sentiment Classification (ABSC), a fine-grained approach to sentiment analysis that focuses on identifying and classifying sentiments about specific aspects. The paper categorizes ABSC models into three groups: knowledge-based models, machine learning models (including SVMs and deep learning), and hybrid approaches that combine both.The essay also discusses key challenges, such as handling implicit aspects, processing sentences with multiple aspects, and dealing with complex language structures. Recent advances in deep learning and transformer models are highlighted as major contributors to improving performance in ABSC tasks. Finally, the essay points to future directions, suggesting a focus on better aspect detection, handling implicit aspects more effectively, and improving the scalability of ABSC models.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>#paper/aspectBased</p>
<h1 id="a-survey-on-aspect-based-sentiment-classification">A Survey on
Aspect-Based Sentiment Classification</h1>
<h2 id="abstract">Abstract</h2>
<p>随着互联网上评论和其他带有情感的信息文本数量的不断增长，自动情感分析算法的需求持续上升。基于方面的情感分类（Aspect-Based
Sentiment Classification,
ABSC）能够从文本文档或句子中自动提取极为细粒度的情感信息。这篇综述总结了ABSC研究的快速发展状态，并提出了一种新颖的分类法，将ABSC模型分为三大类：基于知识的方法、机器学习模型和混合模型。文中不仅提供了这些模型性能的总结概览，还对各种ABSC模型进行了技术性和直观性的解释。</p>
<p>本文讨论了当前最先进的ABSC模型，例如基于Transformer模型的模型和结合知识库的混合深度学习模型。此外，还回顾了各种模型输入表示与输出评估的技术。文章进一步探讨了ABSC研究的趋势，并就未来如何推进该领域发展提出了讨论。</p>
<h1 id="introduction">1. Introduction</h1>
<p>万维网为人们通过各种基于文本的渠道（如在线评论和社交媒体）在线分享意见提供了途径。能够利用这些意见准确评估人们对某一产品、人物或地点的看法，在许多行业中都具有极高的价值。餐厅可以根据在线食品评论调整菜单，公司可以根据消费者的具体需求改进产品，政治活动的效果也可以通过分析社交媒体帖子来评估。因此，随着互联网的兴起，情感分析这一任务变得越来越重要【149】。</p>
<p>情感分析是一项从文本中提取并分析人们对特定实体的情感的任务【118】。在文献中，情感分析有时也被称为“观点挖掘”。然而，需要区分“情感”（sentiments）和“观点”（opinions）。具体来说，“观点”表示某人对某一特定问题的看法，而“情感”则反映某人对某事物的感受。不过，这两个概念高度相关，通常可以通过“观点词”来提取情感【94,
95】。
在情感分析中，目标是根据文本内容分配情感极性。尽管“极性”（polarity）和“情感”（sentiment）这两个术语经常被互换使用，但实际上它们之间也可以进行区分：情感指代一种感受，而极性则表达一种方向性（例如，“正面”、“中性”或“负面”）。情感分析任务的粒度可以通过三个独立的特征来描述：情感类型、任务层次和任务目标。首先，情感输出的类型需要被明确。例如，该任务可能涉及简单的二元分类（“正面”或“负面”标签），但也可能包括其他标签（如“中性”），甚至需要预测情感的强度或分值。其次，任务层次决定了从文本中提取情感的层级。例如，在文档级别的情感分析中，情感是针对整个文档进行分析的；而在句子级别的情感分析中，情感是针对文档中的每一个句子进行分析的。此外，还有许多其他层级，例如单词、段落、句子组或文本片段。最后，任务目标决定了情感的焦点。某些情况下可能不存在情感目标，这意味着任务是为整个文本分配一个情感分数或标签。而在其他情况下，可能需要了解针对文本中特定主题、实体或方面的情感。例如，假设我们在分析一篇产品评论。如果未定义目标，我们仅考虑整篇文本的情感。然而，明确消费者对产品的哪些具体方面感到满意或不满可能会更有用，因为这提供了许多应用所需的详细信息【115】。在文本中识别这些方面并分析其情感的任务被称为基于方面的情感分析（Aspect-Based
Sentiment Analysis, ABSA）。</p>
<p>ABSA
是一个相对较新的研究领域，由于其广泛的应用价值而迅速获得了极高的关注【32】。然而，ABSA
和一般的情感分析任务一样，具有较高的难度，这主要源于句子结构和写作风格的多样性【174】。[143]
对 ABSA 中存在的问题和挑战进行了全面概述。与一般的情感分析任务类似，ABSA
可以在多个层次上执行，其中文档级和句子级通常是最受欢迎的【157】。文档级
ABSA
方法侧重于识别文档中与某一实体相关的一般方面，并为其分配情感。相比之下，句子级
ABSA
方法尝试逐句识别所有方面，确定这些方面相关的情感，并可能在评论层面聚合情感【157】。因此，文档级
ABSA 考虑的是总结文本情感的一般概念，而句子级 ABSA
则关注单独提及的各个方面。ABSA
的任务可以进一步分为三个子任务：<strong>方面检测/提取</strong>、<strong>情感分类</strong>和<strong>情感聚合</strong>【174】。方面提取的任务是识别文本中存在的方面；分类步骤是为提取的方面分配情感标签或评分；聚合步骤则是汇总方面的情感分类结果。在本次综述中，我们的重点是情感分类步骤，这通常被称为<strong>基于方面的情感分类（Aspect-Based
Sentiment Classification, ABSC）</strong>。</p>
<p>关于 ABSA 的各种综述已经发表【152, 174】。然而，专注于 ABSC
的综述似乎能够更有效地深入讨论和评估 ABSC 模型。目前唯一专门针对 ABSC
的综述是文献【230】，其提供了对深度学习技术的概览。尽管深度学习模型目前是
ABSC 的最先进方法，但我们认为，扩展研究范围可以更有效地评估 ABSC
研究的当前状态和未来发展。例如，ABSC
中的一个重要部分是将知识库引入分类模型的相关研究。此外，之前的综述中缺少了一些重要的深度学习模型，例如基于
Transformer 的模型【201】。</p>
<p>基于上述原因，本综述提供了关于当前最先进的 ABSC
模型的全面概览。为此，我们提出了一种文献中尚未出现的 ABSC
模型新分类法。该分类法将 ABSC
模型划分为三大类别：<strong>基于知识的方法</strong>、<strong>机器学习模型</strong>和<strong>混合模型</strong>。基于这一分类结构，我们使用技术性和直观性的解释讨论并比较了不同
ABSC 模型的架构。此外，我们还提供了比以往综述更大范围的 ABSC
模型性能总结概览。最后，我们识别了 ABSC
研究中的趋势，并利用这些发现讨论了推进该领域未来发展的可能途径。</p>
<p>本综述的章节安排基于设计 ABSC 方法时的主要步骤。</p>
<p>• <strong>第 2 章</strong> 讨论 ABSC 输入表示的不同方法。</p>
<p>• <strong>第 3 章</strong> 回顾了用于评估模型性能的技术。</p>
<p>• <strong>第 4 章</strong> 根据提出的分类法介绍了各种 ABSC
模型，讨论了这些模型如何利用预处理后的输入生成所需的分类输出，并比较了文献中不同模型的性能。</p>
<p>• <strong>第 5 章</strong> 探讨了与 ABSC 相关的其他主题。</p>
<p>• <strong>第 6 章</strong>
总结了主要内容，并讨论了未来研究的发展方向。</p>
<h1 id="input-representation">2. Input Representation</h1>
<p>在本章中，将详细解释 ABSC 所需的输入表示。我们首先根据 Schouten 和
Frasincar 的定义【174】引入一些定义。给定包含记录 <span
class="math inline">\(R_1, R_2, \dots, R_{n_R}\)</span> 的语料库 <span
class="math inline">\(C\)</span>，ABSA 可以被形式化为在每条记录 <span
class="math inline">\(R_j\)</span> 中找到所有四元组 <span
class="math inline">\((y, a, h, t)\)</span>【118】，其中 <span
class="math inline">\(y\)</span> 表示情感，<span
class="math inline">\(a\)</span> 表示情感的目标方面，<span
class="math inline">\(h\)</span> 表示情感的表达者，<span
class="math inline">\(t\)</span>
表示情感的表达时间【174】。一条记录被定义为语料库中的一段文本，可能是一个短语、一句话或一篇较大的文本，例如一篇文档。</p>
<p>通常，大多数方法关注的是查找 <span class="math inline">\((y,
a)\)</span>，即目标方面及其对应的情感。由于本综述仅关注 ABSA
的分类步骤，因此假设文本中目标方面 <span
class="math inline">\(a\)</span> 已经被识别。因此，ABSC
模型仅专注于寻找与给定方面 <span class="math inline">\(a\)</span>
对应的情感 <span
class="math inline">\(y\)</span>。例如，考虑一条形式为餐厅评论的记录：“The
atmosphere was fantastic, but the food was
bland.”这句话包含两个方面“atmosphere”和“food”，我们假设这些方面已在先前的方面提取阶段被识别。最终目标是利用
ABSC
模型为这些方面确定情感分类。在此示例中，正确的情感分别为“positive”和“negative”。</p>
<p>然而，在尝试这一任务之前，必须构建输入表示，因为文本通常不能直接用作分类模型的输入。方面及其对应的上下文必须通过数值特征进行表示。需要注意的是，在基于方面或特征的情感分析中，“特征”一词有时被用来描述例如产品的各个方面。然而，在此上下文中，“特征”指的并不是方面本身，而是数据的特征。在实现
ABSC 模型之前，需要一个预处理阶段来构建数值表示【70,
80】。用于输入的特征是分类过程中的关键部分，因为它们决定了 ABSC
模型可访问的信息量。在保证不包含冗余或无关特征的情况下，重要的是以一种尽可能多地保留信息的方式表示文本，以便模型可以最佳地执行。</p>
<p>ABSC
的输入表示通常由三个特性组成：<strong>上下文、维度性和特征类型</strong>。我们将在
2.1 节、2.2 节和 2.3 节分别讨论这些特性。</p>
<h1 id="context">2.1 context</h1>
<p>给定一条记录 <span
class="math inline">\(R_j\)</span>，我们将上下文定义为被视为输入的一部分单词子集。如果文本仅包含一个方面，则可以选择简单地考虑所有单词。然而，如果记录中包含多个方面，则需要为每个方面单独开发一种表示方法，例如，为每个方面提取一部分单词。</p>
<p>输入表示方法根据文本中是否存在目标短语（Target
Phrase）而有所不同。因此，我们将输入表示技术分为<strong>显式方面</strong>和<strong>隐式方面</strong>两种情况。</p>
<p><strong>显式方面(Explicit)</strong></p>
<p>显式方面通常是最常见的类型。例如，在评论句子“The price of this phone
is very
high.”中，方面被明确表述为“price”。确定这种显式方面的上下文最简单的方法是围绕目标短语使用一个窗口，仅基于窗口中的单词构建输入表示。例如，Guha
等人【77】仅考虑方面本身、方面左侧的三个单词以及右侧的三个单词。然而，这种基于物理邻近性的简单方法可能并不理想，因为表达情感的单词可能距离方面较远。因此，更健壮的方法不会依赖单词之间的物理距离。例如，可以使用语法依赖关系（Grammatical
Dependencies）来确定记录中与方面相关的单词，并将这些单词纳入上下文【193】。另一种方法是使用文本核（Text
Kernels），通过单词之间的关系表达距离。例如，Nguyen 和
Shirai【144】使用树核（Tree
Kernels）进行关系提取，以确定在分析中考虑哪些单词。</p>
<p><strong>隐式方面(implicit)</strong></p>
<p>隐式方面的一个例子是“This phone is really
expensive.”，其中方面仍是“price”，但方面未在文本中直接提及。由于没有目标短语来定义窗口中心或确定单词的距离，隐式方面的处理需要不同的方法。如果假设记录中仅包含一个方面，例如上述例子，则可以简单地将整个句子视为输入表示。然而，正如
Dosoula
等人【52】所述，评论中通常包含多个隐式方面，甚至可能出现在同一句中。例如，之前的例子可以扩展为：“This
phone is really expensive, but also very
fast.”。此时，句中有两个隐式方面。Dosoula
等人【52】提出了基于方面代理词（Aspect Proxy
Words）的不同方法来确定句子中每个方面的上下文。</p>
<h1 id="dimensionality">2.2 Dimensionality</h1>
<p>输入表示的维度性直接由后续情感分类步骤中使用的模型类型决定。一些模型（例如支持向量机和决策树）只能处理单一向量表示，而其他模型（例如循环神经网络）则可以处理一组向量或数据矩阵。</p>
<p>假设记录 <span class="math inline">\(R_j\)</span>
包含一个方面，我们希望用数值特征表示整个记录。根据所使用的 ABSC
模型，记录 <span class="math inline">\(R_j\)</span> 可以表示为单一向量
<span class="math inline">\(\mathbf{x}_j \in
\mathbb{R}^{d_x}\)</span>，或者表示为矩阵 <span
class="math inline">\(\mathbf{X}_j \in \mathbb{R}^{d_x \times
n_j}\)</span>，其中 <span class="math inline">\(d_x\)</span>
表示用于表示的特征数，<span class="math inline">\(n_j\)</span> 表示记录
<span class="math inline">\(R_j\)</span> 中单词的数量。</p>
<p>•
<strong>单一向量表示</strong>：将记录表示为一个嵌入向量，其中每个元素表示一个特征的存在，例如是否存在特定的情感词或短语。</p>
<p>•
<strong>数据矩阵表示</strong>：提供更详细的信息，因为它无需将所有信息总结为一个向量。矩阵的每一列是一个嵌入向量，表示记录的一部分，例如一个句子、一个单词或一个字符。</p>
<p>输入的维度性通常会影响所使用的特征类型，因为某些特征类型更适合向量表示，而另一些则可以更有效地用于矩阵表示。下一节将对此进行更详细的讨论。</p>
<h1 id="feature-types">2.3 Feature Types</h1>
<p>在将文本表示为单一向量时，可以使用传统的文本分类特征类型。其中最简单且最常用的方法之一是<strong>词袋模型</strong>（Bag-of-Words,
BoW）表示。BoW
表示是一种特征向量，其中每个元素代表一个单词。这种表示被称为“袋子”，是因为我们简单地将单词汇集在一起，而忽略了文本的结构。例如，假设我们希望使用单一向量
<span class="math inline">\({x}_j\)</span> 表示记录 <span
class="math inline">\(R_j\)</span>，首先需要构建一个词汇表，该词汇表包含语料库
<span class="math inline">\(C\)</span>
中每条记录中所有使用过的唯一单词。对于 BoW 表示，向量 <span
class="math inline">\(\mathbf{x}_j\)</span> 中的每个元素 <span
class="math inline">\(\mathbf{x_{i,j}}\)</span>
对应于词汇表中的一个唯一单词。最简单的 BoW
向量是一个二值向量，其中，如果对应的单词出现在 <span
class="math inline">\(R_j\)</span> 中，则 <span
class="math inline">\(\mathbf{x_{i,j}}\)</span>，否则 <span
class="math inline">\(\mathbf{x_{i,j}} = 0\)</span>。</p>
<p>一种更常用的版本是考虑单词的频率，通过将向量 <span
class="math inline">\(\mathbf{x}_j\)</span> 中的每个元素 <span
class="math inline">\(\mathbf{x_{i,j}}\)</span> 设置为对应单词在 <span
class="math inline">\(R_j\)</span>
中出现的次数【169】。然而，一些单词的使用频率本身较高，这可能导致单词计数中对某些单词的偏倚。因此，可以选择使用<strong>词频-逆文档频率</strong>（Term
Frequency-Inverse Document Frequency,
TF-IDF）方法【103】来替代直接的单词计数。该方法本质上是记录中单词的频率与其在所有记录中出现频率的缩放比例。对于词汇表中的单词
<span class="math inline">\(w_i\)</span>，TF-IDF
可通过以下公式计算：</p>
<p><span class="math display">\[x_{i,j} = \frac{n_{i,j}}{n_j} \times
\log \frac{n_R}{|\{R_j \in C : w_i \in R_j\}|}\]</span> 其中：</p>
<p>• <span class="math inline">\(n_{i,j}\)</span> 表示单词 <span
class="math inline">\(w_i\)</span> 在记录 <span
class="math inline">\(R_j\)</span> 中出现的次数，</p>
<p>• <span class="math inline">\(n_j\)</span> 表示记录 <span
class="math inline">\(R_j\)</span> 中的单词总数，</p>
<p>• <span class="math inline">\(n_R\)</span> 表示记录的总数，</p>
<p>• <span class="math inline">\(C\)</span> 表示记录的集合。</p>
<p>正如前文所述，在 ABSC
中，当记录中存在多个方面时，最好不要使用单一的文本表示。因此，可以仅基于被视为上下文中的单词创建一个词袋（BoW）表示。</p>
<p>BoW 特征在 ABSC
中表现良好，但可能会引发一些使分类过程变得困难的问题。首先，大型语料库可能包含大量不同的单词，这意味着如果每个单词都用一个元素表示，词汇表及其对应的特征向量将会非常庞大。因此，开发了一些方法来选择可用作特征的最重要单词。例如，可以基于列表或词典过滤掉通常不具有实际意义或语义价值的单词，如停用词。尽管这些技术可以显著减少特征数量，但向量通常仍然较大。因此，ABSC
模型的一个重要特性是需要能够很好地处理高维向量，这将在第 4
章中更详细地讨论。</p>
<p>其次，如前文所述，BoW
完全忽略了单词的结构，使得难以捕捉方面与其上下文之间的关系。一种解决方法是引入
n-grams，但这会进一步加剧前述的高维性问题。另一种解决方案是包括定义单词之间关系的特征。通常会使用其他形式的特征类型，以包含不同类型的信息。例如，Al-Smadi
等人【6】在 TF-IDF BoW
表示的基础上增强了形态学、句法和语义特征。类似地，Mullen 和
Collier【141】实现了语义、句法和基于邻近性的特征。表 1
概述了各种特征类型及其对应的示例。</p>
<table>
<colgroup>
<col style="width: 31%" />
<col style="width: 68%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>特征类型 (Feature Types)</strong></th>
<th><strong>特征示例 (Feature Examples)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>单词特征 (Word Features)</strong></td>
<td>词袋（Bag-of-Words）【177】，词嵌入（Word
Embeddings）【132】，n-grams【148】</td>
</tr>
<tr class="even">
<td><strong>句法特征 (Syntactic Features)</strong></td>
<td>词性标注（Part-of-Speech Tagging）【54, 162】，依存分析（Dependency
Parsing）【54, 111, 140, 173】</td>
</tr>
<tr class="odd">
<td><strong>基于邻近的特征 (Proximity-Based Features)</strong></td>
<td>相对于情感词【82, 141】或目标词【100】的邻近性</td>
</tr>
<tr class="even">
<td><strong>语义特征 (Semantic Features)</strong></td>
<td>上下文词的情感得分（Sentiment Scores of Context Words）【83,
198】</td>
</tr>
<tr class="odd">
<td><strong>形态学特征 (Morphological Features)</strong></td>
<td>词元（Lexemes）和词形（Lemmas）【1, 6, 173】</td>
</tr>
</tbody>
</table>
<p>假设我们现在希望用矩阵 <span
class="math inline">\(\mathbf{X}_j\)</span> 表示记录 <span
class="math inline">\(R_j\)</span>。在这种情况下，想法是将文本中的每个单词表示为一个向量，并将其存储为
<span class="math inline">\(\mathbf{X}_j\)</span>
中的一列。一种标准选择是使用词嵌入。关于各种类型的词嵌入，可以参考文献【27】。一些示例包括：GloVe【153】、fastText【73】和
Word2Vec【133】。这些嵌入类型都尝试使用有限大小的向量表示单词的意义。</p>
<p>虽然经常使用预训练的词嵌入，但也可以在 ABSC
模型训练过程中同时训练词嵌入。此外，上述嵌入模型都是非上下文的，这意味着每个单词的嵌入是独立于其他单词生成的。然而，众所周知，一个单词的意义可能会根据其使用的上下文而变化。因此，另一种解决方案是使用上下文相关的词嵌入，例如基于
ELMo【154】或
BERT【48】的嵌入。这些词嵌入模型会根据周围上下文为单词生成不同的向量。例如，单词“playing”在“playing
tennis”和“playing piano”中的含义不同，因此会被分配不同的词嵌入。</p>
<p>此外，还有专门为情感分析设计的词嵌入【189】。尽管词嵌入本身是一个强大的表示工具，但仍可以添加额外的特征类型，例如表
1
中的示例。例如，文献【7】将词嵌入与句法和语义特征结合使用。本节讨论的输入表示是第
4
章中分类模型的基础。在本文的其余部分，我们假设输入表示已经预先构建完毕。仅当分类模型对输入表示进行了修改时，我们才会进一步阐述输入表示。此外，为了清晰起见，本文其余部分不使用特定于记录或方面的下标。</p>
<h1 id="performance-evaluation">3. Performance Evaluation</h1>
<p>ABSC 模型的目标是利用输入表示生成输出。因此，给定包含方面 <span
class="math inline">\(a\)</span> 的记录 <span
class="math inline">\(R\)</span>，ABSC 模型使用特征向量 <span
class="math inline">\(\mathbf{x} \in \mathbb{R}^{d_x}\)</span> 或矩阵
<span class="math inline">\(\mathbf{X} \in \mathbb{R}^{d_x \times
n_x}\)</span> 生成一个标签 <span class="math inline">\(\hat{y} \in
\mathbb{R}^1\)</span>，其中 <span class="math inline">\(d_x\)</span>
表示使用的特征数量，<span class="math inline">\(n_x\)</span>
表示被视为上下文的单词数量。</p>
<p>可以通过评估输出 <span class="math inline">\(\hat{y}\)</span>
使用多种性能指标来比较 ABSC
模型的有效性。这些指标突出显示了分类模型的某些优点和缺点。本节介绍了用于评估
ABSC 模型的各种技术。这些指标将在第 4 章中用于比较和对比不同的 ABSC
方法。</p>
<p>ABSC 和 ABSA
中最常用的性能指标包括众所周知的准确率（Accuracy）、精确率（Precision）、召回率（Recall）和
F1 值【174】。这些指标通过将情感分类 <span
class="math inline">\(\hat{y}\)</span> 与方面的真实情感标签 <span
class="math inline">\(y\)</span>
进行比较来进行评估。达到高值的模型将提供更高质量的预测。</p>
<p>•
<strong>准确率（Accuracy）</strong>：衡量正确分类的方面数量与数据集中方面总数的比率。这一指标直观清晰，是对模型预测能力的直接反映。在二分类场景中，一个简单的基线是随机分类器，其期望准确率为
<span
class="math inline">\(0.5\)</span>，任何预测模型都应优于这一水平。然而，在不平衡数据集中，准确率往往不是有效的性能指示器。如果一个数据集包含两个类别，其中
90% 的记录属于一个类别，则始终预测该类别可以得到高达 <span
class="math inline">\(0.9\)</span> 的准确率。</p>
<p>• <strong>精确率和召回率（Precision and
Recall）</strong>：为了解决不平衡数据集中的问题，使用了其他指标如精确率和召回率，这些指标提供了更有意义的分类性能评估。可以通过计算每个类别的精确率和召回率的均值（宏平均，Macro-Averaging）或根据所有类别的贡献进行聚合（微平均，Micro-Averaging）来获得模型性能的总体度量。</p>
<p>• <strong>F1
值（F1-Measure）</strong>：由于精确率和召回率关注模型性能的不同方面，F1
值通常用于总结这些信息。</p>
<p>类似的方法可以用于聚合来自多个不同数据集的结果。可以计算不同数据集性能指标的均值（宏平均），或者基于数据集的贡献进行聚合（微平均）。</p>
<p>本文讨论的大多数研究都使用了准确率（Accuracy）、精确率（Precision）、召回率（Recall）和
F1 值。然而，偶尔也会使用一些替代指标，例如均方误差（Mean Squared Error,
MSE）和排序损失（Ranking Loss）。</p>
<p>假设我们有一个包含 <span class="math inline">\(N\)</span> 个特征向量
<span class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_N\)</span>
的训练数据集，以及其对应的标签 <span class="math inline">\(y_1, \dots,
y_N\)</span>。每个特征向量 <span class="math inline">\(\mathbf{x}_1,
\dots, \mathbf{x}_N\)</span> 表示记录中的一个方面及其相应的上下文（如第
2 章所述）。对应的标签 <span class="math inline">\(y_1, \dots,
y_N\)</span> 表示针对这些方面表达的真实情感。通过 ABSC
模型，可以生成标签预测 <span class="math inline">\(\hat{y}_1, \dots,
\hat{y}_N\)</span>。</p>
<p>情感分析的标签通常被视为有序数据，均方误差（MSE）可按以下公式计算：</p>
<p><span class="math display">\[\text{MSE} = \frac{1}{N} \sum_{i=1}^N
(\hat{y}_i - y_i)^2\]</span></p>
<p>由于平方的存在，MSE
对大误差的惩罚比小误差更大。一种替代方法是排序损失（Ranking
Loss）【40】，它以更均等的方式惩罚大误差和小误差。排序损失与平均绝对误差密切相关，其公式如下：</p>
<p><span class="math display">\[\text{Ranking Loss} = \frac{1}{N}
\sum_{i=1}^N |\hat{y}_i - y_i|\]</span></p>
<p>排序损失和 MSE
都用于衡量分类预测中的误差。因此，对于这些指标，值越低，模型性能越好。</p>
<p>另一种性能指标是接收者操作特性（ROC）曲线下面积，简称为 AUC。ROC
曲线绘制了召回率与真正率的关系。曲线下面积（AUC）反映了模型区分类别的能力，AUC
值越高，模型性能越好。</p>
<h1 id="sentiment-classification">4. Sentiment Classification</h1>
<p>ABSC
模型通常可以分为三大类：<strong>基于知识的方法</strong>、<strong>机器学习模型</strong>和<strong>混合模型</strong>。图
1 展示了包含这些类别及其子类别的分类法。这些分类的详细解释分别见
4.1、4.2 和 4.3 小节。每个小节中都包含对该类别主要 ABSC
模型的概述，并以总结表格的形式呈现。</p>
<p>每个表格包含以下列信息：</p>
<p>• 模型名称；</p>
<p>• 使用的数据类型；</p>
<p>• 每个模型报告的各种性能指标。</p>
<p>对于使用来自不同领域的数据集的模型，每一行可能包含多个条目。此外，我们还报告了其他研究中使用不同数据集对同一模型的实现结果。当模型在另一篇论文中被重新实现时，我们会在条目下方标明这一点。对于不可用的信息，以“-”
表示。</p>
<p>由于无法包含每篇论文报告的所有结果，我们采取了一些步骤对结果进行总结：</p>
<ol type="1">
<li><p>仅包含每篇论文中提出的最佳模型架构的结果；</p></li>
<li><p>当在同一领域中使用多个数据集时，我们取这些数据集的平均结果。唯一的例外是当模型在另一篇论文中被重新实现时，因为无法保证模型的实现完全相同。</p></li>
</ol>
<p>为了便于模型比较，表格中列出了所有数据集的参考文献。一些 ABSC
数据集的一般特性见表
2。需要注意的是，此表仅包括列出数据集中最常用的领域和语言。例如，SemEval-2016
数据集包含许多其他语言，如荷兰语、中文和土耳其语。</p>
<h2 id="knowledge-based">4.1 knowledge-based</h2>
<p>基于知识的方法（亦称符号人工智能）是利用知识库的技术。知识库通常被定义为包含信息的存储系统，并伴随有一组规则、关系和假设，计算机系统可以据此推理。基于知识的方法与输入表示密切相关，因为这些方法通常使用知识库来定义特征。</p>
<p>基于知识的方法的一个优势是其可解释性。具体而言，通常可以轻松识别用于生成模型输出的信息。基于知识的方法的底层机制通常相对简单，这使得
ABSC
的方法具有高度透明性。这些方法无需训练时间，但知识库的构建可能需要耗费大量时间。</p>
<p>我们讨论以下三种基于知识的方法：<strong>基于词典的方法</strong>、<strong>基于本体的方法</strong>和<strong>基于语篇的方法</strong>。各种基于知识的方法的性能见表
3。</p>
<p>![[图 1. ABSC 方法的分类体系。.png]] Table 2. Overview of datasets
used for ABSC. ![[Table 2. Overview of datasets used for ABSC..png]]</p>
<h3 id="dictionary-based">4.1.1 Dictionary-Based</h3>
<p>早期的 ABSC 方法大多基于词典。给定包含一个方面 <span
class="math inline">\(a\)</span> 的记录 <span
class="math inline">\(R\)</span>，基于词典的方法使用词典构建一个特征向量
<span class="math inline">\(\mathbf{x}\)</span>，其中每个元素 <span
class="math inline">\(x_i\)</span>
表示上下文中某个单词相对于该方面的情感得分或方向性。ABSC
可以使用多种词典，例如 WordNet【134】和
SentiWordNet【14】。这些词典定义了单词集合及其之间的语言关系。例如，WordNet【134】将名词、动词、形容词和副词分组为所谓的“同义集”（synsets），即同义词集合或具有相同意义的单词组。这些关系可以用于
ABSC，因为单词通常表现出与其同义词相同的情感【94】。</p>
<p>基于此方法，首先需要一组已知情感的种子单词。例如，可以使用“good”、“fantastic”和“perfect”作为正面种子单词，使用“bad”、“boring”和“ugly”作为负面种子单词。然后，可以利用词典（如
WordNet【134】）中定义的同义集，基于种子单词确定围绕某一方面的单词情感。与正面种子单词同义或与负面种子单词反义的单词将获得正面的情感得分。SentiWordNet【14】部分基于这种思想。该词典通过结合种子集扩展方法和多种分类模型，为每个同义集生成情感标签（“正面”、“中性”或“负面”）。</p>
<p>![[Table 3. Overview of prominent knowledge-based ABSC models and
their reported performances..png]]
在为上下文单词确定情感得分后，需要实施一种方法来生成情感输出。例如，文献【94】的作者检查上下文中是否主要包含负面或正面单词。在【94】中，上下文单词的情感得分被编码为：正面为
1，负面为 -1，中性为 0。然后，通过对特征向量 <span
class="math inline">\(\mathbf{x}\)</span>
的各元素求和来确定情感分类。如果总和为正，则返回正面标签；否则返回负面标签。</p>
<p>类似地，在文献【58】中，上下文单词被分配到 [-4, 4]
范围内的情感得分。然后，根据与方面相关的意见单词的平均得分来确定该方面的情感极性。</p>
<h3 id="ontology-based">4.1.2 Ontology-Based</h3>
<p>基于本体的方法。本体通常被定义为“共享概念化的显式、可机器读取的规范”【74,
182】，其定义了一组对应于实体属性的实体和关系。本体与词典的主要区别在于，词典捕获的是单词之间的语言关系，而本体表示的是实际实体之间的关系。这些关系可以用于确定记录中哪些单词对确定方面的情感是重要的。</p>
<p>在 ABSC
中，可以使用现有的本体，也可以根据当前领域创建一个新的本体【108】。现有本体的示例包括语义互联网络社区（SIOC）本体【181】，该本体捕获了来自在线社区网站的数据，以及文献【165】中提出的情感本体。然而，与依赖现有本体相比，大多数研究者更倾向于创建自己的本体。这是因为很难找到一个与特定领域密切相关的现有本体，因为许多现有本体通常仅捕获通用概念。</p>
<p>本体设计通常使用特定方法，例如形式概念分析（Formal Concept
Analysis）【147】或 OntoClean
方法论【76】。为了确保捕获关系的准确性，本体通常是手动创建的【131】。然而，这一过程非常耗时。因此，在创建大型本体时，半自动甚至完全自动化的本体创建方法变得至关重要。例如，文献【38】中提出了一种语义资产管理工作台（SAMW），用于半自动创建
ABSC 的本体。而文献【8】则提出了一种完全自动化创建本体的方法。</p>
<p>本体捕获了某一领域中对象的结构。这些关系可用于确定与某一方面相关的上下文【108】。然后，可以利用基于本体获取的上下文通过某种方法确定情感标签，例如使用基于词典的情感分类器【231】。</p>
<p>尽管常规本体是定义对象之间关系的有用工具，但为了进一步支持情感分析，可以将情感信息整合到本体中。这类情感本体专门定义单词或实体之间的情感关系。例如，在文献【146】中，构建了一种情感本体树，将产品方面与对应情感得分的意见词连接起来。在文献【215】中，利用
SentiWordNet 词典【14】定义情感关系构建了情感本体。Zhuang
等人【235】提出了一种针对 ABSA
的半自动本体构建方法，根据情感关系构建本体。</p>
<p>在对某一方面进行分类时，可以将上下文中的单词链接到情感本体中的概念和关系，并总结情感关系以生成情感分类。文献【235】提出的半自动方法主要关注单词频率，因为领域数据量有限。在文献【46】中，提出了另一种基于
WordNet【134】词典的同义集（synsets）的半自动本体构建方法。最后，ten
Haaf 等人【190】提出了一种基于
word2vec【133】方法生成的词嵌入的半自动本体构建方法。</p>
<h3 id="discourse-based">4.1.3 Discourse-Based</h3>
<p><strong>基于语篇的方法</strong>。另一种可用于 ABSC
的知识库是基于<strong>修辞结构理论</strong>（Rhetorical Structure
Theory, RST）构建的语篇树【128】。RST
可用于在记录中定义一个层次化的语篇结构，将短语分类为基本语篇单元（Elementary
Discourse Units,
EDU）。与本体类似，语篇树定义了一组关系，这些关系可用于确定在为某一方面分配情感分类时哪些单词是重要的。</p>
<p>对于记录 <span
class="math inline">\(R\)</span>，可以构建一个语篇树以定义记录中的层次化语篇关系。为了确定记录中某一方面
<span class="math inline">\(a\)</span>
的上下文，文献【91】的作者提出了一种基于 RST
的方法，该方法生成一棵语篇树的子树，包含与方面直接相关的语篇关系。这棵子树被称为<strong>上下文树</strong>（context
tree），可以用来确定该方面的情感分类。</p>
<p>由于上下文树本身不包含任何情感信息，文献【91】中的作者使用了一种基于词典的方法，为上下文树的叶节点分配句子级
ABSC
的情感方向得分。然后，可以通过求上下文树中定义的情感得分的总和来确定方面的情感分类。在文献【170】中，采用了类似的技术，但使用了更复杂的得分聚合方法。</p>
<p>除了 RST
之外，还有其他类型的语篇结构理论【93】。例如，<strong>跨文档结构理论</strong>（Cross-Document
Structure Theory,
CST）【160】可用于分析文档组之间的结构和语篇关系。跨文档结构分析在社交媒体分析中非常有用，因为社交媒体帖子通常是短文档，且彼此高度关联。一个有趣的示例是
SMACk
系统【53】，该系统基于抽象论证理论【56】分析跨文档结构。例如，某一产品可能会收到来自不同用户的多条评论，这些评论可能相互回应。SMACk
系统分析评论中提出的不同论点之间的关系，从而改进针对不同方面表达的情感聚合【54】。</p>
<h2 id="machine-learning">4.2 Machine Learning</h2>
<p>与利用知识库实现情感分类的基于知识的方法不同，机器学习模型（亦称为次符号人工智能，subsymbolic
AI）使用包含特征向量及其对应正确标签的训练数据集。机器学习模型通过训练从数据中提取可用于区分情感类别的模式。机器学习模型种类繁多，可以分为以下几类：<strong>支持向量机（Support
Vector Machines）</strong>、<strong>基于树的模型（Tree-Based
Models）</strong>、<strong>深度学习模型（Deep Learning
Models）以及基于注意力的深度学习模型（Attention-Based Deep Learning
Models）</strong>。各种机器学习模型的性能表现见表 4。</p>
<p>![[Table 4. Overview of prominent machine learning ABSC models and
their reported performances..png]]</p>
<h3 id="support-vector-machines">4.2.1 Support Vector Machines</h3>
<p>支持向量机（SVM）模型【39】长期以来一直是情感分析和 ABSC
的热门选择【7, 141, 150, 151, 200】。SVM
模型通过构造一个超平面，将属于不同类别的数据向量分离开来，从而实现分类【26,
39, 200】。在 ABSC 的场景中，这相当于基于特征向量 <span
class="math inline">\(\mathbf{x}\)</span>
将方面分为情感类别（“正面”、“中性”和“负面”）。</p>
<p>假设我们有一个包含 <span class="math inline">\(N\)</span> 个特征向量
<span class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_N\)</span>
的训练数据集，以及其对应的标签 <span class="math inline">\(y_1, \dots,
y_N\)</span>。每个特征向量 <span class="math inline">\(\mathbf{x}_1,
\dots, \mathbf{x}_N\)</span> 表示记录中的一个方面及其上下文（如第 2
章所述）。对应的标签 <span class="math inline">\(y_1, \dots,
y_N\)</span> 表示针对这些方面表达的真实情感。</p>
<p>我们首先考虑只有两个情感类别的情况：“正面”和“负面”。对于表达正面情感的方面，其标签被编码为
1；对于表达负面情感的方面，其标签被编码为 -1。SVM
分类器可以总结为以下公式：</p>
<p><span class="math display">\[\hat{y} = \text{sign}(\mathbf{w}^T \cdot
\phi(\mathbf{x}) + b)\]</span></p>
<p>其中：</p>
<p>• <span class="math inline">\(\mathbf{w}\)</span> 表示权重向量，</p>
<p>• <span class="math inline">\(\phi(\mathbf{x})\)</span>
表示特征映射函数，</p>
<p>• <span class="math inline">\(b\)</span> 表示偏置项，</p>
<p>• <span class="math inline">\(\hat{y}\)</span> 表示预测标签。</p>
<p>其中，<span class="math inline">\(\mathbf{w} \in
\mathbb{R}^{d_x}\)</span> 是学习得到的权重向量，<span
class="math inline">\(b \in \mathbb{R}^1\)</span>
是学习得到的偏置常数。权重的确定是通过构造一个超平面来完成的，该超平面最大化了训练数据标签
<span class="math inline">\(y_1, \dots, y_N\)</span> 和特征向量 <span
class="math inline">\(\mathbf{x}_1, \dots, \mathbf{x}_N\)</span>
之间的分离。虽然一些数据集可以通过线性函数形式分离，但其他问题可能无法如此轻松地解决。在这种情况下，可以使用核函数
<span class="math inline">\(\phi()\)</span> 将特征向量 <span
class="math inline">\(\mathbf{x}\)</span>
转换到更高维空间，从而使标签更容易分离【171】。然而，使用核函数的一个缺点是，由于非线性，学习到的系数变得难以解释。当需要将某一方面分类为多个情感类别时，必须调整
SVM 模型。例如，可以使用“一对多”实现（one-versus-all
implementation），即训练一个 SVM
模型来将每个类别与所有其他类别分离。最终的预测基于具有最高值的决策函数。SVM
以良好的泛化能力和对噪声数据的鲁棒性而闻名【217】。此外，如第 2
章所述，ABSC 的特征向量通常包含大量特征，而 SVM
模型被认为能够很好地处理这些特征【102】。然而，找到适合的核函数通常是一项困难的任务【26】，并且需要手工设计特征以使模型表现良好【7】。</p>
<h3 id="tree-based">4.2.2 Tree-Based</h3>
<p><strong>基于树的方法</strong>。基于树的方法是以可训练决策树模型【23】为基础的技术。决策树模型由类似树的结构组成，其中每个内部节点（或决策节点）表示基于特定特征
<span class="math inline">\(\mathbf{x}\)</span>
的条件检查，而每个叶节点表示特定的情感类别。给定特征向量 <span
class="math inline">\(\mathbf{x}\)</span>，从树的根节点开始，检查分裂条件。条件和
<span class="math inline">\(\mathbf{x}\)</span>
中的相应特征决定了下一步移动到哪个内部节点。沿树向下移动直到到达叶节点，该叶节点分配有特定的情感类别，决定了预测
<span class="math inline">\(\hat{y}\)</span>。</p>
<p>决策树的一个显著优势是其可解释性。其决策规则通常易于人类理解，因此可以用于发现知识并获得新见解【139】。这些新见解还可以用于改进之前讨论的知识库。</p>
<p>虽然决策树模型在 ABSC
中并不特别流行，但仍有一些成功的应用示例。例如，文献【86】提出了一种增量决策树模型，在性能上优于之前讨论的
SVM 模型的实现。同样，文献【2】表明决策树在多个数据集上的表现优于包括
SVM 在内的其他模型。然而，对于其他问题，SVM
模型可能会优于决策树模型【6】。</p>
<p>决策树的主要问题是过拟合，这对于 ABSC
来说尤其突出，因为通常会使用大量特征。解决此问题的一种方法是使用随机森林模型，这是一种决策树的集成模型【22】。随机森林由大量的决策树模型组成。每棵树接收有限数量的特征和训练数据的自助采样（bootstrapped
sample）来进行训练。通过随机采样数据和限制特征，单个决策树在特定特征或数据上的过拟合倾向较低。预测是通过聚合所有单个决策树模型的预测结果并进行多数投票来实现的。</p>
<p>文献【79】在 SemEval-2014 ABSC
任务中实现了随机森林模型，但在不同数据集上的结果参差不齐。类似的结果也见于文献【191】。其他基于树的方法的示例包括梯度提升树（Gradient
Boosted Trees）【65】和极限树（Extra
Trees）分类器【69】。文献【20】的比较研究表明，这些模型相比随机森林可能会提供轻微的性能改进。</p>
<h3 id="deep-learning">4.2.3 Deep Learning</h3>
<p>深度学习模型【72】革新了许多研究领域【178】，包括情感分析和 ABSC【50,
230】。大量研究致力于为各种类型的数据和学习任务开发深度学习模型。</p>
<p>深度学习模型的主要缺点之一是其高度难以解释。基本的机器学习模型，例如决策树和线性
SVM，可以提供一些有用的模型解释。然而，尽管已经有尝试解释深度学习模型预测的方法【55,
78】，这些黑箱方法在实践中仍被认为难以解释。此外，有效训练深度学习模型需要大量的计算资源。这是因为深度学习模型需要大量数据进行训练，而这些数据在
ABSC
领域并不总是可用。然而，随着深度学习研究的发展和公开数据量的增加【157–159】，深度学习模型因其卓越的预测性能而越来越受欢迎。</p>
<p>用于 ABSC 的深度学习模型包括但不限于：<strong>循环神经网络（Recurrent
Neural Networks, RNN）</strong>、<strong>递归神经网络（Recursive Neural
Networks）和卷积神经网络（Convolutional Neural Networks,
CNN）</strong>。</p>
<p>![[Fig. 2. An illustration of a basic RNN model for ABSC..png]]</p>
<p><strong>循环神经网络（Recurrent Neural Network, RNN）</strong>
模型【92】近年来已成为 ABSC 模型最受欢迎的选择之一。RNN
模型是学习基于序列数据的强大工具【117】。它们在许多基于语言的学习任务中（包括
ABSC 以及各种其他基于序列的任务）取得了显著成果。</p>
<p>RNN 的基本模型如Fig 2 所示。在语言处理任务中，RNN
模型的核心思想是将单词序列依次输入到神经网络中。神经网络基于一个单词生成的隐藏状态被用作下一步的输入，从而使信息在序列中传递。相同的概念也可以用于处理图像序列、时间序列或其他类型的序列数据。</p>
<p>我们再次考虑一个包含方面 <span class="math inline">\(a\)</span>
和标签 <span class="math inline">\(y\)</span> 的记录 <span
class="math inline">\(R\)</span>。然而，与之前讨论的分类方法相比，我们现在假设数值特征以矩阵
<span class="math inline">\(\mathbf{X} \in \mathbb{R}^{d_x \times
n_x}\)</span> 表示，其中 <span class="math inline">\(d_x\)</span>
表示使用的特征数量，<span class="math inline">\(n_x\)</span>
表示上下文中被考虑的单词数量。对于任何任务，给定输入矩阵 <span
class="math inline">\(\mathbf{X}\)</span>，通用的 RNN
模型可以定义为：</p>
<p><span class="math display">\[\mathbf{h}_t = f(\mathbf{h}_{t-1},
\mathbf{x}_t)\]</span></p>
<p>其中：</p>
<p>• <span class="math inline">\(\mathbf{h}_t \in \mathbb{R}^{d_h \times
1}\)</span> 表示时间步 <span class="math inline">\(t\)</span>
的隐藏状态，</p>
<p>• <span class="math inline">\(\mathbf{x}_t \in \mathbb{R}^{d_x \times
1}\)</span> 表示时间步 <span class="math inline">\(t\)</span>
的输入向量，</p>
<p>• <span class="math inline">\(f\)</span>
表示用于更新隐藏状态的函数。</p>
<p>其中，<span class="math inline">\(\mathbf{h}_t \in
\mathbb{R}^{d_h}\)</span> 是步骤 <span class="math inline">\(t\)</span>
的隐藏状态向量，<span class="math inline">\(d_h\)</span>
是隐藏状态向量的预定义维度；<span class="math inline">\(\mathbf{x}_t \in
\mathbb{R}^{d_x}\)</span> 是矩阵 <span
class="math inline">\(\mathbf{X}\)</span> 的第 <span
class="math inline">\(t\)</span> 列，对于 <span class="math inline">\(t
= 1, \dots, n_x\)</span>。在最基本的 RNN 形式中，函数 <span
class="math inline">\(f(.)\)</span> 表示将向量 <span
class="math inline">\(\mathbf{h}_t\)</span> 和 <span
class="math inline">\(\mathbf{x}_t\)</span>
进行拼接（concatenation），然后通过一个由线性变换和非线性激活函数组成的基本神经网络模型。</p>
<p>因此，在每一步 <span
class="math inline">\(t\)</span>，前面单词的信息（包含在 <span
class="math inline">\(\mathbf{h}_{t-1}\)</span>
中）与当前单词的信息（包含在 <span
class="math inline">\(\mathbf{x}_t\)</span>
中）相结合。最后的隐藏状态向量 <span
class="math inline">\(\mathbf{h}_{n_x}\)</span>
应包含从左到右处理的关于方面上下文的所有单词信息。最后的隐藏状态可以通过一个输出层生成标签预测。一个典型的输出层包括一个线性变换和一个
softmax 函数：</p>
<p><span class="math display">\[\mathbf{s} = \text{softmax}(\mathbf{W}_f
\cdot \mathbf{h}_{n_x} + \mathbf{b}_f),\]</span></p>
<p>其中：</p>
<p>• <span class="math inline">\(\mathbf{W}_f \in \mathbb{R}^{d_y \times
d_h}\)</span> 是最终层的可训练权重矩阵，</p>
<p>• <span class="math inline">\(\mathbf{b}_f \in
\mathbb{R}^{d_y}\)</span> 是最终层的偏置向量，</p>
<p>• <span class="math inline">\(\mathbf{s} \in
\mathbb{R}^{d_y}\)</span> 包含每个情感类别为正确标签的概率。</p>
<p>这些概率也可以解释为情感得分。在应用 softmax
函数后，可以通过选择具有最高情感得分或正确性概率的标签来生成预测。</p>
<p>虽然基础 RNN
模型在处理较短序列时效果很好，但在处理较长和更复杂的句子时会出现问题。由于神经网络的乘性特性，RNN
模型通常会严重受到梯度消失问题的影响，这意味着这些模型难以捕获长期依赖【19,
87】。</p>
<p>因此，更高级的 RNN 模型，例如长短期记忆网络（Long Short-Term Memory,
LSTM）【88】和门控循环单元（Gated Recurrent Unit,
GRU）【35】，通过引入一系列门控机制改进了函数 <span
class="math inline">\(f(.)\)</span>。这些门控机制允许信息在模型中流动，而不会丢失关键细节。这些
RNN 模型已被证明适用于许多不同的问题，并且已成为在 ABSC 中实现 RNN
的标准。进一步的改进可以通过双向
RNN（Bi-RNN）实现，该模型不仅从左到右处理单词，还可以从右到左处理。这种双向结构反转了信息流动的方向，允许保留单词序列两端的信息。</p>
<p>RNN 模型是多功能的模型，可应用于许多涉及序列数据的任务，包括
ABSC。文献【7】中实现了一种用于句子级 ABSC 的 LSTM
模型，并将其与用于酒店评论 ABSC 的 SVM 模型进行了比较。SVM 模型显著优于
RNN 模型，作者解释这主要归因于训练 SVM
模型时使用了丰富的手工设计特征向量。</p>
<p>相对而言，文献【166】中实现了一种分层 LSTM 模型，并使用
SemEval-2016【157】数据集进行评估，与其他深度学习架构进行了比较。该分层
LSTM 模型利用文档级信息执行句子级
ABSC。研究表明，即使未使用丰富的手工设计特征，该模型的性能与竞赛中的最佳模型相当。</p>
<p>在文献【187】中，提出了一种处理显式方面的新方法。两种 LSTM
模型分别用于建模目标词左右上下文的部分。该方法在基本 LSTM
模型的基础上有所改进，但与使用丰富特征和池化词嵌入的高级 SVM
模型相比，性能仍然相近【203】。然而，RNN 模型通常在更近期的任务中优于
SVM，特别是在 FiQA-2018 任务【126】中。</p>
<p>在最近的研究中，RNN 等深度学习模型在预测性能方面迅速超越了 SVM
模型。这种趋势在许多语言处理任务中都存在，部分原因是可用训练数据的增加。例如在
GLUE【206】和
SuperGLUE【205】基准测试中，所有表现最佳的模型均为深度学习方法。然而，如前所述，性能的提升需要付出巨大的计算代价，因为深度学习模型的训练通常需要比简单模型（如
SVM）更多的资源。</p>
<p>另一方面，深度学习模型可以节省设计手工特征的时间。因此，在为特定任务选择最佳模型时，除了基准数据集上的预测性能外，还需要考虑多种因素。例如，可能没有足够的计算资源，或者获取足够训练数据以正确训练深度学习模型的成本过高。另一方面，也可能没有足够的时间为高性能的
SVM 设计手工特征。</p>
<p>递归神经网络（Recursive Neural Network, RecNN）模型【71】是 RNN
模型的推广，使用类似树状结构处理单词。与 RNN 模型类似，RecNN
是通用模型，可以应用于多种任务。基本的 RecNN 模型如 Fig. 3 所示。</p>
<p>与 RNN 模型相似，RecNN 定义了一个函数 <span
class="math inline">\(f(.)\)</span>，用于组合输入向量。该函数在整个网络中共享，因此在每个处理步骤中都会使用。如图
3 所示，RecNN
树由叶节点（嵌入的输入向量）和内部节点（用蓝色表示）组成，在内部节点中使用函数
<span class="math inline">\(f(.)\)</span> 将向量组合。</p>
<p>![[Fig. 3. An illustration of a basic RecNN model for ABSC..png]]</p>
<p>对于一个具有两个子节点的内部节点，其输出可以定义为：</p>
<p><span class="math display">\[\mathbf{h}_p = f(\mathbf{h}_l,
\mathbf{h}_r),\]</span></p>
<p>其中：</p>
<p>• <span class="math inline">\(\mathbf{h}_p\)</span>
表示父节点的输出向量，</p>
<p>• <span class="math inline">\(\mathbf{h}_l\)</span> 和 <span
class="math inline">\(\mathbf{h}_r\)</span>
分别表示左子节点和右子节点的向量，</p>
<p>• <span class="math inline">\(f(.)\)</span>
是用于组合子节点向量的函数。</p>
<p>其中，<span class="math inline">\(\mathbf{h} \in
\mathbb{R}^{d_x}\)</span> 是内部节点的输出向量，<span
class="math inline">\(\mathbf{c}_1 \in \mathbb{R}^{d_x}\)</span> 和
<span class="math inline">\(\mathbf{c}_2 \in \mathbb{R}^{d_x}\)</span>
是子节点的输出向量。通过迭代地组合单词嵌入和内部节点的输出向量，模型可以从单词中提取信息。在最后一步中，图
3 中的输出向量 <span class="math inline">\(\mathbf{h}_4\)</span>
应包含整个句子或上下文的信息。</p>
<p>虽然我们使用的是二叉树（ABSC
中最常见的类型）作为示例，但每个内部节点拥有三个或更多子节点的树也是可能的。然而，此时需要调整函数
<span class="math inline">\(f(.)\)</span> 以处理两个以上的输入。</p>
<p>在最基本的 RecNN 设置中，函数 <span
class="math inline">\(f(.)\)</span>
采用标准神经网络的形式。然而，使用这一基本设置的 RecNN 模型可能会遇到与
RNN 相同的问题。因此，诸如 LSTM 模块之类的门控函数已经被调整为适配 RecNN
模型【186】。</p>
<p>使用 RecNN 模型相较于 RNN
的主要优势在于，模型不再局限于从左到右或从右到左顺序处理单词。输入的处理顺序取决于定义树的关系类型。树的结构也可以被设计为使模型按照原始顺序处理单词，这样它就等同于标准
RNN。然而，这也引出了这些模型的一个缺点：即如何定义树结构可能是一项困难的任务。</p>
<p>通常，RecNN
树是通过语言解析器构建的，解析器分析句子的结构和依赖关系，并对单词序列进行分解。解析是一个完全独立的研究领域，其中已经进行了大量研究【111】。现有多种解析器可用，而图
3 中的树是使用流行的 Stanford 神经网络解析器【33】生成的。</p>
<p>前面讨论的 RecNN 特性是适用于任何任务的通用属性。然而，也有针对 ABSC
提出的更专业化的 RecNN 模型。例如，树的构建可以专门针对 ABSC
任务进行调整。在文献【51】中，提出了一种用于处理显式方面的技术。文献【51】中的树是围绕与显式方面对应的目标单词或短语构建的。这意味着模型能够学习向方面前向传播情感信息。此外，与仅使用一个函数
<span class="math inline">\(f(.)\)</span>
不同，该模型实现了多种组合函数，模型根据输入向量和语言特性自适应地选择组合函数。这种专门为
ABSC 设计的 RecNN 模型被证明显著优于之前的 SVM 模型【101】。</p>
<p>Nguyen 和
Shirai【145】在这一思想的基础上进行了扩展，构建了一个结合了依存树和成分树的模型。此外，他们还扩展了对多种组合函数的使用。研究表明，该模型的性能优于文献【51】中提出的模型。然而，根据报告的结果，目前发现的
RecNN 模型的性能仍然不如其他深度学习模型。</p>
<p>卷积神经网络（Convolutional Neural Network,
CNN）模型【116】是另一种广受欢迎的深度学习模型，常用于文本分析任务，例如情感分析和
ABSA【179, 221】。最初，CNN
模型被用于处理图像，采用三种不同的层类型：卷积层、池化层和全连接层。Fig.
4 展示了语言处理背景下这些层的基本 CNN 模型。</p>
<p>• <strong>卷积层（Convolutional Layer）</strong>
通常被描述为一个滤波器，滑过输入数据矩阵，并对窗口内的值进行线性组合，从而生成特征图。</p>
<p>• <strong>池化层（Pooling Layer）</strong>
可用于减少输出的维度，并通过总结卷积层获得的信息来提高泛化能力。常见的池化方法包括最大池化（Max
Pooling）和平均池化（Average Pooling）。</p>
<p>• <strong>全局池化层（Global Pooling Layer）</strong>
通常在最终层之前使用，用于将特征矩阵转换为单个向量。</p>
<p>最终，该向量通过全连接层（Fully-Connected
Layers）进行处理，以生成正确的输出。CNN
模型因其高效性和结构灵活性而在文本分析领域表现出色。</p>
<p>![[Fig. 4. An illustration of a basic CNN model for ABSC..png]]</p>
<p>前述模型层构成了可用于多种任务的通用架构。因此，CNN 模型也被用于
ABSC。在文献【142】中，研究表明，CNN 模型在 ABSC 中的性能优于 SVM
模型。</p>
<p>文献【167】中实现了一种专门设计用于 ABSC 的
CNN，通过引入目标嵌入（Target
Embeddings）明确建模方面。该方法将方面嵌入与输入向量拼接后作为 CNN
的输入。结果显示，这种方法在所用数据集上表现与最佳模型相当。</p>
<p>另一种方法由文献【221】提出，使用方面嵌入，但以非线性门控机制的形式插入到卷积层和池化层之间。该模型生成了高度准确的预测，优于许多其他模型，包括随机森林【79】、RecNN
模型【51, 145】，甚至一些基于注意力的深度学习模型（见 4.2.4
小节讨论）。</p>
<p>文献【227】进一步扩展和改进了该门控
CNN，通过在损失函数中引入语言正则化扩展进行优化。尽管该模型在
SemEval-2014
数据集上接近于最佳性能，但仍被一种混合模型【106】超越，该混合模型将在
4.3.1 小节中讨论。</p>
<h3 id="attention-based-deep-learning">4.2.4 Attention-Based Deep
Learning</h3>
<p>注意力机制（Attention
Mechanism）【15】是前述深度学习模型的一种非常有效的扩展。本节讨论用于
ABSC 的基于注意力的深度学习（Attention-Based Deep Learning,
AB-DL）模型。各种 AB-DL 模型的性能见表 5。图 5
中展示了注意力机制的基本原理，该示例基于图 2 中的 RNN
模型，并扩展了基本的注意力架构。考虑输入表示矩阵 <span
class="math inline">\(\mathbf{X} \in \mathbb{R}^{d_x \times
n_x}\)</span>，其列为向量 <span class="math inline">\(\mathbf{x}_1,
\dots, \mathbf{x}_{n_x} \in \mathbb{R}^{d_x}\)</span>。RNN
单元用于编码单词序列，以生成与 <span class="math inline">\(n_x\)</span>
个单词对应的隐藏状态 <span class="math inline">\(\mathbf{h}_1, \dots,
\mathbf{h}_{n_x} \in
\mathbb{R}^{d_h}\)</span>。为便于说明，我们将隐藏状态连接起来，形成隐藏状态矩阵
<span class="math inline">\(\mathbf{H} \in \mathbb{R}^{d_h \times
n_x}\)</span>。由于输入向量处理的顺序性，最后的隐藏状态 <span
class="math inline">\(\mathbf{h}_{n_x}\)</span>（如Fig. 5. 中的 <span
class="math inline">\(\mathbf{h}_5\)</span>）应包含整个序列的信息。然而，如前所述，RNN
模型在学习长期依赖关系时表现不佳，而注意力机制能够解决这一问题。</p>
<p>![[Table 5. Overview of prominent attention-based deep learning ABSC
models and their reported performances..png]]</p>
<p>基本的注意力架构包括三个操作：<strong>注意力打分</strong>（Attention
Scoring）、<strong>注意力对齐</strong>（Attention
Alignment）和<strong>加权平均操作</strong>（Weighted Averaging
Operation）。在最通用的形式中，注意力机制需要三个输入：<strong>键向量</strong>（Key
Vectors）、<strong>值向量</strong>（Value
Vectors）和<strong>查询向量</strong>（Query
Vector）。这些注意力概念及其对应的符号由文献【42】引入，并在文献【201】中得到进一步推广。</p>
<p>键和值通常从计算注意力的数据矩阵中导出。在Fig. 5.
中，计算注意力的数据矩阵是隐藏向量的矩阵 <span
class="math inline">\(\mathbf{H} = [\mathbf{h}_1, \dots,
\mathbf{h}_{n_x}]\)</span>。键向量可以通过线性变换隐藏状态向量生成，具体方法是使用可训练的权重矩阵
<span class="math inline">\(\mathbf{W}_K \in \mathbb{R}^{d_k \times
d_h}\)</span>，生成键向量 <span class="math inline">\(\mathbf{k}_1,
\dots, \mathbf{k}_{n_x} \in
\mathbb{R}^{d_k}\)</span>。类似地，可以使用可训练的权重矩阵 <span
class="math inline">\(\mathbf{W}_V \in \mathbb{R}^{d_v \times
d_h}\)</span> 获取值向量 <span class="math inline">\(\mathbf{v}_1,
\dots, \mathbf{v}_{n_x} \in \mathbb{R}^{d_v}\)</span>。</p>
<p>然而，为了简化，图 5
中的示例直接使用原始隐藏状态向量作为键和值。在注意力计算的第一步（<strong>注意力打分</strong>）中，键向量与查询向量
<span class="math inline">\(\mathbf{q} \in \mathbb{R}^{d_q}\)</span>
结合，用于计算每个特征向量的注意力分数。这些分数直接决定了对每个单词的关注程度。第
<span class="math inline">\(t\)</span>
个单词的注意力分数取决于对应的键向量 <span
class="math inline">\(\mathbf{k}_t\)</span>、查询向量 <span
class="math inline">\(\mathbf{q}\)</span> 和一个打分函数（Score
Function）。打分函数的形式可以多种多样，但通常用于确定键向量和查询向量之间的关系。</p>
<p>![[Fig. 5. An illustration of a basic attention-based RNN model for
ABSC..png]]</p>
<p>一种常见的打分函数是<strong>加性打分函数</strong>（Additive Score
Function）【15】，它被许多 ABSC 模型使用【210】：</p>
<p><span class="math display">\[s_t = \mathbf{w}_a^T \cdot
\tanh(\mathbf{W}_q \cdot \mathbf{q} + \mathbf{W}_k \cdot \mathbf{k}_t +
\mathbf{b}_a)\]</span></p>
<p>其中：</p>
<p>• <span class="math inline">\(\mathbf{w}_a \in
\mathbb{R}^{d_a}\)</span> 是可训练权重向量，</p>
<p>• <span class="math inline">\(\mathbf{W}_q \in \mathbb{R}^{d_a \times
d_q}\)</span> 和 <span class="math inline">\(\mathbf{W}_k \in
\mathbb{R}^{d_a \times d_k}\)</span> 是可训练权重矩阵，</p>
<p>• <span class="math inline">\(\mathbf{b}_a \in
\mathbb{R}^{d_a}\)</span> 是偏置项，</p>
<p>• <span class="math inline">\(s_t\)</span> 是第 <span
class="math inline">\(t\)</span> 个单词的注意力分数。</p>
<p>其中，<span class="math inline">\(e_t \in \mathbb{R}^1\)</span>
是属于第 <span class="math inline">\(t\)</span>
个单词的注意力分数，<span class="math inline">\(\mathbf{w} \in
\mathbb{R}^{d_w}\)</span>、<span class="math inline">\(\mathbf{W}_k \in
\mathbb{R}^{d_w \times d_k}\)</span>、<span
class="math inline">\(\mathbf{W}_q \in \mathbb{R}^{d_w \times
d_q}\)</span> 和 <span class="math inline">\(\mathbf{b} \in
\mathbb{R}^{d_w}\)</span> 是可训练的权重矩阵和向量，<span
class="math inline">\(d_w\)</span>
是预定义的维度参数。其他类型的打分函数通常基于点积，例如乘性（Multiplicative）【122】、缩放乘性（Scaled-Multiplicative）【201】、一般打分函数（General
Score Function）【122】以及激活的一般打分函数（Activated General Score
Function）【123】。</p>
<p>注意力打分可以理解为确定哪些单词包含关于正确情感分类的最重要信息。分数越高，相关向量中包含的信息就越重要。在打分计算中，查询向量
<span class="math inline">\(\mathbf{q}\)</span>
是决定哪些信息重要的关键。最简单的定义查询的方法是将其设置为一个常量向量。这使得模型能够学习一种通用方式来确定关注哪些单词。然而，由于不同方面的特性可能差异很大，通用查询可能不够灵活。</p>
<p>一种更受欢迎的技术是使用方面的向量表示作为查询。例如，如文献【223】所示，可以将目标的隐藏向量表示（如图
5 中的 <span class="math inline">\(\mathbf{h}_1\)</span> 和 <span
class="math inline">\(\mathbf{h}_2\)</span>
的池化版本）用作查询。在这种情况下，注意力分数的计算决定了哪些信息对于该方面本身最为重要，从而增强了模型在序列中聚焦于最重要单词的能力。这在句子包含多个方面时尤为重要。</p>
<p>注意力分数的目的是在加权平均计算中用作权重。然而，为了这一目的，这些权重需要加和为
1。因此，对注意力分数 <span class="math inline">\(e_1, \dots, e_{n_x}
\in \mathbb{R}^1\)</span> 使用一个对齐函数 <span
class="math inline">\(\text{align}()\)</span>，定义如下：</p>
<p><span class="math display">\[ a_t = \text{align}(e_t;
\mathbf{e})\]</span></p>
<p>其中，<span class="math inline">\(a_t \in \mathbb{R}^1\)</span>
是对应于第 <span class="math inline">\(t\)</span>
个单词的注意力权重，<span class="math inline">\(\mathbf{e} \in
\mathbb{R}^{n_x}\)</span>
是包含所有注意力分数的向量。一种非常流行的对齐函数是软对齐（Soft
Alignment）或全局对齐（Global Alignment），它对注意力分数应用 softmax
函数。其他类型的对齐函数包括硬对齐（Hard
Alignment）【219】和局部对齐（Local
Alignment）【122】，这些方法提供了更聚焦的注意力对齐。</p>
<p>此外，文献【84】中引入了基于句法的对齐函数，该函数专门为 ABSC
设计。它使用全局对齐，但根据对应单词在依存树中距目标的远近缩放注意力权重。</p>
<p>注意力权重 <span class="math inline">\(a_1, \dots, a_{n_x} \in
\mathbb{R}^1\)</span> 与值向量 <span class="math inline">\(v_1, \dots,
v_{n_x} \in \mathbb{R}^{d_v}\)</span> 结合，用于计算所谓的上下文向量
<span class="math inline">\(\mathbf{c} \in
\mathbb{R}^{d_v}\)</span>。为便于说明，图 5 中的注意力权重由向量 <span
class="math inline">\(\mathbf{a} \in \mathbb{R}^{n_x}\)</span>
表示。</p>
<p>上下文向量可以与输出层中的最终隐藏状态结合，用于生成标签预测。上下文向量的计算公式如下：</p>
<p><span class="math display">\[\mathbf{c} = \sum_{t=1}^{n_x} a_t \cdot
\mathbf{v}_t\]</span></p>
<p>其中：</p>
<p>• <span class="math inline">\(\mathbf{c}\)</span> 是上下文向量，</p>
<p>• <span class="math inline">\(a_t\)</span> 是第 <span
class="math inline">\(t\)</span> 个单词的注意力权重，</p>
<p>• <span class="math inline">\(\mathbf{v}_t\)</span> 是对应于第 <span
class="math inline">\(t\)</span> 个单词的值向量。</p>
<p>上下文向量 <span class="math inline">\(\mathbf{c}\)</span>
汇总了与方面对应的上下文中最重要的信息。由于注意力机制可以从任何隐藏状态中选择和提取信息，因此信息的检索不依赖于其在序列中的位置。这大大增强了模型捕获长期依赖的能力。</p>
<p>此外，注意力权重直接反映了某些单词的重要性。因此，通过分析模型倾向于关注哪些单词，注意力模型提供了一定程度的解释性。这为通常被视为黑箱模型的深度学习方法增加了一些可解释性。然而，对注意力权重解释的实用性存在争议【97,
138, 214】。</p>
<p>我们所举的示例将 RNN
模型生成的隐藏状态用于注意力计算。然而，注意力机制也可以应用于其他模型，例如
CNN。在 CNN
的情况下，可以将卷积层生成的特征图用作注意力机制的输入，其余的注意力计算过程保持不变。注意力模型的核心包括打分函数和对齐函数，这些是每个注意力模型的基本组成部分。</p>
<p>尽管基本的注意力模型可以用于 ABSC
任务，但通常会通过扩展来进一步改进或处理其他类型的输入。例如，<strong>共注意力机制（Co-Attention）</strong>【121】可用于计算多个特征矩阵之间的注意力关系，例如在有多个模型输入的情况下。对于
ABSC，如果将输入文本分为不同部分，例如目标词及其左右的单词，共注意力机制可以有效地处理这些情况。类似的方法是<strong>旋转注意力（Rotatory
Attention）</strong>【229】，它在输入之间旋转注意力。</p>
<p>一种更通用的注意力扩展是<strong>多头注意力（Multi-Head
Attention）</strong>【201】，它使用多个注意力“头”来并行计算不同类型的注意力。相反，<strong>多跳注意力（Multi-Hop
Attention）</strong>【196】允许模型按顺序进行多次注意力计算。多头注意力和多跳注意力是
Transformer
模型【201】的重要组成部分。这种模型完全基于注意力机制，不使用 RNN 或 CNN
之类的基础模型。</p>
<p>Transformer
模型通过自注意力（Self-Attention）在特征向量之间计算注意力，并使用提取的信息迭代地转换输入。这类模型的典型特征是使用<strong>缩放乘性打分函数（Scaled-Multiplicative
Score Function）和全局对齐函数（Global Alignment
Function）</strong>，因为这些函数在计算上非常高效。Transformer
模型已被证明在包括 ABSC 在内的许多任务中非常有效。</p>
<p>文献【67】提出了一种针对 ABSC 任务进行扩展的 Transformer
模型，能够更好地处理目标信息。文献【220】中提出了结合共注意力机制的
Transformer 模型。Zeng 等人【226】实现了两个并行的 Transformer
模型，以便分别处理与某一方面相关的全局上下文和局部上下文。这些
Transformer 模型及其扩展显著优于之前的模型，包括其他注意力模型。</p>
<p>在仅考虑预测性能时，Transformer 模型往往在大多数 ABSC
任务以及现代语言处理基准（如 GLUE【206】和
SuperGLUE【205】）中占据主导地位。然而，与其他深度学习模型相比，Transformer
模型从头训练需要大量的训练数据和计算资源。目前的一部分研究集中在通过更高效的架构（如
Linformer【209】）来减少 Transformer 模型所需的资源。尽管 Transformer
模型能够产生令人印象深刻的结果，但对于资源有限的问题，其可能并不适用。</p>
<h2 id="hybrid">4.3 Hybrid</h2>
<p>当可用数据有限时，机器学习模型可能无法提供令人满意的结果【179】。ABSA
是一个数据集通常被认为规模较小的领域【85】（参见表
2），这使得训练大型语言模型成为一项重大挑战。在一些更为小众的子领域中进行
ABSC（如对较不流行的产品或主题的文本情感分析）时，这个问题会变得更加严重。此外，语言种类繁多也加剧了这一问题。对于英语，通常有大量的资源可用。然而，对于其他语言（如阿拉伯语或中文），尽管一些新数据集正逐步引入（参见
SemEval-2016【157】），但数据量仍然有限【50】。因此，对于 ABSC
来说，训练大型语言模型可能很困难。解决这一问题的一种方法是通过引入知识库的额外知识来弥补数据的不足。我们将结合了机器学习和知识库的模型称为<strong>混合模型（Hybrid
Models）</strong>。</p>
<p>将知识库与机器学习结合的方法多种多样。一种常见的方法是使用知识库定义特征，这些特征供机器学习算法用来预测情感。另一种方法是依次或并行实现知识库分类器和机器学习分类器。本节中，我们讨论这些不同的方法，并将其分为三类：<strong>词典增强模型（Dictionary-Enhanced
Models）</strong>、<strong>本体增强模型（Ontology-Enhanced
Models）和语篇增强模型（Discourse-Enhanced
Models）</strong>。各种混合模型的性能见表 Table 6。</p>
<p>![[Table 6. Overview of prominent hybrid ABSC models and their
reported performances..png]]</p>
<h3 id="dictionary-enhanced-machine-learning">4.3.1 Dictionary-Enhanced
Machine Learning</h3>
<p>词典增强的机器学习方法是较为常见的混合方法之一。词典是一种丰富的知识库，可以很方便地用来为机器学习算法定义特征。在文献【200】中，通过将
SentiWordNet【14】词典中的情感得分作为特征，提升了 SVM
模型的性能。同样的技术在文献【47】和【110】中也被使用，均利用
SentiWordNet。此外，SemEval-2014 ABSC 任务中表现最佳的模型
NRC-Canada【106】也是基于类似的概念。然而，作者从未标注的评论语料库中生成了自己的情感词典。文献【203】中提出了另一种不同的方法。首先，使用
word2vec【133】定义词嵌入。然后，对不在情感词典中的单词嵌入进行筛选，仅保留包含有用情感信息的单词。由于分类使用的是
SVM 模型，作者采用了多种池化函数来总结密集特征向量中的特征，以便供 SVM
模型使用。这种结合词典和机器学习的方法有效地提升了情感分类的性能。</p>
<p>在文献【16】中，作者提出注意力模型在使用小型数据集训练时通常容易过拟合。因此，文献【16】提出了一种结合词典特征的基于注意力的
LSTM
模型，以提高注意力深度学习模型在数据不足情况下的灵活性和鲁棒性。</p>
<p>文献【4】中提出了一种专门针对资源匮乏语言（如印地语）的混合模型。该模型结合了经过训练的
CNN
特征和词典特征。结果表明，与未利用知识库的测试基线相比，该模型表现更好。</p>
<p>在文献【96】中，针对印度尼西亚在线市场的评论，提出了一种 Bi-GRU
模型用于
ABSC。研究表明，通过将词典特征整合到模型中，可以显著提高性能。</p>
<p>词典和其他知识库可以增强机器学习模型，但机器学习也可以改进知识库的构建。一个重要的例子是
<strong>SenticNet</strong>【30】，它是一种知识库，使用语言模式、一阶逻辑和深度学习来发现实体、概念和原语之间的关系。这种符号（Symbolic）与次符号（Subsymbolic）工具的结合在情感分析和
ABSA 中非常有用。例如，文献【125】提出了 <strong>Sentic LSTM</strong>
模型，这是一种将 Sentic 知识完全整合到深度学习架构中的基于注意力的
LSTM。研究表明，这种符号与次符号人工智能的结合在性能上优于单独使用符号或次符号人工智能的方法。</p>
<h3 id="ontology-enhanced-machine-learning">4.3.2 Ontology-Enhanced
Machine Learning</h3>
<p>本体（Ontology）既可以作为情感信息的知识库，也可以用于定义概念之间的结构。在文献【45】中，使用了一个专门针对餐厅的本体来为基于评论级
ABSC 的 SVM 模型定义特征。作者利用本体中的概念和情感信息来定义特征。</p>
<p>具体而言，定义了一个“概念袋”（Bag-of-Concepts），其中每个本体概念都对应一个二进制特征。然而，这些特征的值取决于该概念是否与方面本身相关，这意味着文本中的隐式信息可以被编码到特征向量中。其次，还定义了特征以统计本体中具有情感极性的单词在文本中出现的频率。通过这些方法，本体增强的机器学习模型可以更好地捕捉文本中的情感信息，提高分类性能。</p>
<p>第二种本体增强模型的方法是通过一种两步方法，依次采用基于知识的方法和机器学习分类器。首先，使用基于本体的模型尝试对方面进行分类。如果情感标签的分类结果冲突，或者没有关于情感的信息，则使用机器学习分类器作为备用模型。</p>
<p>在文献【204】中，针对句子级
ABSC，先使用基于本体的分类模型，然后在其后实现多跳旋转注意力模型作为备用算法。结果表明，与单独使用任一分类器相比，两步方法的性能更好。</p>
<p>在文献【130】中，采用了一个两步方法，首先使用一个词汇化领域本体，然后是一个注意力模型。该注意力模型结合了句子级内容注意力机制和双向上下文注意力机制。文献【131】进一步扩展了这一模型，加入了
BERT【48】词嵌入、额外的正则化，以及对训练过程的调整。</p>
<p>在文献【197】中，基于本体和多跳旋转注意力模型的两步方法进一步结合了分层注意力机制和深度上下文词嵌入。文献【175】则将两步方法与基于本体的特征结合，用于句子级
ABSC。首先，使用基于本体的分类器尝试确定方面的情感分类。如果无法找到明确的答案，则采用使用本体情感特征增强的词袋特征向量的
SVM 进行分类。</p>
<p>本体增强方法在训练小众领域的语言模型用于 ABSC
时可能非常有用。在文献【44】中，基于本体增强的 SVM 模型在 SemEval-2016
数据集上进行了测试。结果表明，具有本体特征的模型相比没有这些特征的模型获得了显著更高的
F1 分数。此外，具有本体特征的模型在使用不到 60%
的训练数据时即可达到相同的性能。</p>
<p>文献【176】显示，基于本体增强的模型对数据集规模的变化表现出高度鲁棒性。在方面提取任务中，文献【176】中提出的本体增强
SVM 模型的性能在仅使用 20% 原始训练数据时下降不足
10%，而基础方法的性能则显著下降。对于 ABSC
任务，文献【176】中提出的所有模型似乎对数据集规模的变化都表现出鲁棒性。然而，这可以归因于所有方法还结合了另一种形式的知识库，即情感词典。</p>
<p>在文献【68】中，本体增强 ABSC
方法被应用于西班牙语的信息流行病学（Infodemiology）这一小众领域。从
Twitter 帖子中提取方面，并使用 Bi-LSTM
模型和传染病领域的本体进行分类。</p>
<h3 id="discourse-enhanced-machine-learning">4.3.3 Discourse-Enhanced
Machine Learning</h3>
<p><strong>4.3.3 基于语篇增强的机器学习</strong></p>
<p>如前所述，语篇树与本体类似，其本身并不包含情感信息。然而，与本体一样，语篇树的结构仍然可以与机器学习分类器结合使用。</p>
<p>文献【208】中，Wang
等人使用语篇树的结构，通过基于注意力的深度学习模型进行句子级
ABSC。对于每个从句，模型使用一个双向
LSTM（Bi-LSTM）和一个注意力层来生成由注意力层的上下文向量输出表示的从句向量。然后，这些从句表示通过另一个
Bi-LSTM
和注意力层处理，形成分层注意力结构。最终的上下文向量用于预测。然而，该模型仅将语篇树定义的个别从句纳入模型，而未考虑树的关系结构。</p>
<p>在文献【216】中，通过使用<strong>连接规则（Conjunction
Rules）</strong>，将语篇从句之间的关系整合到模型中。首先，使用 Bi-LSTM
层单独处理语篇树标识的每个从句。然而，从句表示仅通过求平均值生成。然后，这些从句表示被用于以下几层：一个用于提取从句间关系的
Bi-LSTM 层，以及一个使用连接规则提取附加特征的层。</p>
<p>连接词指示从句如何连接。例如，通过“and”连接的从句称为并列连接，从句间通常共享相同的情感。当使用“but”时，表示存在对立连接，这通常表明情感相反。这些信息被用来选择性地总结从句表示。通过这些技术，可以将上下文的修辞结构整合到模型中，从而增强情感分类的能力。</p>
<h1 id="related-topics">5. RELATED TOPICS</h1>
<h2 id="aspect-detection-aggregation">5.1 Aspect Detection &amp;
Aggregation</h2>
<p>在第 1 节中，ABSA
被描述为由三步组成：<strong>检测（Detection）</strong>、<strong>分类（Classification）</strong>
和<strong>聚合（Aggregation）</strong>。本综述主要讨论分类步骤，即
ABSC。然而，这三步并非完全独立，因为这些问题是相关的【174】。例如，在设计分类和聚合步骤时，可能需要联合考虑，因为分类步骤中提取的信息在聚合步骤中可能会有用。例如，可以使用句子重要性等特征来创建加权平均值，这通常优于简单平均值的聚合方法【18】。同样，方面检测步骤的信息也可以在分类步骤中发挥作用。例如，可以利用在方面提取阶段提取的特征来预测情感【25】。</p>
<p>此外，也有模型将检测和分类步骤完全联合执行。例如，文献【234】中提出了一种基于
WordNet【134】的词典模型，通过提取并配对方面和观点词来确定方面级情感。文献【172】中提出了一种完全融合检测和分类步骤的端到端
LSTM
模型。该模型联合学习这两个任务，并在两方面均表现出改进的性能。文献【211】中提出了一种胶囊注意力模型（Capsule
Attention
Model），该模型同样以端到端的方式联合学习检测和分类任务。该模型在两个任务上均实现了最新的最佳性能。</p>
<h2 id="sarcasm-thwarting">5.2 Sarcasm &amp; Thwarting</h2>
<p><strong>挫折（Thwarting）</strong>和<strong>讽刺（Sarcasm）</strong>是两种复杂的语言现象，对情感分析模型构成了显著挑战。</p>
<p>•
<strong>挫折</strong>是指通过建立预期，然后与之对比的概念。换句话说，文档的整体情感与文本大部分部分中表达的情感不同【161】。因此，仅依赖单词层面情感聚合的方法很容易忽略被挫折文本的整体上下文。此外，由于缺乏可用的训练数据，开发检测挫折的模型是一项困难的任务【161】。从极端情况来看，检测挫折与识别讽刺这一困难任务有很高的相似性【161】。</p>
<p>•
<strong>讽刺</strong>是通过使用与其实际意义相反的语言，故意嘲讽或质疑某些主题的行为【104】。这种现象在社交媒体文本中特别常见，其中讽刺分析通常还涉及表情符号和标签【129】。识别文本中的讽刺甚至对于人类来说也是一项困难的任务【129】，需要丰富的背景知识，而这些知识很难包含在大多数情感模型中【161】。</p>
<p>这些语言现象的复杂性使得开发能够准确处理挫折和讽刺的情感分析模型变得极具挑战性。</p>
<p><strong>挫折（Thwarting）</strong>和<strong>讽刺（Sarcasm）</strong>问题使得情感分析变得不再简单【11】。然而，在
ABSC 文献中，这两个问题却鲜有被关注。在 ABSC
的背景下，挫折或讽刺意味着记录的大部分或全部语言表达的情感与真实的方面情感相反。</p>
<p>在通用情感分析的文献中，已经提出了一些解决这些问题的尝试。然而，情感分析通常被认为与挫折或讽刺检测是独立的任务。例如：</p>
<p>• 文献【161】中，通过结合相机评论的领域本体和 SVM
模型，识别产品的挫折性评论。</p>
<p>• 文献【112】中，使用基于多头注意力的 LSTM
模型检测社交媒体文本中的讽刺。</p>
<p>然而，挫折和讽刺也可以在情感分析的背景下处理。例如：</p>
<p>•
文献【135】通过跟踪人工标注者眼动数据获取认知特征，用于解决情感分析中的挫折和讽刺问题。研究假设，识别挫折和讽刺的认知过程与阅读者的眼动有关。结果表明，视线特征显著提高了情感分析分类器（如
SVM）的性能，特别是在分析复杂语言结构（如挫折和讽刺）方面表现突出。</p>
<p>• 文献【59】通过多任务模型联合考虑情感分析和讽刺检测任务。该模型使用
Transformer
来识别阿拉伯语推文中的情感和讽刺，研究表明，该多任务模型的性能优于单任务模型。</p>
<p>这些方法同样可以用于 ABSC。例如，可以将认知特征与方面特征结合，以应对
ABSC 任务中的挫折和讽刺问题。同样，ABSC
任务可以与讽刺检测任务联合建模。然而，针对这些问题，需要更为专门化的
ABSC 数据集支持模型训练和评估。</p>
<h2 id="emotions">5.3 Emotions</h2>
<p><strong>情感（Emotion）</strong>是与 ABSC
结合时一个有趣的研究方向。情感分析与情绪分析是两个高度相关的主题。在情感分析中，我们通常为文本分配极性标签或分数，而在情绪分析中，需要考虑更广泛的情绪类别（例如，“喜悦”、“悲伤”、“愤怒”）【28】。</p>
<p>例如：</p>
<p>• 文献【195】中，电影评论被分析并根据情绪沙漏模型（Hourglass of
Emotions）【31】的维度分配情绪分数。</p>
<p>•
文献【81】指出，与情感分析类似，情绪分析也可以在多个层级进行。情绪分析中与
ABSC 对应的任务被称为<strong>基于方面的情绪分析（Aspect-Based Emotion
Analysis）</strong>。</p>
<p>一些具体研究包括：</p>
<p>•
文献【156】中，通过词嵌入质心、词典和表情符号对社交媒体帖子中方面的情绪进行分类。</p>
<p>• 文献【184】测试了几种模型，用于检测餐厅评论中针对方面的情绪。</p>
<p>此外，情绪信息也可以用于提升 ABSC 的任务性能。例如：</p>
<p>• 文献【113】中，通过使用 SenticNet【30】知识库中的情绪特征增强 ABSA
模型。</p>
<p>• 文献【125】提出了一种基于注意力的 LSTM 模型，该模型结合
AffectiveSpace【29】知识库中的情绪信息，在 ABSC
任务中实现了更好的性能。类似的技术也见于文献【124】。</p>
<p>情绪分析和情感分析还可以联合进行。例如：</p>
<p>•
文献【213】提出了一种模型，能够提取包含情感极性和情绪类别的方面级情感知识。</p>
<p>•
文献【212】提出了一个广泛的框架，用于多层级感知情感和情绪，该框架结合了知识库并处理了讽刺问题。</p>
<p>这些研究表明，将情绪分析与情感分析相结合，或者将情绪信息融入 ABSC
模型中，能够为模型带来显著的性能提升。</p>
<h1 id="conclusion">6. CONCLUSION</h1>
<p>在本综述中，我们概述了当前基于方面情感分类（ABSC）领域的最新模型。我们根据
ABSC 的三个主要阶段解释了其过程：<strong>输入表示（Input
Representation）</strong>、<strong>方面情感分类（Aspect Sentiment
Classification）</strong> 和<strong>输出评估（Output
Evaluation）</strong>。</p>
<p>•
<strong>输入表示阶段</strong>涉及将文本表示为数值向量或矩阵，以便分类模型能够识别方面的正确极性标签。</p>
<p>•
<strong>输出评估阶段</strong>通过性能指标评估极性标签预测的质量。预测的质量由分类模型的架构决定。</p>
<p>我们通过提出的分类法和汇总表讨论了多种最新的 ABSC
模型，表格为模型性能提供了概览。这些模型通过直观解释、技术细节和报告的性能数据进行了讨论和比较。此外，我们还探讨了与
ABSC 相关的各种重要主题。</p>
<p><strong>ABSC</strong>
是一项相对较新的任务，因其快速发展的领域特性而备受关注。ABSC
研究的显著进展之一体现在所使用的数据集上。早期的 ABSC
研究中，作者通常通过网络抓取并编译自己的数据集。这种方法的出现是合理的，因为网络上存在大量的公开评论。然而，由于大多数模型在不同数据集上进行测试，这使得性能比较变得困难。研究者开始采用文献【51】中的
Twitter 数据集以及 SemEval
挑战【157–159】中的评论数据集后，实际的比较分析才变得更为可行。这对 ABSC
领域来说是一个重要的发展，但也带来了一些后果，即 ABSC
模型大多仅限于餐厅评论、电子产品评论和 Twitter 数据的实现。尽管
SemEval-2016
数据集【157】包含一组酒店评论，但其他类型的数据（例如酒店评论）却大多被忽略。</p>
<p>为了进一步推动 ABSC
领域的发展，测试模型在其他领域的数据上的表现是非常必要的。这需要更多高质量的公开数据集被发布和采用。例如：</p>
<p>• <strong>FiQA-2018</strong>【126】数据集</p>
<p>• <strong>SentiHood</strong>【168】数据集</p>
<p>• Álvarez-López 等人【9】生产的书评数据集。该数据集从 INEX
Amazon/LibraryThing
图书语料库【109】中提取了书评子集，并在方面级手动标注。然而，该数据集很少被其他研究者采用。此外，对于英语以外的语言，也需要更多的新数据集，以推动多语言
ABSC 研究的发展。这些新数据集的开发将有助于扩展 ABSC
的应用范围，并为进一步的模型比较和改进提供支持。</p>
<p>对于非英语语言的 ABSC 任务，同样需要更多的新数据集。如前所述，英语在
ABSC
任务中拥有相对较多的数据集可用，而其他大多数语言则情况相反，这使得训练语言模型变得更加困难。如果获取更多训练数据的成本过高，诸如
SVM
等较简单的模型通常是更好的选择。然而，通过引入知识库，可以在一定程度上弥补数据的不足，如
4.3 小节讨论的混合模型所示。</p>
<p>正如多项研究【130, 197,
208】所表明，知识库在增强最新模型性能方面非常有效。进一步探索基于知识增强的方法，将有助于提升当前的研究水平。然而，目前的研究中，知识库的整合仍显得相对基础。例如：</p>
<p>• 文献【208】中，模型仅将语篇树提取的从句纳入模型。</p>
<p>•
文献【216】通过加入连接规则对此进行了改进，但这一解决方案未能充分利用其他可能包含有用情感分类信息的语篇关系。</p>
<p>同样，在混合模型中，领域本体通常被作为一种两步方法的一部分，与机器学习模型相互独立。然而，本体包含许多概念和关系，可以为机器学习模型提供重要特征。因此，我们建议进一步整合知识库及其结构。此外，虽然本文仅讨论了三种知识库（词典、本体和语篇树），我们预计未来会有更多新型知识库被探索并应用于
ABSC。同时，可以改进知识库的构建方法。例如，最近关于 ABSC
领域本体半自动构建的研究【46, 175, 190,
235】提供了重要的参考。另一个问题是，对于资源匮乏的语言，适合的知识库可能也很稀缺【3】。因此，开发除标注
ABSC
数据集之外的新型知识资源，是未来发展的重要一步。通过更高效的知识资源构建和整合，可以进一步推动
ABSC 领域的研究进展。</p>
<p>数据不足的问题往往不是数据本身的缺乏，而是缺乏<strong>标注数据</strong>。在标注数据获取成本过高的情况下，无监督或弱监督方法可以非常有用。例如：</p>
<p>•
文献【233】中提出了一种弱监督模型，用于联合方面提取和情感分类。该模型通过带注意力的自动编码器学习情感词典。</p>
<p>•
此外，弱监督系统甚至可以基于专家知识的标注机制【163】，生成新的训练数据。</p>
<p><strong>跨语言（Cross-Lingual）</strong>和<strong>多语言（Multi-Lingual）</strong>模型是解决数据不足问题的另一种方法。可以将具有丰富资源的语言中的知识迁移到其他语言的模型中，以弥补数据的不足【17】。例如：</p>
<p>•
文献【5】中，尝试通过一种基于双语词嵌入的深度学习模型，解决法语和印地语数据集的数据稀疏问题。这些双语词嵌入通过标准机器翻译方法生成的英语-法语和英语-印地语平行语料库构建。</p>
<p><strong>训练更具语言无关性的模型</strong>是语言模型研究的重要一步。同样，开发领域无关的模型也至关重要。通过从数据更丰富的语言领域迁移知识，可以缓解
ABSC 中的数据稀缺问题【34,
218】。一个流行的技术是对大型语言数据集中的模型（如
BERT）进行<strong>预训练（Pre-training）</strong>，然后使用小型领域数据集对模型参数进行<strong>微调（Fine-tuning）</strong>。随着更大、更通用的语言模型的不断出现，这种方法将变得越来越有用【24】。通过这些技术，可以更高效地利用现有资源，解决标注数据不足的问题，同时扩展模型的适用性和通用性。</p>
<p>早期的 ABSC
方法几乎完全基于知识库构建。当更大规模的标注数据集逐渐可用时，诸如 SVM
的机器学习模型成为 ABSC
的标准。不久之后，深度学习方法开始流行，但其性能通常与使用高质量手工设计特征的机器学习方法相当。然而，随着注意力机制的引入，深度学习方法开始迅速超越其他方法。在新的革命性创新出现之前，我们预计基于注意力的深度学习模型将成为
ABSC 的未来。新的注意力模型正在 ABSC
领域内外迅速发展。例如，多维注意力（Multi-Dimensional
Attention）【180】是一种通用的注意力扩展，允许更细粒度的注意力计算，但在
ABSA 中的探索却很少。同样，尽管多头注意力（Multi-Head
Attention）是一种通用的注意力扩展，但通常仅在基于 Transformer
的架构中使用【201】。不可否认，Transformer 模型是一种高度成功的模型，在
ABSC 中实现了最新的最佳性能【67, 105, 218, 220, 226】。随着新的
Transformer 架构被提出（如 Transformer-XL【41】和 Reformer
模型【107】），Transformer 模型无疑将继续在 ABSC
中占据重要地位。注意力模型通过注意力权重提供了一种内在的可解释性，这是许多黑箱算法（如深度学习模型）所缺乏的重要特性。另一种更具解释性的模型是符号人工智能与次符号人工智能的结合。例如，Sentic
LSTM【125】等模型可能为 ABSA
的新型可解释模型奠定基础【185】。未来，随着注意力机制和可解释性模型的进一步发展，ABSC
的研究和应用前景将更加广阔。</p>
<p>ABSC
的发展不仅可以通过模型和数据集的进步实现，还可以通过对其应用的深入研究推动。例如，大多数
ABSC
方法主要关注隐式或显式方面。然而，文本中常常同时包含多个隐式和显式方面【52】，这为
ABSA
方法在现实应用中带来了巨大挑战。因此，需要进一步研究能够同时处理隐式和显式方面的方法。此外，在现实世界的
ABSA
应用中，用户通常更关注聚合在评论或评论集合上的方面情感。然而，大多数
ABSC
方法专注于句子级情感分类，然后将情感在评论级聚合。然而，有研究表明，纯评论级
ABSC 方法的表现优于句子级分类再聚合的方法【45】。因此，对评论级 ABSC
的研究可以进一步推动该领域的发展。ABSC 的一个有趣的新应用是基于 ABSA
的搜索引擎。例如，Smith【36】开发了一种专门的基于意见的搜索引擎，根据在线评论中针对特定方面表达的情感返回餐馆。这类专业化搜索引擎和其他新应用将成为未来推动
ABSC 领域发展的动力。另一个值得探索的方向是重新审视 ABSC
任务的定义。目前，ABSC
的任务通常被定义为静态的情感分类任务，没有考虑时间元素。然而，产品的情感往往会随着时间的推移而变化【136】。一些情感分析研究已考虑到这一概念。例如，在文献【207】中，基于
Twitter 帖子的情感分析被用于跟踪 2012
年美国大选中总统候选人随时间变化的整体情感。尽管如此，随着时间变化的
ABSA
是一个尚未受到足够关注的有趣问题。然而，这一发展需要新的数据集支持，以适应这类任务的需求。通过探索这一方向，可以进一步丰富
ABSC 的研究维度，并开辟新的应用场景。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/paper/" rel="tag"><i class="fa fa-tag"></i> paper</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/45e08570.html" rel="prev" title="基于超图的多模态情绪识别">
                  <i class="fa fa-angle-left"></i> 基于超图的多模态情绪识别
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/c48587a3.html" rel="next" title="Opinion Word Expansion and Target Extraction through Double Propagation">
                  Opinion Word Expansion and Target Extraction through Double Propagation <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><span class="exturl" data-url="aHR0cHM6Ly9iZWlhbi5taWl0Lmdvdi5jbg==">浙ICP备2023011468号-1 </span>
      <img src="https://images-a2q.pages.dev/file/ab4ebf9b9723073c81a21.png" alt="">
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Joey</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">193k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">11:43</span>
  </span>
</div>
  <div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/posts/cc20b26.html"}</script>
  <script src="/js/third-party/quicklink.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":false},"react":{"opacity":1},"log":false});</script></body>
</html>
