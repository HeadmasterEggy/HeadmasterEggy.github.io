<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="mask-icon" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Grand+Hotel:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=JetBrains+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":true,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Deep learning Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning">
<meta property="og:url" content="http://example.com/posts/fd0f6586.html">
<meta property="og:site_name" content="Joey">
<meta property="og:description" content="Deep learning Notes">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images-a2q.pages.dev/file/9174b48689e1c393040c2.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/38ab6dd9542b3cebfb979.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/b2d542e347cf2cc39f231.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/44c39b518a7859eb0c1b4.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/fd3123927499b9be3ce3a.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/4199a79abc3ff39b9fd63.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/64348a5915737ee1ef279.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/10a17d6e466f202f8c9d0.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/4d1600154f68271da5b8a.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/dd0074adc62cc76609d55.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/73fa3d3791d5956a7ab8c.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/a503f980795205f5e59d8.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/f81a09bf629f0471a7a22.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/4f95853c2f976f166748d.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/c0c4b6a69b6a54bf59b59.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/893d629538ac503b2309f.png">
<meta property="article:published_time" content="2023-10-30T10:33:32.000Z">
<meta property="article:modified_time" content="2025-02-15T09:36:58.650Z">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="deepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images-a2q.pages.dev/file/9174b48689e1c393040c2.png">


<link rel="canonical" href="http://example.com/posts/fd0f6586.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/posts/fd0f6586.html","path":"posts/fd0f6586.html","title":"DeepLearning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DeepLearning | Joey</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Joey</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">6</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">44</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98-regression-problem"><span class="nav-text">回归问题 (Regression problem)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-linear-regression"><span class="nav-text">线性回归 (Linear regression)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-loss-function"><span class="nav-text">损失函数 (Loss function)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1"><span class="nav-text">计算损失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent"><span class="nav-text">梯度下降法 (Gradient descent)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="nav-text">计算梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-backward-propagation"><span class="nav-text">反向传播 (Backward
propagation)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0neural-network-overview"><span class="nav-text">神经网络概述（Neural
Network Overview）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%A0%B7%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96vectorizing-across-multiple-examples"><span class="nav-text">多样本向量化（Vectorizing
across multiple examples ）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0activation-functions"><span class="nav-text">激活函数（Activation
functions）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cdeep-neural-networks"><span class="nav-text">深层神经网络(Deep Neural
Networks)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cdeep-l-layer-neural-network"><span class="nav-text">深层神经网络（Deep
L-layer neural network）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-text">正向传播过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97building-blocks-of-deep-neural-networks"><span class="nav-text">搭建神经网络块（Building
blocks of deep neural networks）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joey"
      src="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <p class="site-author-name" itemprop="name">Joey</p>
  <div class="site-description" itemprop="description">A Humble Apprentice</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hlYWRtYXN0ZXJFZ2d5" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HeadmasterEggy"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmhlYWRtYXN0ZXJlZ2d5QGdtYWlsLmNvbQ==" title="E-Mail → mailto:headmastereggy@gmail.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly94LmNvbS9IZWFkbWFzdGVyRWdneQ==" title="X → https:&#x2F;&#x2F;x.com&#x2F;HeadmasterEggy"><i class="fab fa-x-twitter fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9pbnN0YWdyYW0uY29tL2hhYml0YXR1bmljb3Ju" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;habitatunicorn"><i class="fab fa-instagram fa-fw"></i></span>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/fd0f6586.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
      <meta itemprop="name" content="Joey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joey">
      <meta itemprop="description" content="A Humble Apprentice">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DeepLearning | Joey">
      <meta itemprop="description" content="Deep learning Notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DeepLearning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-30 10:33:32" itemprop="dateCreated datePublished" datetime="2023-10-30T10:33:32Z">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-02-15 09:36:58" itemprop="dateModified" datetime="2025-02-15T09:36:58Z">2025-02-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

            <div class="post-description">Deep learning Notes</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>#deepLearning/notes</p>
<h1 id="回归问题-regression-problem">回归问题 (Regression problem)</h1>
<h2 id="线性回归-linear-regression">线性回归 (Linear regression)</h2>
<p>神经元输入向量<span
class="math inline">\(x=[x_{1},x_{2},…,x_{n}]^T\)</span>,
经过函数映射<span
class="math inline">\(f_{θ}：x-&gt;y\)</span>后得到输出y,
其中θ为函数f自身的参数。一种简单的情形是线性变换：<span
class="math inline">\(f(x)=w^Tx+b\)</span></p>
<figure>
<img data-src="https://images-a2q.pages.dev/file/9174b48689e1c393040c2.png"
alt="神经元数学模型" />
<figcaption aria-hidden="true">神经元数学模型</figcaption>
</figure>
<p><span class="math inline">\(w\)</span>称为权重
(weight)，权重决定了每个特征对我们预测值的影响。 <span
class="math inline">\(b\)</span>称为偏置(bias)、偏移量 (offset) 或截距
(intercept) 。 偏置是指当所有特征都取值为0时，预测值应该为多少。</p>
<p>而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。
当我们的输入包含<span
class="math inline">\(d\)</span>个特征时，我们将预测结果<span
class="math inline">\(\hat{y}\)</span> （通常使用“尖角”符号表示<span
class="math inline">\(y\)</span>的估计值）表示为： <span
class="math display">\[\hat{y} = w_1  x_1 + ... + w_d  x_d + b\]</span>
将所有特征放到向量<span class="math inline">\(\mathbf{x} \in
\mathbb{R}^d\)</span>， 并将所有权重放到向量<span
class="math inline">\(\mathbf{w} \in \mathbb{R}^d\)</span>中，
我们可以用点积形式来简洁地表达模型： <span
class="math display">\[\hat{y} = \mathbf{w}^\top \mathbf{x} +
b\]</span></p>
<h2 id="损失函数-loss-function">损失函数 (Loss function)</h2>
<p>考虑对于任何采样点，都有可能
存在观测误差，我们假设观测误差变量𝜖属于均值为𝜇，方差为<span
class="math inline">\(𝜎^2\)</span> 的正态分布(Normal
Distribution，或高斯分布，Gaussian Distribution): <span
class="math inline">\({\mathcal{N}}(\mu,\sigma^{2})\)</span>，则采样到的样本符合:
<span class="math display">\[𝑦 = 𝑤𝑥 + 𝑏 + 𝜖,
𝜖~{\mathcal{N}}(\mu,\sigma^{2})\]</span>
一旦引入观测误差后，即使简单如线性模型，如果仅采样两个数据点，可能会带来较大估
计偏差。如图 2.4
所示，图中的数据点均带有观测误差，如果基于蓝色矩形块的两个数据
点进行估计，则计算出的蓝色虚线与真实橙色直线存在较大偏差。为了减少观测误差引入
的估计偏差，可以通过采样多组数据样本集合<span class="math inline">\(𝔻 =
\{(x^{(1)},y^{(1)}),\bigl(x^{(2)},y^{(2)}\bigr),\ldots,\bigl(x^{(n)},y^{(n)}\bigr)\}\)</span>然后找出一条“最好”的直线，使得它尽可能地
让所有采样点到该直线的误差(Error，或损失 Loss)之和最小。</p>
<figure>
<img data-src="https://images-a2q.pages.dev/file/38ab6dd9542b3cebfb979.png"
alt="带观测误差的估计模型" />
<figcaption aria-hidden="true">带观测误差的估计模型</figcaption>
</figure>
<p>求出当前模型的 所有采样点上的预测值<span
class="math inline">\(𝑤𝑥^{(𝑖)} + 𝑏\)</span>与真实值<span
class="math inline">\(𝑦^{(𝑖)}\)</span>之间的差的平方和作为总误差<span
class="math inline">\(\mathcal{L}\)</span>: <span
class="math display">\[{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}(w
x^{(i)}+b-y^{(i)})^{2}\]</span> 然后搜索一组参数<span
class="math inline">\(𝑤^∗, 𝑏^∗\)</span>使得<span
class="math inline">\(\mathcal{L}\)</span>最小，对应的直线就是我们要寻找的最优直线:
<span
class="math display">\[w_{i}^*,b^{\ast}=\arg\operatorname*{min}_{w,b}\frac{1}{n}\sum_{i=1}^{n}(w
x^{(i)}+b-y^{(i)})^{2}\]</span> 这种误差计算方法称为均方误差(Mean
Squared Error，简称 MSE)。</p>
<h2 id="计算损失">计算损失</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># y = wx + b  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_error_for_line_given_points</span>(<span class="params">b, w, points</span>):  </span><br><span class="line">    totalError = <span class="number">0</span>  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(points)):  </span><br><span class="line">        x = points[i, <span class="number">0</span>]  </span><br><span class="line">        y = points[i, <span class="number">1</span>]  </span><br><span class="line">        <span class="comment"># computer mean-squared-error  </span></span><br><span class="line">        totalError += (y - (w * x + b)) ** <span class="number">2</span>  </span><br><span class="line">    <span class="comment"># average loss for each point  </span></span><br><span class="line">    <span class="keyword">return</span> totalError / <span class="built_in">float</span>(<span class="built_in">len</span>(points))</span><br></pre></td></tr></table></figure>
<h2 id="梯度下降法-gradient-descent">梯度下降法 (Gradient descent)</h2>
<p>我们需要找到一组参数<span class="math inline">\(w^*\)</span>和<span
class="math inline">\(b^*\)</span>，使得ℒ最小。</p>
<p><strong>梯度下降算法</strong>（Gradient
Descent）是神经网络训练中最常用的优化算法，配合强大的图形处理芯片GPU(Graphics
Processing
Unit)的并行加速能力，非常适合优化海量数据的神经网络模型，自然也适合优化我们这里的神经元线性模型。</p>
<p>函数的梯度(Gradient)定义为函数对各个自变量的偏导数(Partial
Derivative)组成的向量。考虑 3 维函数<span class="math inline">\(𝑧 = 𝑓(𝑥,
𝑦)\)</span>，函数对自变量𝑥的偏导数记为<span
class="math inline">\({\frac{\partial z}{\partial x}}\)</span>,
函数对自变量<span class="math inline">\(𝑦\)</span>的偏导数记为<span
class="math inline">\({\frac{\partial z}{\partial
y}}\)</span>，则梯度<span class="math inline">\(∇𝑓\)</span>为向量<span
class="math inline">\(({\frac{\partial z}{\partial x}},{\frac{\partial
z}{\partial y}})\)</span>。</p>
<p>函数在各处的梯度方向<span
class="math inline">\(∇𝑓\)</span>总是指向函数值增
大的方向，那么梯度的反方向<span
class="math inline">\(−∇𝑓\)</span>应指向函数值减少的方向。利用这一性质，我们只需要
按照 <span class="math display">\[x^{\prime}=x-\eta\cdot\nabla
f\]</span> 来迭代更新<span
class="math inline">\(x^{\prime}\)</span>，就能获得越来越小的函数值，其中𝜂用来缩放梯度向量，一般设置为某较小的值，如
0.01、0.001 等。特别地，对于一维函数，上述向量形式可以退化成标量形式:
<span
class="math display">\[x^{\prime}=x-\eta\cdot{\frac{\mathrm{d}y}{\mathrm{d}x}}\]</span>
通过上式迭代更新<span
class="math inline">\(x^{\prime}\)</span>若干次，这样得到的<span
class="math inline">\(x^{\prime}\)</span>处的函数值<span
class="math inline">\(y^{\prime}\)</span>，总是更有可能比在<span
class="math inline">\(𝑥\)</span>处的函数值<span
class="math inline">\(𝑦\)</span>小。
通过上式优化参数的方法称为梯度下降算法，它通过循环计算函数的梯度<span
class="math inline">\(∇𝑓\)</span>并 更新待优化参数<span
class="math inline">\(𝜃\)</span>，从而得到函数<span
class="math inline">\(𝑓\)</span>获得极小值时参数<span
class="math inline">\(𝜃\)</span>的最优数值解。</p>
<p>需要优化的模型参数是𝑤和𝑏，因此我们按照下面方式循环更新参数。 <span
class="math display">\[w^{\prime}=w-\eta{\frac{\partial\mathcal{L}}{\partial
w}}\]</span> <span
class="math display">\[b^{\prime}=b-\eta{\frac{\partial{\mathcal{L}}}{\partial
b}}\]</span></p>
<h2 id="计算梯度">计算梯度</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">step_gradient</span>(<span class="params">b_current, w_current, points, learningRate</span>):  </span><br><span class="line">    b_gradient = <span class="number">0</span>  </span><br><span class="line">    w_gradient = <span class="number">0</span>  </span><br><span class="line">    N = <span class="built_in">float</span>(<span class="built_in">len</span>(points))  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(points)):  </span><br><span class="line">        x = points[i, <span class="number">0</span>]  </span><br><span class="line">        y = points[i, <span class="number">1</span>]  </span><br><span class="line">        <span class="comment"># grad_b = 2(wx+b-y)  </span></span><br><span class="line">        b_gradient += (<span class="number">2</span> / N) * ((w_current * x + b_current) - y)  </span><br><span class="line">        <span class="comment"># grad_w = 2(wx+b-y)*x  </span></span><br><span class="line">        w_gradient += (<span class="number">2</span> / N) * x * ((w_current * x + b_current) - y)  </span><br><span class="line">    <span class="comment"># update w&#x27;  </span></span><br><span class="line">    new_b = b_current - (learningRate * b_gradient)  </span><br><span class="line">    new_w = w_current - (learningRate * w_gradient)  </span><br><span class="line">    <span class="keyword">return</span> [new_b, new_w]</span><br></pre></td></tr></table></figure>
<h2 id="反向传播-backward-propagation">反向传播 (Backward
propagation)</h2>
<p>逻辑回归的推导:</p>
<p><span class="math display">\[\left.
\begin{matrix}
{x} \\
{w} \\
{b} \\
\end{matrix}
\right\}\Longrightarrow\mathrm{~}z=w^{T}x+b\implies\alpha=\sigma(z)\mathrm{~}\Longrightarrow
L\left(a,y\right)\]</span> 正向传播步骤:计算<span
class="math inline">\(z^{[1]}, a^{[1]}\)</span>，然后<span
class="math inline">\(z^{[2]},a^{[2]}\)</span>，然后损失函数<span
class="math inline">\(L\)</span></p>
<p>$$_{d{w}=d{z} x,d{b}=d{z}}</p>
<p>_{d{z}=d{a}g^{'}(z),g(z)!=!(z)!, !=!,,,g!(z)!=!g^{'}(z)}</p>
<pre><code>\underbrace&#123;\alpha=\sigma(z)\mathrm&#123;~&#125;\Longrightarrow L\left(a,y\right)&#125;_&#123;d a=&#123;\frac&#123;d&#125;&#123;d a&#125;&#125;\,L&#123;\big(&#125;a,y&#123;\big)&#125;=\left(-y\log\alpha-&#123;\big(&#125;1-y&#123;\big)&#125;\log&#123;\big(&#125;1-a&#123;\big)&#125;\right)^&#123;\prime&#125;=-\,&#123;\frac&#123;y&#125;&#123;a&#125;&#125;+&#123;\frac&#123;1-y&#125;&#123;1-a&#125;&#125;&#125;$$</code></pre>
<p>前向传播： 计算<span class="math inline">\(z^{[1]}\)</span>, <span
class="math inline">\(a^{[1]}\)</span>, 再计算<span
class="math inline">\(z^{[2]}\)</span>, <span
class="math inline">\(a^{[2]}\)</span>，，最后得到<strong>loss
function</strong>。</p>
<p>反向传播： <span class="math display">\[dz^{[2]}=a^{[2]}-Y\]</span>
<span
class="math display">\[dW^{[2]}=\frac{1}{m}dz^{[2]}a^{[1]T}\]</span>
<span class="math display">\[{\cal
L}\,=\,\frac{1}{m}\sum_{i}^{n}\,L(\hat{y},y)\]</span> <span
class="math display">\[db^{\left[2\right]}=\frac{1}{m}np.sum(dZ^{\left[2\right]},axis=1,keepdims=True)\]</span>
<span class="math display">\[d Z^{[1]}=W^{[2]T}d
Z^{[2]}*g^{[1]\prime}(Z^{[1]})\]</span> <span class="math display">\[d
W^{[1]}=\frac{1}{m}d Z^{[1]}X^{T}\]</span> <span
class="math display">\[db^{\left[1\right]}=\frac{1}{m}np.sum(dZ^{\left[1\right]},axis=1,keepdims=True)\]</span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line"><span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">dZ2 = A2 - Y</span><br><span class="line">dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">db2 = np.<span class="built_in">sum</span>(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) / m</span><br><span class="line">dZ1 = np.multiply(np.dot(W2.T,dZ2), <span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">db1 = np.<span class="built_in">sum</span>(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) / m</span><br></pre></td></tr></table></figure> # 浅层神经网络(Shallow neural networks)</p>
<h2 id="神经网络概述neural-network-overview">神经网络概述（Neural
Network Overview）</h2>
<p><img data-src="https://images-a2q.pages.dev/file/b2d542e347cf2cc39f231.png" /> ##
神经网络的表示（Neural Network Representation ）</p>
<p>单隐藏层神经网络就是典型的<strong>浅层（shallow）神经网络</strong>
<img data-src="https://images-a2q.pages.dev/file/44c39b518a7859eb0c1b4.png" />
单隐藏层神经网络也被称为两层神经网络（2 layer NN)</p>
<p>第<span class="math inline">\(l\)</span>层的权重<span
class="math inline">\(W^{[l]}\)</span>叫维度的行等于<span
class="math inline">\(l\)</span>层神经元的个数，列等于<span
class="math inline">\(l-1\)</span>层神经元的个数; 第<span
class="math inline">\(i\)</span>层常数项维度的行等于<span
class="math inline">\(I\)</span>层神经元的个数，列始终为1 ##
计算一个神经网络的输出（Computing a Neural Network's output ）</p>
<p>两层神经网络可以看成是逻辑回归再重复计算一次</p>
<p>逻辑回归的正向计算可以分解成计算<span
class="math inline">\(z\)</span>和<span
class="math inline">\(a\)</span>的两部分:</p>
<p><span class="math display">\[z=w^{T}x+b\]</span>​</p>
<p><span class="math display">\[ a=\sigma(z)\]</span>​</p>
<p><img data-src="https://images-a2q.pages.dev/file/fd3123927499b9be3ce3a.png" /></p>
<p>两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算
<span class="math display">\[z^{[1]}=W^{[1]}x+b^{[1]}\]</span></p>
<p><span class="math display">\[a^{[1]}\ =\sigma(z^{[1]})\]</span> <span
class="math display">\[z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\]</span> <span
class="math display">\[a^{[2]}\ =\sigma(z^{[2]})\]</span> <img data-src="https://images-a2q.pages.dev/file/4199a79abc3ff39b9fd63.png" /></p>
<h2
id="多样本向量化vectorizing-across-multiple-examples">多样本向量化（Vectorizing
across multiple examples ）</h2>
<p>矩阵运算的形式： <span
class="math display">\[{Z}^{[1]}=W^{[1]}{X}+b^{[1]}\]</span> <span
class="math display">\[A^{(1)}=\sigma(Z^{(1)})\]</span> <span
class="math display">\[Z^{(2)}=W^{[2]}A^{[1]}+b^{[2]}\]</span> <span
class="math display">\[A^{(2)}=\sigma(Z^{(2)})\]</span>
行表示神经元个数，列表示样本数目 <code>m</code></p>
<h2 id="激活函数activation-functions">激活函数（Activation
functions）</h2>
<p><strong>sigmoid函数</strong> <img data-src="https://images-a2q.pages.dev/file/64348a5915737ee1ef279.png"
alt="sigmoid" /></p>
<p><strong>tanh函数</strong> <img data-src="https://images-a2q.pages.dev/file/10a17d6e466f202f8c9d0.png"
alt="tanh" /></p>
<p><strong>ReLU函数</strong></p>
<figure>
<img data-src="https://images-a2q.pages.dev/file/4d1600154f68271da5b8a.png"
alt="ReLU" />
<figcaption aria-hidden="true">ReLU</figcaption>
</figure>
<p><strong>Leaky ReLU函数</strong> <img data-src="https://images-a2q.pages.dev/file/dd0074adc62cc76609d55.png"
alt="Leaky ReLU" /></p>
<p>对于隐藏层的激活函数，<span
class="math inline">\(tanh\)</span>函数要比<span
class="math inline">\(sigmoid\)</span>函数表现更好一些。因为<span
class="math inline">\(tanh\)</span>函数的取值范围在<span
class="math inline">\([-1,+1]\)</span>之间，隐藏层的输出被限定在<span
class="math inline">\([-1,+1]\)</span>之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化(均值为0)的效果。</p>
<p>对于输出层的激活函数，因为二分类问题的输出取值为<span
class="math inline">\({0,+1}\)</span>，所以一般会选择<span
class="math inline">\(sigmoid\)</span>作为激活函数选择<span
class="math inline">\(ReLU\)</span>作为激活函数能够保证<span
class="math inline">\(x\)</span>大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当<span
class="math inline">\(z\)</span>小于零时，存在梯度为0的缺点</p>
<p><span class="math inline">\(Leaky
ReLU\)</span>激活函数，能够保证<span
class="math inline">\(z\)</span>小于零时梯度不为0</p>
<h1 id="深层神经网络deep-neural-networks">深层神经网络(Deep Neural
Networks)</h1>
<h2 id="深层神经网络deep-l-layer-neural-network">深层神经网络（Deep
L-layer neural network）</h2>
<p><img data-src="https://images-a2q.pages.dev/file/73fa3d3791d5956a7ab8c.png" /></p>
<p><span class="math inline">\(L−layer NN\)</span>，则包含了<span
class="math inline">\(L−1\)</span>个隐藏层，最后的<span
class="math inline">\(L\)</span>层是输出层 <span
class="math inline">\(a^{[l]}\)</span>和<span
class="math inline">\(W^{[l]}\)</span>中的上标<span
class="math inline">\(l\)</span>都是从<span
class="math inline">\(1\)</span>开始的，<span
class="math inline">\(l=1,⋯,L\)</span> 输<span
class="math inline">\(x\)</span>记为<span
class="math inline">\(a^{[0]}\)</span>, 把输出层<span
class="math inline">\({\hat{y}}\)</span>记为<span
class="math inline">\(a^{[L]}\)</span> <img data-src="https://images-a2q.pages.dev/file/a503f980795205f5e59d8.png" /> ##
前向传播和反向传播（Forward and backward propagation）</p>
<h3 id="正向传播过程">正向传播过程</h3>
<p><span
class="math display">\[z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\]</span> <span
class="math display">\[a^{[l]}=g^{[l]}(z^{[l]})\]</span>
<code>m</code>个训练样本，向量化形式为: <span
class="math display">\[Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\]</span> <span
class="math display">\[A^{[l]}=g^{[l]}(z^{[l]})\]</span> ###
反向传播过程 <span class="math display">\[d z^{[l]}=d
a^{[l]}*g^{[l]^{&#39;}}(z^{[l]})\]</span> <span class="math display">\[d
W^{[l]}=d z^{[l]}\cdot a^{[l-1]^{T}}\]</span> <span
class="math display">\[d b^{[l]}=d z^{[l]}\]</span> <span
class="math display">\[d a^{[l-1]}=W^{[l]T}\cdot d z^{[l]}\]</span>
得到： <span class="math display">\[d z^{[l]}=W^{[l+1]T}\cdot d
z^{[l+1]}\ast
g^{[l]&#39;}(z^{[l]})\]</span><code>m</code>个训练样本，向量化形式为:
<span class="math display">\[d Z^{[l]}=d
A^{[l]}*g^{[l]^{\prime}}(Z^{[l]})\]</span> <span
class="math display">\[d W^{[l]}=\frac{1}{m}d Z^{[l]}\cdot
A^{[l-1]T}\]</span> <span class="math display">\[d b^{[l]}=\frac{1}{m}n
p.s u m(d Z^{[l]},a x i s=1,k e e p d i m=T r u e)\]</span> <span
class="math display">\[d A^{\left[l-1\right]}=W^{\left[l\right]T}\cdot d
Z^{\left[l\right]}\]</span> <span class="math display">\[d
Z^{[l]}=W^{[l+1]T}\cdot d Z^{[l+1]}\ast
g^{[l]^{\prime}}(Z^{[l]})\]</span> <img data-src="https://images-a2q.pages.dev/file/f81a09bf629f0471a7a22.png" /> ##
层网络中的前向传播（Forward propagation in a Deep Network ）</p>
<p>对于第<span class="math inline">\(l\)</span>层，其正向传播过程的<span
class="math inline">\(Z^{[l]}\)</span>和<span
class="math inline">\(A^{[l]}\)</span>可以表示为： <span
class="math display">\[Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\]</span> <span
class="math display">\[A^{[l]}=g^{[l]}(Z^{[l]})\]</span> 其中<span
class="math inline">\(l=1,\cdots,L\)</span></p>
<h2
id="搭建神经网络块building-blocks-of-deep-neural-networks">搭建神经网络块（Building
blocks of deep neural networks）</h2>
<p>第<span class="math inline">\(l\)</span>层的流程块图 <img data-src="https://images-a2q.pages.dev/file/4f95853c2f976f166748d.png" />
对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：
<img data-src="https://images-a2q.pages.dev/file/c0c4b6a69b6a54bf59b59.png" />
<img data-src="https://images-a2q.pages.dev/file/893d629538ac503b2309f.png" /> ##
参数 VS 超参数（Parameters vs Hyperparameters）</p>
<p>神经网络中的参数是<span class="math inline">\(W^{[l]}\)</span>和<span
class="math inline">\(b^{[l]}\)</span>
<strong>超参数</strong>则是例如学习速率<span
class="math inline">\(\alpha\)</span>，训练迭代次数<span
class="math inline">\(N\)</span>，神经网络层数<span
class="math inline">\(L\)</span>，各层神经元个数<span
class="math inline">\(n^{[l]}\)</span>，激活函数<span
class="math inline">\(g(z)\)</span>等
叫做超参数的原因是它们决定了参数<span
class="math inline">\(W^{[l]}\)</span>和<span
class="math inline">\(b^{[l]}\)</span>的值</p>
<p><strong>如何设置最优的超参数：</strong>
通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试<strong>cost
function</strong>随着迭代次数增加的变化，根据结果选择<strong>cost
function</strong>最小时对应的超参数值</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deepLearning/" rel="tag"><i class="fa fa-tag"></i> deepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/df2849ec.html" rel="prev" title="Java">
                  <i class="fa fa-angle-left"></i> Java
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/2362a8ea.html" rel="next" title="链表">
                  链表 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><span class="exturl" data-url="aHR0cHM6Ly9iZWlhbi5taWl0Lmdvdi5jbg==">浙ICP备2023011468号-1 </span>
      <img src="https://images-a2q.pages.dev/file/ab4ebf9b9723073c81a21.png" alt="">
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Joey</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">193k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">11:43</span>
  </span>
</div>
  <div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/posts/fd0f6586.html"}</script>
  <script src="/js/third-party/quicklink.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":false},"react":{"opacity":1},"log":false});</script></body>
</html>
