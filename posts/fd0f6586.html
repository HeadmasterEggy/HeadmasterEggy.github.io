<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="mask-icon" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,400,400italic,700,700italic%7CGrand+Hotel:300,300italic,400,400italic,700,700italic%7CUbuntu:300,300italic,400,400italic,700,700italic%7CJetBrains+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Deep learning Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning">
<meta property="og:url" content="http://example.com/posts/fd0f6586.html">
<meta property="og:site_name" content="Joey">
<meta property="og:description" content="Deep learning Notes">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images-a2q.pages.dev/file/9174b48689e1c393040c2.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/38ab6dd9542b3cebfb979.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/b2d542e347cf2cc39f231.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/44c39b518a7859eb0c1b4.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/fd3123927499b9be3ce3a.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/4199a79abc3ff39b9fd63.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/64348a5915737ee1ef279.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/10a17d6e466f202f8c9d0.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/4d1600154f68271da5b8a.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/dd0074adc62cc76609d55.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/73fa3d3791d5956a7ab8c.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/a503f980795205f5e59d8.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/f81a09bf629f0471a7a22.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/4f95853c2f976f166748d.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/c0c4b6a69b6a54bf59b59.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/893d629538ac503b2309f.png">
<meta property="article:published_time" content="2023-10-30T02:33:32.000Z">
<meta property="article:modified_time" content="2024-02-24T09:37:53.245Z">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="deepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images-a2q.pages.dev/file/9174b48689e1c393040c2.png">


<link rel="canonical" href="http://example.com/posts/fd0f6586.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/posts/fd0f6586.html","path":"posts/fd0f6586.html","title":"DeepLearning"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DeepLearning | Joey</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Joey</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">7</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">6</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">21</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98-Regression-problem"><span class="nav-text">回归问题 (Regression problem)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-Linear-regression"><span class="nav-text">线性回归 (Linear regression)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-function"><span class="nav-text">损失函数 (Loss function)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1"><span class="nav-text">计算损失</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-Gradient-descent"><span class="nav-text">梯度下降法 (Gradient descent)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="nav-text">计算梯度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-Backward-propagation"><span class="nav-text">反向传播 (Backward propagation)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Shallow-neural-networks"><span class="nav-text">浅层神经网络(Shallow neural networks)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0%EF%BC%88Neural-Network-Overview%EF%BC%89"><span class="nav-text">神经网络概述（Neural Network Overview）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%A1%A8%E7%A4%BA%EF%BC%88Neural-Network-Representation-%EF%BC%89"><span class="nav-text">神经网络的表示（Neural Network Representation ）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%88Computing-a-Neural-Network%E2%80%99s-output-%EF%BC%89"><span class="nav-text">计算一个神经网络的输出（Computing a Neural Network’s output ）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%A0%B7%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%88Vectorizing-across-multiple-examples-%EF%BC%89"><span class="nav-text">多样本向量化（Vectorizing across multiple examples ）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-functions%EF%BC%89"><span class="nav-text">激活函数（Activation functions）</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Deep-Neural-Networks"><span class="nav-text">深层神经网络(Deep Neural Networks)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Deep-L-layer-neural-network%EF%BC%89"><span class="nav-text">深层神经网络（Deep L-layer neural network）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Forward-and-backward-propagation%EF%BC%89"><span class="nav-text">前向传播和反向传播（Forward and backward propagation）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-text">正向传播过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-text">反向传播过程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%82%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88Forward-propagation-in-a-Deep-Network-%EF%BC%89"><span class="nav-text">层网络中的前向传播（Forward propagation in a Deep Network ）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9D%97%EF%BC%88Building-blocks-of-deep-neural-networks%EF%BC%89"><span class="nav-text">搭建神经网络块（Building blocks of deep neural networks）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E6%95%B0-VS-%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%88Parameters-vs-Hyperparameters%EF%BC%89"><span class="nav-text">参数 VS 超参数（Parameters vs Hyperparameters）</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joey"
      src="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <p class="site-author-name" itemprop="name">Joey</p>
  <div class="site-description" itemprop="description">A Humble Apprentice</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hlYWRtYXN0ZXJFZ2d5" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HeadmasterEggy"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmhlYWRtYXN0ZXJlZ2d5QGdtYWlsLmNvbQ==" title="E-Mail → mailto:headmastereggy@gmail.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9IZWFkbWFzdGVyRWdneQ==" title="X → https:&#x2F;&#x2F;twitter.com&#x2F;HeadmasterEggy"><i class="fab fa-x-twitter fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9pbnN0YWdyYW0uY29tL2VnZ3lvbGRnb29zZQ==" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;eggyoldgoose"><i class="fab fa-instagram fa-fw"></i></span>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/fd0f6586.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
      <meta itemprop="name" content="Joey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joey">
      <meta itemprop="description" content="A Humble Apprentice">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DeepLearning | Joey">
      <meta itemprop="description" content="Deep learning Notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DeepLearning
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-30 10:33:32" itemprop="dateCreated datePublished" datetime="2023-10-30T10:33:32+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-24 17:37:53" itemprop="dateModified" datetime="2024-02-24T17:37:53+08:00">2024-02-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>2.8k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>10 mins.</span>
    </span>
</div>

            <div class="post-description">Deep learning Notes</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="回归问题-Regression-problem"><a href="#回归问题-Regression-problem" class="headerlink" title="回归问题 (Regression problem)"></a>回归问题 (Regression problem)</h1><h2 id="线性回归-Linear-regression"><a href="#线性回归-Linear-regression" class="headerlink" title="线性回归 (Linear regression)"></a>线性回归 (Linear regression)</h2><p>神经元输入向量$x&#x3D;[x_{1},x_{2},…,x_{n}]^T$,  经过函数映射$f_{θ}：x-&gt;y$后得到输出y, 其中θ为函数f自身的参数。一种简单的情形是线性变换：$f(x)&#x3D;w^Tx+b$</p>
<p><img data-src="https://images-a2q.pages.dev/file/9174b48689e1c393040c2.png" alt="神经元数学模型"></p>
<p>$w$称为权重 (weight)，权重决定了每个特征对我们预测值的影响。 $b$称为偏置(bias)、偏移量 (offset) 或截距 (intercept) 。 偏置是指当所有特征都取值为0时，预测值应该为多少。</p>
<p>而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含$d$个特征时，我们将预测结果$\hat{y}$ （通常使用“尖角”符号表示$y$的估计值）表示为：<br>$$\hat{y} &#x3D; w_1  x_1 + … + w_d  x_d + b$$<br>将所有特征放到向量$\mathbf{x} \in \mathbb{R}^d$， 并将所有权重放到向量$\mathbf{w} \in \mathbb{R}^d$中， 我们可以用点积形式来简洁地表达模型：<br>$$\hat{y} &#x3D; \mathbf{w}^\top \mathbf{x} + b$$</p>
<h2 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数 (Loss function)"></a>损失函数 (Loss function)</h2><p>考虑对于任何采样点，都有可能 存在观测误差，我们假设观测误差变量𝜖属于均值为𝜇，方差为$𝜎^2$ 的正态分布(Normal Distribution，或高斯分布，Gaussian Distribution): ${\mathcal{N}}(\mu,\sigma^{2})$，则采样到的样本符合:<br>$$𝑦 &#x3D; 𝑤𝑥 + 𝑏 + 𝜖, 𝜖~{\mathcal{N}}(\mu,\sigma^{2})$$<br>一旦引入观测误差后，即使简单如线性模型，如果仅采样两个数据点，可能会带来较大估 计偏差。如图 2.4 所示，图中的数据点均带有观测误差，如果基于蓝色矩形块的两个数据 点进行估计，则计算出的蓝色虚线与真实橙色直线存在较大偏差。为了减少观测误差引入 的估计偏差，可以通过采样多组数据样本集合$𝔻 &#x3D; {(x^{(1)},y^{(1)}),\bigl(x^{(2)},y^{(2)}\bigr),\ldots,\bigl(x^{(n)},y^{(n)}\bigr)}$然后找出一条“最好”的直线，使得它尽可能地 让所有采样点到该直线的误差(Error，或损失 Loss)之和最小。</p>
<p><img data-src="https://images-a2q.pages.dev/file/38ab6dd9542b3cebfb979.png" alt="带观测误差的估计模型"></p>
<p>求出当前模型的 所有采样点上的预测值$𝑤𝑥^{(𝑖)} + 𝑏$与真实值$𝑦^{(𝑖)}$之间的差的平方和作为总误差$\mathcal{L}$:<br>$${\mathcal{L}}&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^{n}(w x^{(i)}+b-y^{(i)})^{2}$$<br>然后搜索一组参数$𝑤^∗, 𝑏^∗$使得$\mathcal{L}$最小，对应的直线就是我们要寻找的最优直线:<br>$$w_{i}^*,b^{\ast}&#x3D;\arg\operatorname*{min}<em>{w,b}\frac{1}{n}\sum</em>{i&#x3D;1}^{n}(w x^{(i)}+b-y^{(i)})^{2}$$<br>这种误差计算方法称为均方误差(Mean Squared Error，简称 MSE)。</p>
<h2 id="计算损失"><a href="#计算损失" class="headerlink" title="计算损失"></a>计算损失</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># y = wx + b  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_error_for_line_given_points</span>(<span class="params">b, w, points</span>):  </span><br><span class="line">    totalError = <span class="number">0</span>  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(points)):  </span><br><span class="line">        x = points[i, <span class="number">0</span>]  </span><br><span class="line">        y = points[i, <span class="number">1</span>]  </span><br><span class="line">        <span class="comment"># computer mean-squared-error  </span></span><br><span class="line">        totalError += (y - (w * x + b)) ** <span class="number">2</span>  </span><br><span class="line">    <span class="comment"># average loss for each point  </span></span><br><span class="line">    <span class="keyword">return</span> totalError / <span class="built_in">float</span>(<span class="built_in">len</span>(points))</span><br></pre></td></tr></table></figure>

<h2 id="梯度下降法-Gradient-descent"><a href="#梯度下降法-Gradient-descent" class="headerlink" title="梯度下降法 (Gradient descent)"></a>梯度下降法 (Gradient descent)</h2><p>我们需要找到一组参数$w^*$和$b^*$，使得ℒ最小。</p>
<p><strong>梯度下降算法</strong>（Gradient Descent）是神经网络训练中最常用的优化算法，配合强大的图形处理芯片GPU(Graphics Processing Unit)的并行加速能力，非常适合优化海量数据的神经网络模型，自然也适合优化我们这里的神经元线性模型。</p>
<p>函数的梯度(Gradient)定义为函数对各个自变量的偏导数(Partial Derivative)组成的向量。考虑 3 维函数$𝑧 &#x3D; 𝑓(𝑥, 𝑦)$，函数对自变量𝑥的偏导数记为${\frac{\partial z}{\partial x}}$, 函数对自变量$𝑦$的偏导数记为${\frac{\partial z}{\partial y}}$，则梯度$∇𝑓$为向量$({\frac{\partial z}{\partial x}},{\frac{\partial z}{\partial y}})$。</p>
<p>函数在各处的梯度方向$∇𝑓$总是指向函数值增 大的方向，那么梯度的反方向$−∇𝑓$应指向函数值减少的方向。利用这一性质，我们只需要 按照<br>$$x^{\prime}&#x3D;x-\eta\cdot\nabla f$$<br>来迭代更新$x^{\prime}$，就能获得越来越小的函数值，其中𝜂用来缩放梯度向量，一般设置为某较小的值，如 0.01、0.001 等。特别地，对于一维函数，上述向量形式可以退化成标量形式:<br>$$x^{\prime}&#x3D;x-\eta\cdot{\frac{\mathrm{d}y}{\mathrm{d}x}}$$<br>通过上式迭代更新$x^{\prime}$若干次，这样得到的$x^{\prime}$处的函数值$y^{\prime}$，总是更有可能比在$𝑥$处的函数值$𝑦$小。<br>通过上式优化参数的方法称为梯度下降算法，它通过循环计算函数的梯度$∇𝑓$并 更新待优化参数$𝜃$，从而得到函数$𝑓$获得极小值时参数$𝜃$的最优数值解。</p>
<p>需要优化的模型参数是𝑤和𝑏，因此我们按照下面方式循环更新参数。<br>$$w^{\prime}&#x3D;w-\eta{\frac{\partial\mathcal{L}}{\partial w}}$$<br>$$b^{\prime}&#x3D;b-\eta{\frac{\partial{\mathcal{L}}}{\partial b}}$$</p>
<h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">step_gradient</span>(<span class="params">b_current, w_current, points, learningRate</span>):  </span><br><span class="line">    b_gradient = <span class="number">0</span>  </span><br><span class="line">    w_gradient = <span class="number">0</span>  </span><br><span class="line">    N = <span class="built_in">float</span>(<span class="built_in">len</span>(points))  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(points)):  </span><br><span class="line">        x = points[i, <span class="number">0</span>]  </span><br><span class="line">        y = points[i, <span class="number">1</span>]  </span><br><span class="line">        <span class="comment"># grad_b = 2(wx+b-y)  </span></span><br><span class="line">        b_gradient += (<span class="number">2</span> / N) * ((w_current * x + b_current) - y)  </span><br><span class="line">        <span class="comment"># grad_w = 2(wx+b-y)*x  </span></span><br><span class="line">        w_gradient += (<span class="number">2</span> / N) * x * ((w_current * x + b_current) - y)  </span><br><span class="line">    <span class="comment"># update w&#x27;  </span></span><br><span class="line">    new_b = b_current - (learningRate * b_gradient)  </span><br><span class="line">    new_w = w_current - (learningRate * w_gradient)  </span><br><span class="line">    <span class="keyword">return</span> [new_b, new_w]</span><br></pre></td></tr></table></figure>

<h2 id="反向传播-Backward-propagation"><a href="#反向传播-Backward-propagation" class="headerlink" title="反向传播 (Backward propagation)"></a>反向传播 (Backward propagation)</h2><p>逻辑回归的推导:</p>
<p>$$\left.<br>\begin{matrix}<br>{x} \<br>{w} \<br>{b} \<br>\end{matrix}<br>\right}\Longrightarrow\mathrm{<del>}z&#x3D;w^{T}x+b\implies\alpha&#x3D;\sigma(z)\mathrm{</del>}\Longrightarrow L\left(a,y\right)$$<br>正向传播步骤:计算$z^{[1]}, a^{[1]}$，然后$z^{[2]},a^{[2]}$，然后损失函数$L$</p>
<p>$$\underbrace{\left.<br>\begin{matrix}<br>{x} \<br>{w} \<br>{b} \<br>\end{matrix}<br>\right}}_{d{w}&#x3D;d{z} \cdot x,d{b}&#x3D;d{z}}</p>
<p>\Longleftarrow\mathrm{~} </p>
<p>\underbrace{z&#x3D;w^{T}x+b}_{d{z}&#x3D;d{a}\cdot g^{‘}\left(z\right),g(z)!&#x3D;!\sigma(z)!,<br>\frac{d L}{d z}!&#x3D;!\frac{d L}{d a}\cdot\frac{d a}{d z},,\frac{d}{d z},g!\left(z\right)!&#x3D;!g^{‘}\left(z\right)}</p>
<p>\Longleftarrow\mathrm{~}</p>
<pre><code>\underbrace&#123;\alpha=\sigma(z)\mathrm&#123;~&#125;\Longrightarrow L\left(a,y\right)&#125;_&#123;d a=&#123;\frac&#123;d&#125;&#123;d a&#125;&#125;\,L&#123;\big(&#125;a,y&#123;\big)&#125;=\left(-y\log\alpha-&#123;\big(&#125;1-y&#123;\big)&#125;\log&#123;\big(&#125;1-a&#123;\big)&#125;\right)^&#123;\prime&#125;=-\,&#123;\frac&#123;y&#125;&#123;a&#125;&#125;+&#123;\frac&#123;1-y&#125;&#123;1-a&#125;&#125;&#125;$$
</code></pre>
<p>前向传播：<br>计算$z^{[1]}$, $a^{[1]}$, 再计算$z^{[2]}$, $a^{[2]}$，，最后得到<strong>loss function</strong>。</p>
<p>反向传播：<br>$$dz^{[2]}&#x3D;a^{[2]}-Y$$<br>$$dW^{[2]}&#x3D;\frac{1}{m}dz^{[2]}a^{[1]T}$$<br>$${\cal L},&#x3D;,\frac{1}{m}\sum_{i}^{n},L(\hat{y},y)$$<br>$$db^{\left[2\right]}&#x3D;\frac{1}{m}np.sum(dZ^{\left[2\right]},axis&#x3D;1,keepdims&#x3D;True)$$<br>$$d Z^{[1]}&#x3D;W^{[2]T}d Z^{[2]}*g^{[1]\prime}(Z^{[1]})$$<br>$$d W^{[1]}&#x3D;\frac{1}{m}d Z^{[1]}X^{T}$$<br>$$db^{\left[1\right]}&#x3D;\frac{1}{m}np.sum(dZ^{\left[1\right]},axis&#x3D;1,keepdims&#x3D;True)$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line"><span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">dZ2 = A2 - Y</span><br><span class="line">dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">db2 = np.<span class="built_in">sum</span>(dZ2, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) / m</span><br><span class="line">dZ1 = np.multiply(np.dot(W2.T,dZ2), <span class="number">1</span> - np.power(A1, <span class="number">2</span>))</span><br><span class="line">dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">db1 = np.<span class="built_in">sum</span>(dZ1, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>) / m</span><br></pre></td></tr></table></figure>
<h1 id="浅层神经网络-Shallow-neural-networks"><a href="#浅层神经网络-Shallow-neural-networks" class="headerlink" title="浅层神经网络(Shallow neural networks)"></a>浅层神经网络(Shallow neural networks)</h1><h2 id="神经网络概述（Neural-Network-Overview）"><a href="#神经网络概述（Neural-Network-Overview）" class="headerlink" title="神经网络概述（Neural Network Overview）"></a>神经网络概述（Neural Network Overview）</h2><p><img data-src="https://images-a2q.pages.dev/file/b2d542e347cf2cc39f231.png"></p>
<h2 id="神经网络的表示（Neural-Network-Representation-）"><a href="#神经网络的表示（Neural-Network-Representation-）" class="headerlink" title="神经网络的表示（Neural Network Representation ）"></a>神经网络的表示（Neural Network Representation ）</h2><p>单隐藏层神经网络就是典型的<strong>浅层（shallow）神经网络</strong><br><img data-src="https://images-a2q.pages.dev/file/44c39b518a7859eb0c1b4.png"><br>单隐藏层神经网络也被称为两层神经网络（2 layer NN)</p>
<p>第$l$层的权重$W^{[l]}$叫维度的行等于$l$层神经元的个数，列等于$l-1$层神经元的个数;<br>第$i$层常数项维度的行等于$I$层神经元的个数，列始终为1</p>
<h2 id="计算一个神经网络的输出（Computing-a-Neural-Network’s-output-）"><a href="#计算一个神经网络的输出（Computing-a-Neural-Network’s-output-）" class="headerlink" title="计算一个神经网络的输出（Computing a Neural Network’s output ）"></a>计算一个神经网络的输出（Computing a Neural Network’s output ）</h2><p>两层神经网络可以看成是逻辑回归再重复计算一次</p>
<p>逻辑回归的正向计算可以分解成计算$z$和$a$的两部分:</p>
<p>$$z&#x3D;w^{T}x+b$$​</p>
<p>$$ a&#x3D;\sigma(z)$$​</p>
<p><img data-src="https://images-a2q.pages.dev/file/fd3123927499b9be3ce3a.png"></p>
<p>两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算<br>$$z^{[1]}&#x3D;W^{[1]}x+b^{[1]}$$</p>
<p>$$a^{[1]}\ &#x3D;\sigma(z^{[1]})$$<br>$$z^{[2]}&#x3D;W^{[2]}a^{[1]}+b^{[2]}$$<br>$$a^{[2]}\ &#x3D;\sigma(z^{[2]})$$<br><img data-src="https://images-a2q.pages.dev/file/4199a79abc3ff39b9fd63.png"></p>
<h2 id="多样本向量化（Vectorizing-across-multiple-examples-）"><a href="#多样本向量化（Vectorizing-across-multiple-examples-）" class="headerlink" title="多样本向量化（Vectorizing across multiple examples ）"></a>多样本向量化（Vectorizing across multiple examples ）</h2><p>矩阵运算的形式：<br>$${Z}^{[1]}&#x3D;W^{[1]}{X}+b^{[1]}$$<br>$$A^{(1)}&#x3D;\sigma(Z^{(1)})$$<br>$$Z^{(2)}&#x3D;W^{[2]}A^{[1]}+b^{[2]}$$<br>$$A^{(2)}&#x3D;\sigma(Z^{(2)})$$<br>行表示神经元个数，列表示样本数目 <code>m</code></p>
<h2 id="激活函数（Activation-functions）"><a href="#激活函数（Activation-functions）" class="headerlink" title="激活函数（Activation functions）"></a>激活函数（Activation functions）</h2><p><strong>sigmoid函数</strong><br><img data-src="https://images-a2q.pages.dev/file/64348a5915737ee1ef279.png" alt="sigmoid"></p>
<p><strong>tanh函数</strong><br><img data-src="https://images-a2q.pages.dev/file/10a17d6e466f202f8c9d0.png" alt="tanh"></p>
<p><strong>ReLU函数</strong></p>
<p><img data-src="https://images-a2q.pages.dev/file/4d1600154f68271da5b8a.png" alt="ReLU"></p>
<p><strong>Leaky ReLU函数</strong><br><img data-src="https://images-a2q.pages.dev/file/dd0074adc62cc76609d55.png" alt="Leaky ReLU"></p>
<p>对于隐藏层的激活函数，$tanh$函数要比$sigmoid$函数表现更好一些。因为$tanh$函数的取值范围在$[-1,+1]$之间，隐藏层的输出被限定在$[-1,+1]$之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化(均值为0)的效果。</p>
<p>对于输出层的激活函数，因为二分类问题的输出取值为${0,+1}$，所以一般会选择$sigmoid$作为激活函数选择$ReLU$作为激活函数能够保证$x$大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当$z$小于零时，存在梯度为0的缺点</p>
<p>$Leaky ReLU$激活函数，能够保证$z$小于零时梯度不为0</p>
<h1 id="深层神经网络-Deep-Neural-Networks"><a href="#深层神经网络-Deep-Neural-Networks" class="headerlink" title="深层神经网络(Deep Neural Networks)"></a>深层神经网络(Deep Neural Networks)</h1><h2 id="深层神经网络（Deep-L-layer-neural-network）"><a href="#深层神经网络（Deep-L-layer-neural-network）" class="headerlink" title="深层神经网络（Deep L-layer neural network）"></a>深层神经网络（Deep L-layer neural network）</h2><p><img data-src="https://images-a2q.pages.dev/file/73fa3d3791d5956a7ab8c.png"></p>
<p>$L−layer NN$，则包含了$L−1$个隐藏层，最后的$L$层是输出层<br>$a^{[l]}$和$W^{[l]}$中的上标$l$都是从$1$开始的，$l&#x3D;1,⋯,L$<br>输$x$记为$a^{[0]}$, 把输出层${\hat{y}}$记为$a^{[L]}$<br><img data-src="https://images-a2q.pages.dev/file/a503f980795205f5e59d8.png"></p>
<h2 id="前向传播和反向传播（Forward-and-backward-propagation）"><a href="#前向传播和反向传播（Forward-and-backward-propagation）" class="headerlink" title="前向传播和反向传播（Forward and backward propagation）"></a>前向传播和反向传播（Forward and backward propagation）</h2><h3 id="正向传播过程"><a href="#正向传播过程" class="headerlink" title="正向传播过程"></a>正向传播过程</h3><p>$$z^{[l]}&#x3D;W^{[l]}a^{[l-1]}+b^{[l]}$$<br>$$a^{[l]}&#x3D;g^{[l]}(z^{[l]})$$<br><code>m</code>个训练样本，向量化形式为:<br>$$Z^{[l]}&#x3D;W^{[l]}A^{[l-1]}+b^{[l]}$$<br>$$A^{[l]}&#x3D;g^{[l]}(z^{[l]})$$</p>
<h3 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h3><p>$$d z^{[l]}&#x3D;d a^{[l]}*g^{[l]^{‘}}(z^{[l]})$$<br>$$d W^{[l]}&#x3D;d z^{[l]}\cdot a^{[l-1]^{T}}$$<br>$$d b^{[l]}&#x3D;d z^{[l]}$$<br>$$d a^{[l-1]}&#x3D;W^{[l]T}\cdot d z^{[l]}$$<br>得到：<br>$$d z^{[l]}&#x3D;W^{[l+1]T}\cdot d z^{[l+1]}\ast g^{[l]’}(z^{[l]})$$<code>m</code>个训练样本，向量化形式为:<br>$$d Z^{[l]}&#x3D;d A^{[l]}*g^{[l]^{\prime}}(Z^{[l]})$$<br>$$d W^{[l]}&#x3D;\frac{1}{m}d Z^{[l]}\cdot A^{[l-1]T}$$<br>$$d b^{[l]}&#x3D;\frac{1}{m}n p.s u m(d Z^{[l]},a x i s&#x3D;1,k e e p d i m&#x3D;T r u e)$$<br>$$d A^{\left[l-1\right]}&#x3D;W^{\left[l\right]T}\cdot d Z^{\left[l\right]}$$<br>$$d Z^{[l]}&#x3D;W^{[l+1]T}\cdot d Z^{[l+1]}\ast g^{[l]^{\prime}}(Z^{[l]})$$<br><img data-src="https://images-a2q.pages.dev/file/f81a09bf629f0471a7a22.png"></p>
<h2 id="层网络中的前向传播（Forward-propagation-in-a-Deep-Network-）"><a href="#层网络中的前向传播（Forward-propagation-in-a-Deep-Network-）" class="headerlink" title="层网络中的前向传播（Forward propagation in a Deep Network ）"></a>层网络中的前向传播（Forward propagation in a Deep Network ）</h2><p>对于第$l$层，其正向传播过程的$Z^{[l]}$和$A^{[l]}$可以表示为：<br>$$Z^{[l]}&#x3D;W^{[l]}A^{[l-1]}+b^{[l]}$$<br>$$A^{[l]}&#x3D;g^{[l]}(Z^{[l]})$$<br>其中$l&#x3D;1,\cdots,L$</p>
<h2 id="搭建神经网络块（Building-blocks-of-deep-neural-networks）"><a href="#搭建神经网络块（Building-blocks-of-deep-neural-networks）" class="headerlink" title="搭建神经网络块（Building blocks of deep neural networks）"></a>搭建神经网络块（Building blocks of deep neural networks）</h2><p>第$l$层的流程块图<br><img data-src="https://images-a2q.pages.dev/file/4f95853c2f976f166748d.png"><br>对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：<br><img data-src="https://images-a2q.pages.dev/file/c0c4b6a69b6a54bf59b59.png"><br><img data-src="https://images-a2q.pages.dev/file/893d629538ac503b2309f.png"></p>
<h2 id="参数-VS-超参数（Parameters-vs-Hyperparameters）"><a href="#参数-VS-超参数（Parameters-vs-Hyperparameters）" class="headerlink" title="参数 VS 超参数（Parameters vs Hyperparameters）"></a>参数 VS 超参数（Parameters vs Hyperparameters）</h2><p>神经网络中的参数是$W^{[l]}$和$b^{[l]}$<br><strong>超参数</strong>则是例如学习速率$\alpha$，训练迭代次数$N$，神经网络层数$L$，各层神经元个数$n^{[l]}$，激活函数$g(z)$等<br>叫做超参数的原因是它们决定了参数$W^{[l]}$和$b^{[l]}$的值</p>
<p><strong>如何设置最优的超参数：</strong><br>通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试<strong>cost function</strong>随着迭代次数增加的变化，根据结果选择<strong>cost function</strong>最小时对应的超参数值</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deepLearning/" rel="tag"><i class="fa fa-tag"></i> deepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/df2849ec.html" rel="prev" title="Java">
                  <i class="fa fa-angle-left"></i> Java
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/0.html" rel="next" title="">
                   <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><span class="exturl" data-url="aHR0cHM6Ly9iZWlhbi5taWl0Lmdvdi5jbg==">浙ICP备 2023011468号-1 </span>
      <img src="https://images-a2q.pages.dev/file/ab4ebf9b9723073c81a21.png" alt="">
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Joey</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">98k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">5:55</span>
  </span>
</div>
  <div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span>
  </div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js","integrity":"sha256-mm3Re3y7xlvh+yCD+l/Zs1d+PU0AEad93MkWvljfm/s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/posts/fd0f6586.html"}</script>
  <script src="/js/third-party/quicklink.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":false},"react":{"opacity":1},"log":false});</script></body>
</html>
