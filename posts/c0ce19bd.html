<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="mask-icon" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Grand+Hotel:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Ubuntu:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=JetBrains+Mono:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css" integrity="sha256-dABdfBfUoC8vJUBOwGVdm8L9qlMWaHTIfXt+7GnZCIo=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.22.0","exturl":true,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Word2vec Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="Word2Vec">
<meta property="og:url" content="http://example.com/posts/c0ce19bd.html">
<meta property="og:site_name" content="Joey">
<meta property="og:description" content="Word2vec Notes">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images-a2q.pages.dev/file/7a1e78633b697eed40588.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/2ee900a0cfb63b3ded0a4.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/b68a6e26d650f6b050f48.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/060a94c685f8bd1d4b60a.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/802a54ee77c9c6b741098.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/aec140283a7c73f4bf244.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/fba7e46a42e92ce4960df.jpg">
<meta property="og:image" content="https://images-a2q.pages.dev/file/2e1e4d379e665542a6a98.jpg">
<meta property="og:image" content="https://images-a2q.pages.dev/file/c262b9a8d7e3bb017bd30.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/411c4d26a047cb63512a7.jpg">
<meta property="og:image" content="https://images-a2q.pages.dev/file/82c3f878c6f603891dd04.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/46a702ff685173b2f022f.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/0ca3402b95c8d65775f22.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/589e9a4fcfa426dd6546a.png">
<meta property="article:published_time" content="2024-04-01T08:14:40.000Z">
<meta property="article:modified_time" content="2024-07-02T15:12:58.966Z">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="#deepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images-a2q.pages.dev/file/7a1e78633b697eed40588.png">


<link rel="canonical" href="http://example.com/posts/c0ce19bd.html">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/posts/c0ce19bd.html","path":"posts/c0ce19bd.html","title":"Word2Vec"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Word2Vec | Joey</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Joey</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">6</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">34</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-text">预训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-text">语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-text">统计语言模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#word-embedding"><span class="nav-text">Word Embedding</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#word2vec"><span class="nav-text">Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#skip-gram"><span class="nav-text">Skip-gram</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rnn"><span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#lstm"><span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bert"><span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#nspnext-sentence-prediction"><span class="nav-text">NSP(Next Sentence Prediction)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mlmmasked-language-model"><span class="nav-text">MLM(Masked Language Model)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA"><span class="nav-text">输入表示</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bert-%E9%80%82%E5%BA%94%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1"><span class="nav-text">BERT 适应下游任务</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A5%E5%AF%B9%E5%88%86%E7%B1%BB"><span class="nav-text">句对分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E9%97%AE%E7%AD%94"><span class="nav-text">文本问答</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E5%8F%A5%E6%A0%87%E6%B3%A8"><span class="nav-text">单句标注</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joey"
      src="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <p class="site-author-name" itemprop="name">Joey</p>
  <div class="site-description" itemprop="description">A Humble Apprentice</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hlYWRtYXN0ZXJFZ2d5" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HeadmasterEggy"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmhlYWRtYXN0ZXJlZ2d5QGdtYWlsLmNvbQ==" title="E-Mail → mailto:headmastereggy@gmail.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly94LmNvbS9IZWFkbWFzdGVyRWdneQ==" title="X → https:&#x2F;&#x2F;x.com&#x2F;HeadmasterEggy"><i class="fab fa-x-twitter fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9pbnN0YWdyYW0uY29tL2hhYml0YXR1bmljb3Ju" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;habitatunicorn"><i class="fab fa-instagram fa-fw"></i></span>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/c0ce19bd.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
      <meta itemprop="name" content="Joey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joey">
      <meta itemprop="description" content="A Humble Apprentice">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Word2Vec | Joey">
      <meta itemprop="description" content="Word2vec Notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Word2Vec
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-01 16:14:40" itemprop="dateCreated datePublished" datetime="2024-04-01T16:14:40+08:00">2024-04-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-02 23:12:58" itemprop="dateModified" datetime="2024-07-02T23:12:58+08:00">2024-07-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>7.6k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>28 mins.</span>
    </span>
</div>

            <div class="post-description">Word2vec Notes</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>#deepLearning/word2vec</p>
<h1 id="预训练">预训练</h1>
<p>预训练的思想：任务 A 对应的模型 A
的参数不再是随机初始化的，而是通过任务 B 进行预先训练得到模型
B，然后利用模型 B 的参数对模型 A 进行初始化，再通过任务 A 的数据对模型 A
进行训练。注：模型 B 的参数是随机初始化的。</p>
<h1 id="语言模型">语言模型</h1>
<p>语言模型通俗点讲就是<strong>计算一个句子的概率</strong>。也就是说，对于语言序列 <span
class="math inline">\(w_1,w_2,\cdots,w_n\)</span>，语言模型就是计算该序列的概率，即 <span
class="math inline">\(P(w_1,w_2,\cdots,w_n)\)</span>。</p>
<p>下面通过两个实例具体了解上述所描述的意思：</p>
<ol type="1">
<li>假设给定两句话 <strong>"判断这个词的磁性"</strong> 和
<strong>"判断这个词的词性"</strong>
，语言模型会认为后者更自然。转化成数学语言也就是：<span
class="math display">\[P(判断，这个，词，的，词性) \gt
P(判断，这个，词，的，磁性)\]</span></li>
<li>假设给定一句话做填空
"**判断这个词的__**"，则问题就变成了给定前面的词，找出后面的一个词是什么，转化成数学语言就是：<span
class="math display">\[P(词性|判断，这个，词，的) \gt
P(磁性|判断，这个，词，的)\]</span></li>
</ol>
<p>通过上述两个实例，可以给出语言模型更加具体的描述：给定一句由 <span
class="math inline">\(n\)</span> 个词组成的句子 <span
class="math inline">\(W=w_1,w_2,\cdots,w_n\)</span>，计算这个句子的概率 <span
class="math inline">\(P(w_1,w_2,\cdots,w_n)\)</span>，或者计算根据上文计算下一个词的概率 <span
class="math inline">\(P(w_n|w_1,w_2,\cdots,w_{n-1})\)</span>。</p>
<p>下面将介绍语言模型的两个分支，统计语言模型和神经网络语言模型。</p>
<h2 id="统计语言模型">统计语言模型</h2>
<p>统计语言模型的基本思想就是<strong>计算条件概率</strong>。</p>
<p>给定一句由 <span
class="math inline">\(n\)</span> 个词组成的句子 <span
class="math inline">\(W=w_1,w_2,\cdots,w_n\)</span>，计算这个句子的概率 <span
class="math inline">\(P(w_1,w_2,\cdots,w_n)\)</span> 的公式如下（条件概率乘法公式的推广，链式法则）：</p>
<p><span class="math display">\[
\begin{align*}
P(w_1, w_2, \cdots, w_n) &amp;= P(w_1) \cdot P(w_2 \mid w_1) \cdot P(w_3
\mid w_1, w_2) \cdots P(w_n \mid w_1, w_2, \cdots, w_{n-1}) \\
&amp;= \prod_{i=1}^{n} P(w_i \mid w_1, w_2, \cdots, w_{i-1})
\end{align*}
\]</span></p>
<p>对于上一节提到的 “判断这个词的词性”
这句话，利用上述的公式，可以得到：</p>
<p><span class="math display">\[
\begin{align*}
&amp; P(判断，这个，词，的，词性) = \\
&amp; P(判断)P(这个|判断)P(词|判断，这个) \\
&amp;
P(的|判断，这个，词)P(词性|判断，这个，词，的)P(判断，这个，词，的，词性)
\end{align*}
\]</span></p>
<p>对于上一节提到的另外一个问题，当给定前面词的序列 “判断，这个，词，的”
时，想要知道下一个词是什么，可以直接计算如下概率：</p>
<p><span
class="math display">\[P(w_{next}|判断，这个，词，的)\]</span></p>
<p>其中，<span class="math inline">\(w_{next} \in
V\)</span>表示词序列的下一个词，<span
class="math inline">\(V\)</span> 是一个具有 <span
class="math inline">\(|V|\)</span> 个词的词典（词集合）。</p>
<p>对于公式（3），可以展开成如下形式：</p>
<p><span class="math display">\[P(w_{next}|判断，这个，词，的) =
\frac{count(w_{next}，判断，这个，词，的)}{count(判断，这个，词，的)}\]</span></p>
<p>对于公式（4），可以把字典 <span
class="math inline">\(V\)</span> 中的多有单词，逐一作为 <span
class="math inline">\(w_{next}\)</span>，带入计算，最后取最大概率的词作为 <span
class="math inline">\(w_{next}\)</span> 的候选词。</p>
<p>如果 <span
class="math inline">\(|V|\)</span> 特别大，公式（4）的计算将会非常困难，但是我们可以引入马尔科夫链的概念（当然，在这里只是简单讲讲如何做，关于马尔科夫链的数学理论知识可以自行查看其他参考资料）。</p>
<p>假设字典 V 中有 “火星” 一词，可以明显发现 “火星” 不可能出现在
“判断这个词的”
后面，因此（火星，判断，这个，词，的）这个组合是不存在的，并且词典中会存在很多类似于
“火星” 这样的词。</p>
<p>进一步，可以发现我们把（火星，判断，这个，词，的）这个组合判断为不存在，是因为
“火星” 不可能出现在 “词的”
后面，也就是说我们可以考虑是否把公式（3）转化为</p>
<p><span class="math display">\[
P(w_{next}|判断，这个，词，的) \approx P(w_{next}|词，的)
\]</span></p>
<p>公式（5）就是马尔科夫链的思想：假设 <span
class="math inline">\(w_{next}\)</span> 只和它之前的 <strong>k 个词有相关性</strong>，<span
class="math inline">\(k=1\)</span> 时称作一个单元语言模型，<span
class="math inline">\(k=2\)</span> 时称为二元语言模型。</p>
<p>可以发现通过马尔科夫链后改写的公式计算起来将会简单很多，下面我们举个简单的例子介绍下如何计算一个二元语言模型的概率。</p>
<p>其中二元语言模型的公式为：</p>
<p><span
class="math display">\[P(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\]</span></p>
<p>假设有一个文本集合：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">“词性是动词”</span><br><span class="line">“判断单词的词性”</span><br><span class="line">“磁性很强的磁铁”</span><br><span class="line">“北京的词性是名词”</span><br></pre></td></tr></table></figure>
<p>对于上述文本，如果要计算 P(词性|的) 的概率，通过公式（6），需要统计
“的，词性” 同时按序出现的次数，再除以 “的” 出现的次数：</p>
<p><span class="math display">\[
P(词性|的) = \frac{count(的，词性)}{count(的)} = \frac{2}{3}
\]</span></p>
<p>上述文本集合是我们自定制的，然而对于绝大多数具有现实意义的文本，会出现数据稀疏的情况，例如<strong>训练时未出现，测试时出现了的未登录单词</strong>。</p>
<p>由于数据稀疏问题，则会出现概率值为 0
的情况（填空题将无法从词典中选择一个词填入），为了避免 0
值的出现，会使用一种平滑的策略——分子和分母都加入一个非 0
正数，例如可以把公式（6）改为：</p>
<p><span class="math display">\[
P(w_i|w_{i-1}) = \frac{count(w_{i-1},w_i)+1}{count(w_{i-1})+|V|}
\]</span> # 独热编码(one-hot)</p>
<p>假设我们现在有单词数量为<span
class="math inline">\(𝑁\)</span>的词表，那可以生成一个长度为<span
class="math inline">\(𝑁\)</span>的向量来表示一个单词，在这个向量中该单词对应的位置数值为1，其余单词对应的位置数值全部为0。</p>
<p><strong>词典</strong>: [queen, king, man, woman, boy, girl]</p>
<figure>
<img data-src="https://images-a2q.pages.dev/file/7a1e78633b697eed40588.png"
alt="one-hot 编码图" />
<figcaption aria-hidden="true">one-hot 编码图</figcaption>
</figure>
<p>假设当前词典中有以上6个单词，图总展示了其中4个单词的one-hot编码表示。</p>
<p><strong>缺点</strong>
在<code>one-hot</code>编码中，每个单词被表示为一个很长的向量，这个向量的维度等于词汇表的大小，向量中只有一个元素是1（表示该单词），其余元素都是0。这种表示方法虽然简单，但是它无法捕捉单词之间的关系，而且随着词汇表的增大，向量的维度也会大幅增加，导致计算效率低下。</p>
<h1 id="word-embedding">Word Embedding</h1>
<p>单独的<code>one-hot</code>仅仅只代表了一个无意义的编码。通过词嵌入的方式，使得此单词可以被更多维度的特征所描述，而这些特征是在一个连续的向量空间中表示的。</p>
<p><code>word embedding</code>将每个单词表示为一个固定长度的稠密向量，语义相近的单词之间的距离在向量空间中会比较近，语义不同的单词之间距离会比较远。
<img data-src="https://images-a2q.pages.dev/file/2ee900a0cfb63b3ded0a4.png" />
可以看到<code>France, England, Italy</code>等国家之间比较近，并形成一个小簇；<code>dog, dogs,cat,cats</code>形成一个小簇。</p>
<h1 id="word2vec">Word2Vec</h1>
<p><code>Word2Vec</code>是建模了一个单词预测的任务，通过这个任务来学习词向量。假设有这样一句话<strong>Pineapples
are spiked and
yellow</strong>，现在假设spiked这个单词被删掉了，现在要预测这个位置原本的单词是什么。</p>
<p><code>Word2Vec</code>本身就是在建模这个单词预测任务，当这个单词预测任务训练完成之后，那每个单词对应的词向量也就训练好了。</p>
<p><code>Word2vec</code>是2013年被Mikolov提出来的词向量训练算法，在<span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvcGRmLzEzMDEuMzc4MS5wZGY=">论文<i class="fa fa-external-link-alt"></i></span>中作者提到了两种<code>word2vec</code>的具体实现方式：<strong>连续词袋模型CBOW</strong>和<strong>Skip-gram</strong>:</p>
<p><strong>CBOW(Continuous Bag of Words)</strong>:
根据上下文预测当前单词 <strong>Skip-gram</strong>:
根据当前单词预测上下文</p>
<p><img data-src="https://images-a2q.pages.dev/file/b68a6e26d650f6b050f48.png"
alt="CBOW和Skip-gram的对比" />
一般来说，<strong>CBOW</strong>比<strong>Skip-garm</strong>训练快且更加稳定一些，然而，<strong>Skip-garm</strong>不会刻意地回避生僻词（即出现频率比较低的词），比<strong>CBOW</strong>能够更好地处理生僻词。</p>
<h2 id="skip-gram">Skip-gram</h2>
<p>前边说到，<strong>Skip-gram</strong>是通过中心词来预测上下文。以<strong>Pineapples
are spiked and
yellow</strong>为例进行讲解，如<strong>图</strong>所示，中心词是spiked，上下文是Pineapples
are and
yellow，在<strong>Skip-gram</strong>中，上下文是我们要预测的词，因此这些词也叫目标词。
<img data-src="https://images-a2q.pages.dev/file/060a94c685f8bd1d4b60a.png"
alt="Skip-gram原理图" /> ## Skip-gram网络结构 <img data-src="https://images-a2q.pages.dev/file/802a54ee77c9c6b741098.png"
alt="Skip-gram网络结构" /></p>
<p><strong>Skip-gram</strong>的网络结构共包含三层：输入层，隐藏层和输出层。它的处理步骤是这样的：</p>
<ol type="1">
<li><p>输入层接收shape为 <span class="math inline">\([1,V]\)</span> 的
<span class="math inline">\(one-hot\)</span> 向量 <span
class="math inline">\(x\)</span>，其中 <span
class="math inline">\(V\)</span> 代表词表中单词的数量，这个 <span
class="math inline">\(one-hot\)</span>
向量就是上边提到的中心词。</p></li>
<li><p>隐藏层包含一个shape为 <span class="math inline">\([V,N]\)</span>
的参数矩阵 <span class="math inline">\(W_1\)</span>，其中这个 <span
class="math inline">\(N\)</span> 代表词向量的维度，<span
class="math inline">\(W_1\)</span> 就是word embedding
矩阵，即我们要学习的词向量。将输入的 <span
class="math inline">\(one-hot\)</span> 向量 <span
class="math inline">\(x\)</span> 与 <span
class="math inline">\(W_1\)</span> 相乘，便可得到一个shape为 <span
class="math inline">\([1, N]\)</span> 的向量，即该输入单词对应的词向量
<span class="math inline">\(e\)</span>。</p></li>
<li><p>输出层包含一个shape为 <span class="math inline">\([N,V]\)</span>
的参数矩阵 <span class="math inline">\(W_2\)</span>，将隐藏层输出的
<span class="math inline">\(e\)</span> 与 <span
class="math inline">\(W_2\)</span> 相乘，便可以得到shape为 <span
class="math inline">\([1,V]\)</span> 的向量 <span
class="math inline">\(r\)</span>，内部的数值分别代表每个候选词的打分，使用softmax函数，对这些打分进行归一化，即得到中心词的预测各个单词的概率。</p></li>
</ol>
<p>这是一种比较<strong>理想的</strong>实现方式，但是这里有两个问题：</p>
<ol type="1">
<li><p>这个输入向量是个one-hot编码的方式，只有一个元素为1，其他全是0，是个极其稀疏的向量，假设它第2个位置为1，它和word
embedding相乘，便可获得word
embedding矩阵的第二行的数据。那么我们知道这个规律，直接通过访存的方式直接获取就可以了，不需要进行矩阵相乘。</p></li>
<li><p>在获取了输入单词对应的词向量 <span
class="math inline">\(e\)</span> 后，它是一个 <span
class="math inline">\([1,N]\)</span>
向量。接下来，会使用这个向量和另外一个大的矩阵 <span
class="math inline">\(W_2\)</span> 进行相乘，最终会获得一个 <span
class="math inline">\(1*V\)</span>
的向量，然后对这个向量进行softmax，可以看到这个向量具有词表的长度，对这么长的向量进行softmax本身也是一个极其消耗资源的事情。
## 负采样解决大规模分类问题 <img data-src="https://images-a2q.pages.dev/file/aec140283a7c73f4bf244.png"
alt="使用负采样策略训练Skip-gram模型" />
如<strong>图</strong>所示，其中中心词是spiked和上下文词是正样本Pineapples
are and yellow，这里这个正样本代表该词是中心词的上下文。</p></li>
</ol>
<p>以正样本单词Pineapples为例，<strong>之前的做法</strong>是在使用softmax学习时，需要最大化Pineapples的推理概率，同时最小化其他词表中词的推理概率。之所以计算缓慢，是因为需要对词表中的所有词都计算一遍。<strong>然而</strong>我们还可以使用另一种方法，就是随机从词表中选择几个代表词，通过最小化这几个代表词的概率，去<strong>近似</strong>最小化整体的预测概率。</p>
<p>例如，先指定一个中心词（spiked）和一个目标词正样本（Pineapples），再随机在词表中采样几个目标词负样本（如<code>dog</code>，<code>house</code>等）。</p>
<p>有了这些正负样本，我们的skip-gram模型就变成了一个二分类任务。对于目标词正样本，我们需要最大化它的预测概率；对于目标词负样本，我们需要最小化它的预测概率。通过这种方式，我们就可以完成计算加速。这个做法就是<strong>负采样</strong>。</p>
<p>我们再回到<strong>图7</strong>看一看整体的训练流程是怎么样的。图7中相当于有两个词向量矩阵：黄色的和灰色的，他们的shape都是一样的。整体的流程大概是这样的。</p>
<ol type="1">
<li><p>获取中心词spiked的正负样本（正负样本是目标词），这里一般会设定个固定的窗口，比如中心词前后3个词算是中心词的上下文(即正样本)；</p></li>
<li><p>获取对应词的词向量，其中中心词从黄色的向量矩阵中获取词向量，目标词从灰色的向量矩阵中获取词向量。</p></li>
<li><p>将中心词和目标词的词向量进行点积并经过sigmoid函数，我们知道sigmoid是可以用于2分类的函数，通过这种方式来预测中心词和目标词是否具有上下文关系。</p></li>
<li><p>将预测的结果和标签使用交叉熵计算损失值，并计算梯度进行反向迭代，优化参数。</p></li>
</ol>
<p>经过这个训练的方式，我们就可以训练出我们想要的词向量，但<strong>图7</strong>中包含两个词向量矩阵（黄色的和灰色的），一般是将中心词对应的词向量矩阵(黄色的)作为正式训练出的词向量。</p>
<h1 id="rnn">RNN</h1>
<p>传统的神经网络无法获取时序信息，然而<strong>时序信息在自然语言处理任务中非常重要</strong>。</p>
<p>例如对于这一句话 “我吃了一个苹果”，“苹果”
的词性和意思，在这里取决于前面词的信息，如果没有 “我吃了一个”
这些词，“苹果” 也可以翻译为乔布斯搞出来的那个被咬了一口的苹果。</p>
<p><img data-src="https://images-a2q.pages.dev/file/fba7e46a42e92ce4960df.jpg" /></p>
<p>上图左边部分称作 RNN 的一个 timestep，在这个 timestep
中可以看到，在 𝑡 时刻，输入变量 <span
class="math inline">\(x_t\)</span>，通过 RNN 的一个基础模块
A，输出变量 <span class="math inline">\(h_t\)</span>，而 <span
class="math inline">\(t\)</span> 时刻的信息，将会传递到下一个时刻 <span
class="math inline">\(t+1\)</span>。</p>
<p>如果把模块按照时序展开，则会如上图右边部分所示，<strong>由此可以看到
RNN 为多个基础模块 A
的互连，每一个模块都会把当前信息传递给下一个模块</strong>。</p>
<p>RNN
解决了时序依赖问题，但这里的时序一般指的是短距离的，短距离依赖和长距离依赖的区别：</p>
<ul>
<li>短距离依赖：对于这个填空题 “我想看一场篮球___”，我们很容易就判断出
“篮球” 后面跟的是 “比赛”，这种短距离依赖问题非常适合 RNN。</li>
<li>长距离依赖：对于这个填空题
“我出生在中国的瓷都景德镇，小学和中学离家都很近，……，我的母语是___”，对于短距离依赖，“我的母语是”
后面可以紧跟着
“汉语”、“英语”、“法语”，但是如果我们想精确答案，则必须回到上文中很长距离之前的表述
“我出生在中国的瓷都景德镇”，进而判断答案为 “汉语”，而 RNN
是很难学习到这些信息的。</li>
</ul>
<h1 id="lstm">LSTM</h1>
<p>为了解决 RNN 缺乏的序列长距离依赖问题，LSTM 被提了出来，首先来看 LSTM
相对于 RNN 做了哪些改进： <img data-src="https://images-a2q.pages.dev/file/2e1e4d379e665542a6a98.jpg" />
如上图所示，LSTM 的 RNN 门控结构（LSTM 的 timestep），LSTM
前向传播过程包括：</p>
<ul>
<li><strong>遗忘门</strong>：决定了丢弃哪些信息，遗忘门接收 <span
class="math inline">\(t-1\)</span> 时刻的状态 <span
class="math inline">\(h_{t-1}\)</span>，以及当前的输入 <span
class="math inline">\(x_t\)</span>，经过 Sigmoid 函数后输出一个 0 到 1
之间的值 <span class="math inline">\(f_t\)</span>
<ul>
<li>输出： <span class="math inline">\(f_{t} = \sigma(W_fh_{t-1} +
U_fx_{t} + b_f)\)</span></li>
</ul></li>
<li><strong>输入门</strong>：决定了哪些新信息被保留，并更新细胞状态，输入门的取值由 <span
class="math inline">\(h_{t-1}\)</span> 和 <span
class="math inline">\(x_t\)</span> 决定，通过 Sigmoid 函数得到一个 0 到
1 之间的值 <span class="math inline">\(i_t\)</span>，而 <span
class="math inline">\(\tanh\)</span> 函数则创造了一个当前细胞状态的候选 <span
class="math inline">\(a_t\)</span>
<ul>
<li>输出：<span class="math inline">\(i_{t} = \sigma(W_ih_{t-1} +
U_ix_{t} + b_i) , \tilde{C_{t} }= tanhW_ah_{t-1} + U_ax_{t} +
b_a\)</span></li>
</ul></li>
<li><strong>细胞状态</strong>：旧细胞状态 <span
class="math inline">\(C_{t-1}\)</span> 被更新到新的细胞状态 <span
class="math inline">\(C_t\)</span> 上，
<ul>
<li>输出：<span class="math inline">\(C_{t} = C_{t-1}\odot f_{t} +
i_{t}\odot \tilde{C_{t} }\)</span></li>
</ul></li>
<li><strong>输出门</strong>：决定了最后输出的信息，输出门取值由 <span
class="math inline">\(h_{t-1}\)</span> 和 <span
class="math inline">\(x_t\)</span> 决定，通过 Sigmoid 函数得到一个 0 到
1 之间的值 <span class="math inline">\(o_t\)</span>，最后通过 <span
class="math inline">\(\tanh\)</span> 函数决定最后输出的信息
<ul>
<li>输出：<span class="math inline">\(o_{t} = \sigma(W_oh_{t-1} +
U_ox_{t} + b_o) , h_{t} = o_{t}\odot tanhC_{t}\)</span></li>
</ul></li>
<li><strong>预测输出：</strong><span class="math inline">\(\hat{y}_{t} =
\sigma(Vh_{t}+c)\)</span></li>
</ul>
<h1 id="bert">BERT</h1>
<p>BERT 全称为 Bidirectional Encoder Representations from
Transformers（来自 Transformers 的双向编码器表示），是谷歌发表的论文
<em>Pre-training of Deep Bidirectional Transformers for Language
Understanding</em>
中提出的一个面向自然语言处理任务的无监督预训练语言模型，是近年来自然语言处理领域公认的里程碑模型。</p>
<ul>
<li>BERT
的意义在于：从大量无标记数据集中训练得到的深度模型，可以<strong>显著</strong>提高各项自然语言处理任务的准确率。</li>
<li>BERT 被认为是近年来优秀预训练语言模型的集大成者，其参考了 ELMo
模型的双向编码思想，借鉴了 GPT 用 Transformer
作为特征提取器的思路，并采用了 word2vec 所使用的 CBOW 训练方法。</li>
<li>单向编码和双向编码的差异，例如
“今天天气很{}，我们不得不取消户外运动”，分别从单向编码和双向编码的角度去考虑
{} 中应该填什么词：
<ul>
<li>单向编码：单向编码只会考虑 “今天天气很”，以人类的经验，大概率会从
“好”、“不错”、“差”、“糟糕”
这几个词中选择，这些词可以被划为截然不同的两类</li>
<li>双向编码：<strong>双向编码会同时考虑上下文的信息</strong>，即除了会考虑
“今天天气很” 这五个字，还会考虑 “我们不得不取消户外运动”
来帮助模型判断，则大概率会从 “差”、“糟糕” 这一类词中选择</li>
</ul></li>
<li>不考虑模型的复杂度和训练数据量，双向编码与单向编码相比，可以利用更多的上下文信息来辅助当前词的语义判断。在语义理解能力上，采用双向编码的方式是最科学的，而BERT
的成功很大程度上由此决定。</li>
</ul>
<h2 id="nspnext-sentence-prediction">NSP(Next Sentence Prediction)</h2>
<p>处理两个句子之间的关系</p>
<p>NSP 的具体做法是，BERT 输入的语句将由两个句子构成，其中，50%
的概率将语义连贯的两个连续句子作为训练文本（<strong>连续句对一般选自篇章级别的语料，以此确保前后语句的语义强相关</strong>），另外
50% 的概率将完全随机抽取两个句子作为训练文本。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">连续句对：[CLS]今天天气很糟糕[SEP]下午的体育课取消了[SEP]</span><br><span class="line">随机句对：[CLS]今天天气很糟糕[SEP]鱼快被烤焦啦[SEP]</span><br></pre></td></tr></table></figure>
<p>[SEP]: nsp二分类中采用一个符号sep对两个句子进行一个隔断区分 [CLS]:
训练时，将 CLS 的输出向量接一个二分类器来做二分类任务</p>
<p>结果为 1，表示输入为连续句对；结果为 0，表示输入为随机句对。</p>
<p>通过训练 [CLS] 编码后的输出标签，<strong>BERT
可以学会捕捉两个输入句对的文本语义</strong>，在连续句对的预测任务中，BERT
的正确率可以达到 97%-98%。</p>
<h2 id="mlmmasked-language-model">MLM(Masked Language Model)</h2>
<p>基本原理是将输入句子中的某些单词随机掩盖（即用一个特殊的标记替换），然后让模型预测被掩盖的单词是什么。</p>
<ul>
<li>随机把一些子词用 Mask 代替</li>
<li>让模型去预测被 Mask 代替的子词</li>
</ul>
<p>例如:
<code>["&lt;CLS&gt;", "my", "dog", "is", "cute", "&lt;SEP&gt;", "he", "likes", "play", "##ing", "&lt;SEP&gt;"]</code></p>
<p>变为:
<code>["&lt;CLS&gt;", "my", "&lt;mask&gt;", "is", "dog", "&lt;SEP&gt;", "he", "likes", "play", "&lt;mask&gt;", "&lt;SEP&gt;"]</code></p>
<p>随机去掉的 token 被称作掩码词，在训练中，掩码词将以 15%
的概率被替换成 [MASK]，这个操作则称为掩码操作。注意：<strong>在CBOW
模型中，每个词都会被预测一遍。</strong></p>
<p>但是这样设计 MLM
的训练方法会引入弊端：<strong>在模型微调训练阶段或模型推理（测试）阶段，输入的文本中将没有
[MASK]，进而导致产生由训练和预测数据偏差导致的性能损失。</strong></p>
<p>考虑到上述的弊端，BERT 并没有总用 [MASK]
替换掩码词，而是按照一定比例选取替换词。在选择 15%
的词作为掩码词后这些掩码词有三类替换选项：</p>
<ul>
<li>80% 的训练样本中：将选中的词用 [MASK] 来代替，例如：
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">“地球是[MASK]八大行星之一”</span><br></pre></td></tr></table></figure></li>
<li>10%
的训练样本中：选中的词不发生变化，<strong>该做法是为了缓解训练文本和预测文本的偏差带来的性能损失</strong>，例如：
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">“地球是太阳系八大行星之一”</span><br></pre></td></tr></table></figure></li>
<li>10%
的训练样本中：将选中的词用任意的词来进行代替，<strong>该做法是为了让
BERT 学会根据上下文信息自动纠错</strong>，例如： <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">“地球是苹果八大行星之一”</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Mask代码 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> mlm_pred_position <span class="keyword">in</span> candidate_pred_positions:</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(pred_positions_and_labels) &gt;= num_mlm_preds:</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">	masked_token = <span class="literal">None</span></span><br><span class="line">	<span class="comment"># 80% of the time: replace the word with the &#x27;&lt;mask&gt;&#x27; token</span></span><br><span class="line">	<span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">		masked_token = <span class="string">&#x27;&lt;mask&gt;&#x27;</span></span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="comment"># 10% of the time: keep the word unchanged</span></span><br><span class="line">			<span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>: </span><br><span class="line">			masked_token = tokens[mlm_pred_position]</span><br><span class="line">		<span class="comment"># 10% of the time: replace the word with a random word</span></span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			masked_token = random.choice(vocab.idx_to_token)</span><br></pre></td></tr></table></figure></p>
<h2 id="输入表示">输入表示</h2>
<p>Bert 使用多个 encoder 堆叠在一起，其中 bert base 使用的是 12 层的
encoder，bert large 使用的是 24 层的 encoder。</p>
<p>由于 BERT 通过 Transformer 模型堆叠而成，所以 BERT 的输入需要两套
Embedding 操作：</p>
<ol type="1">
<li>一套为 One-hot 词表映射编码（Token Embeddings），在BERT中，One-hot
编码向量通过嵌入矩阵转换为密集的词嵌入向量。嵌入矩阵是一个学习参数矩阵，表示词汇表中的每个词汇的向量表示。通过预训练，这些向量能够捕捉到词汇的语义和上下文关系。</li>
<li>另一套为位置编码（Position Embeddings），<strong>不同于 Transformer
的位置编码用三角函数表示，BERT
的位置编码将在预训练过程中训练得到（训练思想类似于Word Embedding 的 Q
矩阵）</strong></li>
<li>由于在 MLM 的训练过程中，存在单句输入和双句输入的情况，因此 BERT
还需要一套区分输入语句的分割编码（Segment Embeddings），BERT
的分割编码也将在预训练过程中训练得到</li>
</ol>
<p><span class="math inline">\(\text{Input Representation} = \text{Token
Embeddings} + \text{Position Embeddings} + \text{Segment
Embeddings}\)</span></p>
<p><img data-src="https://images-a2q.pages.dev/file/c262b9a8d7e3bb017bd30.png" /></p>
<p>对于分割编码，Segment Embeddings 层只有两种向量表示。前一个向量 <span
class="math inline">\(E_A\)</span> 是把 0 赋给第一个句子中的各个
token，后一个向量 <span class="math inline">\(E_B\)</span> 是把 1
赋给第二个句子中的各个 token ；如果输入仅仅只有一个句子，那么它的
segment embedding 就是全 0，下面我们简单举个例子描述下： <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">[CLS]I like dogs[SEP]I like cats[SEP] 对应编码 0 0 0 0 0 1 1 1 1</span><br><span class="line">[SEP]I Iike dogs and cats[SEP] 对应编码 0 0 0 0 0 0 0</span><br></pre></td></tr></table></figure>
<img data-src="https://images-a2q.pages.dev/file/411c4d26a047cb63512a7.jpg" /></p>
<h1 id="bert-适应下游任务">BERT 适应下游任务</h1>
<p>BERT
根据自然语言处理下游任务的输入和输出的形式，将微调训练支持的任务分为四类，分别是句对分类、单句分类、文本问答和单句标注。</p>
<h2 id="句对分类">句对分类</h2>
<p>给定两个句子，判断它们的关系，称为句对分类，例如判断句对是否相似、判断后者是否为前者的答案。</p>
<p>针对句对分类任务，BERT 在预训练过程中就使用了 NSP
训练方法获得了直接捕获句对语义关系的能力。 针对二分类任务，BERT
不需要对输入数据和输出数据的结构做任何改动，直接使用与 NSP
训练方法一样的输入和输出结构就行。</p>
<p>句对用 [SEP] 分隔符拼接成文本序列，在句首加入标签
[CLS]，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练。
<img data-src="https://images-a2q.pages.dev/file/82c3f878c6f603891dd04.png" /></p>
<p>针对多分类任务，需要在句首标签 [CLS] 的输出特征向量后接一个全连接层和
Softmax 层，保证输出维数与类别数目一致，最后通过 arg max
操作（取最大值时对应的索引序号）得到相对应的类别结果。 <figure class="highlight text"><table><tr><td class="code"><pre><span class="line">任务：判断句子 “我很喜欢你” 和句子 “我很中意你” 是否相似</span><br><span class="line">输入改写：“[CLS]我很喜欢你[SEP]我很中意你”</span><br><span class="line">取 “[CLS]” 标签对应输出：[0.02, 0.98]</span><br><span class="line">通过 arg max 操作得到相似类别为 1（类别索引从 0 开始），即两个句子相似</span><br></pre></td></tr></table></figure> ##
单句分类
给定一个句子，判断该句子的类别，统称为单句分类，例如判断情感类别、判断是否为语义连贯的句子。</p>
<p>针对单句二分类任务，也无须对 BERT
的输入数据和输出数据的结构做任何改动。</p>
<p>单句分类在句首加入标签
[CLS]，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练。
<img data-src="https://images-a2q.pages.dev/file/46a702ff685173b2f022f.png" /></p>
<p>同样，针对多分类任务，需要在句首标签 [CLS]
的输出特征向量后接一个全连接层和 Softmax
层，保证输出维数与类别数目一致，最后通过 argmax
操作得到相对应的类别结果。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">任务：判断句子“海大球星饭茶吃” 是否为一句话</span><br><span class="line">输入改写：“[CLS]海大球星饭茶吃”</span><br><span class="line">取 “[CLS]” 标签对应输出：[0.99, 0.01]</span><br><span class="line">通过 arg max 操作得到相似类别为 0，即这个句子不是一个语义连贯的句子</span><br></pre></td></tr></table></figure>
<h2 id="文本问答">文本问答</h2>
<p>给定一个问句和一个蕴含答案的句子，找出答案在后者中的位置，称为文本问答，例如给定一个问题（句子
A），在给定的段落（句子 B）中标注答案的起始位置和终止位置。</p>
<p><strong>文本问答任何和前面讲的其他任务有较大的差别，无论是在优化目标上，还是在输入数据和输出数据的形式上，都需要做一些特殊的处理。</strong></p>
<p>为了标注答案的起始位置和终止位置，BERT 引入两个辅助向量 <span
class="math inline">\(s\)</span>（start，判断答案的起始位置） 和 <span
class="math inline">\(e\)</span>（end，判断答案的终止位置）。</p>
<p>BERT 判断句子 B 中答案位置的做法是，将句子 B
中的每一个词得到的最终特征向量 <span
class="math inline">\(T_i&#39;\)</span> 经过全连接层（利用全连接层将词的抽象语义特征转化为任务指向的特征）后，分别与向量
<span class="math inline">\(s\)</span> 和 <span
class="math inline">\(e\)</span> 求内积，对所有内积分别进行 softmax
操作，即可得到词 Tok <span class="math inline">\(m（m\in
[1,M]）\)</span>作为答案起始位置和终止位置的概率。最后，取概率最大的片段作为最终的答案。
<img data-src="https://images-a2q.pages.dev/file/0ca3402b95c8d65775f22.png" /></p>
<p>文本回答任务的微调训练使用了两个技巧：</p>
<ol type="1">
<li>用全连接层把 BERT
提取后的深层特征向量转化为用于判断答案位置的特征向量</li>
<li>引入辅助向量 <span class="math inline">\(s\)</span> 和 <span
class="math inline">\(e\)</span>
作为答案起始位置和终止位置的基准向量，明确优化目标的方向和度量方法</li>
</ol>
<blockquote>
<p>任务：给定问句 “今天的最高温度是多少”，在文本
“天气预报显示今天最高温度 37 摄氏度”
中标注答案的起始位置和终止位置。</p>
<p>输入改写：“[CLS]今天的最高温度是多少[SEP]天气预报显示今天最高温度 37
摄氏度”</p>
<p>BERT Softmax 结果：</p>
<table>
<thead>
<tr class="header">
<th>篇章文本</th>
<th>天气</th>
<th>预报</th>
<th>显示</th>
<th>今天</th>
<th>最高温度</th>
<th>37</th>
<th>摄氏度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>起始位置概率</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.04</td>
<td>0.10</td>
<td>0.80</td>
<td>0.03</td>
</tr>
<tr class="even">
<td>终止位置概率</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.03</td>
<td>0.04</td>
<td>0.10</td>
<td>0.80</td>
</tr>
</tbody>
</table>
<p>对 Softmax 的结果取 <code>arg max</code>，得到答案的起始位置为
6，终止位置为 7，即答案为 “37 摄氏度”。</p>
</blockquote>
<h2 id="单句标注">单句标注</h2>
<p>给定一个句子，标注每个词的标签，称为单句标注。例如给定一个句子，标注句子中的人名、地名和机构名。</p>
<p>单句标注任务和 BERT
预训练任务具有较大差异，但与文本问答任务较为相似。</p>
<p>在进行单句标注任务时，需要在每个词的最终语义特征向量之后添加全连接层，将语义特征转化为序列标注任务所需的特征，单句标注任务需要对每个词都做标注，因此不需要引入辅助向量，直接对经过全连接层后的结果做
Softmax 操作，即可得到各类标签的概率分布。 <img data-src="https://images-a2q.pages.dev/file/589e9a4fcfa426dd6546a.png" /></p>
<p>由于 BERT 需要对输入文本进行分词操作，独立词将会被分成若干子词，因此
BERT 预测的结果将会是 5 类（细分为 13 小类）：</p>
<ul>
<li>O（非人名地名机构名，O 表示 Other）</li>
<li>B-PER/LOC/ORG（人名/地名/机构名初始单词，B 表示 Begin）</li>
<li>I-PER/LOC/ORG（人名/地名/机构名中间单词，I 表示 Intermediate）</li>
<li>E-PER/LOC/ORG（人名/地名/机构名终止单词，E 表示 End）</li>
<li>S-PER/LOC/ORG（人名/地名/机构名独立单词，S 表示 Single）</li>
</ul>
<p>将 5 大类的首字母结合，可得 IOBES，这是序列标注最常用的标注方法。</p>
<p>下面给出命名实体识别（NER）任务的示例：</p>
<blockquote>
<p>任务：给定句子 “爱因斯坦在柏林发表演讲”，根据 IOBES 标注 NER 结果
输入改写：“[CLS]爱 因 斯 坦 在 柏 林 发 表 演 讲” BERT Softmax
结果：</p>
<table>
<thead>
<tr class="header">
<th>BOBES</th>
<th>爱</th>
<th>因</th>
<th>斯坦</th>
<th>在</th>
<th>柏林</th>
<th>发表</th>
<th>演讲</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>O</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.90</td>
<td>0.01</td>
<td>0.90</td>
<td>0.90</td>
</tr>
<tr class="even">
<td>B-PER</td>
<td>0.90</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr class="odd">
<td>I-PER</td>
<td>0.01</td>
<td>0.90</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr class="even">
<td>E-PER</td>
<td>0.01</td>
<td>0.01</td>
<td>0.90</td>
<td>0.01</td>
<td>0.90</td>
<td>0.01</td>
<td>0.01</td>
</tr>
<tr class="odd">
<td>S-LOC</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>对 Softmax 的结果取 arg max，得到最终地 NER 标注结果为：“爱因斯坦”
是人名；“柏林” 是地名</p>
</blockquote>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deepLearning/" rel="tag"><i class="fa fa-tag"></i> #deepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/763a3d0e.html" rel="prev" title="DecisionTree">
                  <i class="fa fa-angle-left"></i> DecisionTree
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/4863b368.html" rel="next" title="损失函数">
                  损失函数 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><span class="exturl" data-url="aHR0cHM6Ly9iZWlhbi5taWl0Lmdvdi5jbg==">浙ICP备 2023011468号-1 </span>
      <img src="https://images-a2q.pages.dev/file/ab4ebf9b9723073c81a21.png" alt="">
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Joey</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">179k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">10:51</span>
  </span>
</div>
  <div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.min.js","integrity":"sha256-G8ouPAnw4zzMbnAenHnVz6h9XpKbNdOkrqTh7AadyHs="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/posts/c0ce19bd.html"}</script>
  <script src="/js/third-party/quicklink.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":false},"react":{"opacity":1},"log":false});</script></body>
</html>
