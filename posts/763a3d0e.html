<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">
<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="32x32" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="icon" type="image/png" sizes="16x16" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <link rel="mask-icon" href="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300italic,400,400italic,700,700italic%7CGrand+Hotel:300,300italic,400,400italic,700,700italic%7CUbuntu:300,300italic,400,400italic,700,700italic%7CJetBrains+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.css" integrity="sha256-6cQIC71/iBIYXFK+0RHAvwmjwWzkWd+r7v/BX3/vZDc=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.1","exturl":true,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Decision Tree Notes">
<meta property="og:type" content="article">
<meta property="og:title" content="DecisionTree">
<meta property="og:url" content="http://example.com/posts/763a3d0e.html">
<meta property="og:site_name" content="Joey">
<meta property="og:description" content="Decision Tree Notes">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://images-a2q.pages.dev/file/f6b70503bdd7370dba5a0.png">
<meta property="og:image" content="https://images-a2q.pages.dev/file/892f242a8d5bd0498a77a.png">
<meta property="article:published_time" content="2024-03-12T11:46:06.000Z">
<meta property="article:modified_time" content="2024-06-30T16:36:59.570Z">
<meta property="article:author" content="Joey">
<meta property="article:tag" content="deepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images-a2q.pages.dev/file/f6b70503bdd7370dba5a0.png">


<link rel="canonical" href="http://example.com/posts/763a3d0e.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/posts/763a3d0e.html","path":"posts/763a3d0e.html","title":"DecisionTree"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DecisionTree | Joey</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Joey</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">6</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">7</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">32</span></a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="nav-text">决策树模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-text">算法原理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A6%99%E5%86%9C%E7%86%B5-entropy"><span class="nav-text">香农熵(entropy)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-text">代码</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E7%86%B5-H-Y-X"><span class="nav-text">条件熵 $H(Y|X)$</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="nav-text">信息增益</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%EF%BC%88ID3%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-text">信息增益（ID3）代码实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0-Gini"><span class="nav-text">基尼指数 (Gini)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-text">决策树的生成</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ID3-%E7%AE%97%E6%B3%95"><span class="nav-text">ID3 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4"><span class="nav-text">具体步骤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#C4-5-%E7%9A%84%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95"><span class="nav-text">C4.5 的生成算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%AF%94"><span class="nav-text">信息增益比</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#D3%E5%92%8CC4-5%E7%AE%97%E6%B3%95%E7%9A%84Python%E5%AE%9E%E7%8E%B0%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="nav-text">D3和C4.5算法的Python实现算法流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B"><span class="nav-text">算法流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ID3%E7%AE%97%E6%B3%95%E7%9A%84Python%E5%AE%9E%E7%8E%B0"><span class="nav-text">ID3算法的Python实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%AA%E6%9E%9D"><span class="nav-text">剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D%E5%92%8C%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="nav-text">预剪枝和后剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="nav-text">预剪枝</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="nav-text">后剪枝</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#CART-%E7%AE%97%E6%B3%95"><span class="nav-text">CART 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CART-%E7%94%9F%E6%88%90"><span class="nav-text">CART 生成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CART-%E5%89%AA%E6%9E%9D"><span class="nav-text">CART 剪枝</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Joey"
      src="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
  <p class="site-author-name" itemprop="name">Joey</p>
  <div class="site-description" itemprop="description">A Humble Apprentice</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hlYWRtYXN0ZXJFZ2d5" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HeadmasterEggy"><i class="fab fa-github fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmhlYWRtYXN0ZXJlZ2d5QGdtYWlsLmNvbQ==" title="E-Mail → mailto:headmastereggy@gmail.com"><i class="fa fa-envelope fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly90d2l0dGVyLmNvbS9IZWFkbWFzdGVyRWdneQ==" title="X → https:&#x2F;&#x2F;twitter.com&#x2F;HeadmasterEggy"><i class="fab fa-x-twitter fa-fw"></i></span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9pbnN0YWdyYW0uY29tL2VnZ3lvbGRnb29zZQ==" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;eggyoldgoose"><i class="fab fa-instagram fa-fw"></i></span>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/posts/763a3d0e.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://images-a2q.pages.dev/file/b89bbe01d52ee39e69cd8.png">
      <meta itemprop="name" content="Joey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Joey">
      <meta itemprop="description" content="A Humble Apprentice">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="DecisionTree | Joey">
      <meta itemprop="description" content="Decision Tree Notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DecisionTree
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-12 19:46:06" itemprop="dateCreated datePublished" datetime="2024-03-12T19:46:06+08:00">2024-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-01 00:36:59" itemprop="dateModified" datetime="2024-07-01T00:36:59+08:00">2024-07-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Word count in article">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Word count in article: </span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="Reading time">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Reading time &asymp;</span>
      <span>15 mins.</span>
    </span>
</div>

            <div class="post-description">Decision Tree Notes</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>#deepLearning&#x2F;decisionTree </p>
<h1 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h1><p>分类决策树模型(Decision Tree)是一种描述对实例进行分类的树形结构，决策树由结点(node)和有向边(directed edge)组成，结点有两种类型:内部结点(internal node)和叶结点(leaf node)，内部结点表示一个特征或属性，叶结点表示一个类。</p>
<p>决策树是一种基本的分类和回归方法。它通过模拟人类决策过程来预测目标变量的值，是一种树形结构的模型，其中每个内部节点代表一个属性上的测试，每个分支代表测试的结果，而每个叶节点代表一个类别（在分类树中）或一个连续值（在回归树中）。决策树的主要优点是模型具有很好的可解释性，即人们可以很容易地理解模型的决策过程。</p>
<h1 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h1><ul>
<li>数据准备 → 通过数据清洗和数据处理，将数据整理为没有缺省值的向量。</li>
<li>寻找最佳特征 → 遍历每个特征的每一种划分方式，找到最好的划分特征。</li>
<li>生成分支 → 划分成两个或多个节点。</li>
<li>生成决策树 → 对分裂后的节点分别继续执行 2-3 步，直到每个节点只有一种类别。</li>
<li>决策分类 → 根据训练决策树模型，将预测数据进行分类。</li>
</ul>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>决定用哪个特征来划分特征空间。<br>通过<strong>信息增益</strong>选取对训练数据具有分类能力的特征。</p>
<h2 id="香农熵-entropy"><a href="#香农熵-entropy" class="headerlink" title="香农熵(entropy)"></a>香农熵(entropy)</h2><p>在信息论与概率统计中，<strong>熵</strong>(entropy)是表示随机变量不确定性的度量。设 $X$ 是一个取有限个值的离散随机变量，其概率分布为<br>$$<br>P(X&#x3D;x_{i})&#x3D;p_{i},\ i&#x3D;1,2, \cdots ,n<br>$$<br>则随机变量 $X$ 的熵定义为</p>
<p>$$<br>H(X) &#x3D; - \sum_{i&#x3D;1}^{n}p_i \log_2{p_i}<br>$$<br>当随机变量只取两个值，例如$1，0$时，即 $X$ 的分布为<br>$$<br>P(X&#x3D;1)&#x3D;p \ , \quad P(X&#x3D;0)&#x3D;1-p \ ,\quad 0\le p\le1<br>$$<br>熵为<br>$$<br>H(p)&#x3D;-p\log_{2}p-(1-p)\log_{2}(1-p)<br>$$<br>这时，熵 $H(p)$ 随概率 $p$ 变化的曲线</p>
<p><img data-src="https://images-a2q.pages.dev/file/f6b70503bdd7370dba5a0.png" alt="分布为贝努利分布时熵与概率的关系"></p>
<ul>
<li>当 $p&#x3D;0$ 或 $p&#x3D;1$ 时 $H(p)&#x3D;0$ ，随机变量完全没有不确定性。</li>
<li>当 $p&#x3D;0.5$ 时 $H(p)&#x3D;1$ ，熵取值最大，随机变量不确定性最大.</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_Ent</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    data -- 数据集</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    Ent -- 信息熵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算信息熵</span></span><br><span class="line">    num_sample = <span class="built_in">len</span>(data)  <span class="comment"># 样本个数</span></span><br><span class="line">    label_counts = &#123;&#125;  <span class="comment"># 初始化标签统计字典</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_sample):</span><br><span class="line">        each_data = data.iloc[i, :]</span><br><span class="line">        current_label = each_data[<span class="string">&quot;labels&quot;</span>]  <span class="comment"># 得到当前元素的标签（label）</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果标签不在当前字典中，添加该类标签并初始化 value=0,否则该类标签 value+1</span></span><br><span class="line">        <span class="keyword">if</span> current_label <span class="keyword">not</span> <span class="keyword">in</span> label_counts.keys():</span><br><span class="line">            label_counts[current_label] = <span class="number">0</span></span><br><span class="line">        label_counts[current_label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    Ent = <span class="number">0.0</span>  <span class="comment"># 初始化信息熵</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> label_counts:</span><br><span class="line">        prob = <span class="built_in">float</span>(label_counts[key])/num_sample</span><br><span class="line">        Ent -= prob * math.log(prob, <span class="number">2</span>)  <span class="comment"># 应用信息熵公式计算信息熵</span></span><br><span class="line">    <span class="keyword">return</span> Ent</span><br></pre></td></tr></table></figure>

<h2 id="条件熵-H-Y-X"><a href="#条件熵-H-Y-X" class="headerlink" title="条件熵 $H(Y|X)$"></a>条件熵 $H(Y|X)$</h2><p>$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的数学期望<br>$$<br>\text{H}(D|A) &#x3D; \sum_{j&#x3D;1}^{n} \frac{|D_j|}{|D|} \text{H}(D_j)<br>$$<br>其中，$n$ 是属性 $A$ 的不同值的数目，$D_j$​ 是D中属性 $A$ 取第 $j$ 个值时的子集，$|D_j​|$ 是 $D_j$​ 的样本数量，$|D|$ 是 $D$ 的样本总数量。</p>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>在划分数据集之前之后信息发生的变化称为<strong>信息增益</strong>，即划分前的信息熵减去划分后的信息熵。</p>
<p>信息增益 $g(D,A)$ 定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D|A)$ 之差，即<br>$$<br>Gain(D,A)&#x3D;H(D)-H(D \mid A)<br>$$</p>
<h2 id="信息增益（ID3）代码实现"><a href="#信息增益（ID3）代码实现" class="headerlink" title="信息增益（ID3）代码实现"></a>信息增益（ID3）代码实现</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_gain</span>(<span class="params">data, base_ent, feature</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    data -- 数据集</span></span><br><span class="line"><span class="string">    base_ent -- 根节点的信息熵</span></span><br><span class="line"><span class="string">    feature -- 计算信息增益的特征</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    Ent -- 信息熵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算信息增益</span></span><br><span class="line">    feature_list = data[feature]  <span class="comment"># 得到一个特征的全部取值</span></span><br><span class="line">    unique_value = <span class="built_in">set</span>(feature_list)  <span class="comment"># 特征取值的类别</span></span><br><span class="line">    feature_ent = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> each_feature <span class="keyword">in</span> unique_value:</span><br><span class="line">        temp_data = data[data[feature] == each_feature]</span><br><span class="line">        weight = <span class="built_in">len</span>(temp_data)/<span class="built_in">len</span>(feature_list)  <span class="comment"># 计算该特征的权重值</span></span><br><span class="line">        temp_ent = weight*get_Ent(temp_data)</span><br><span class="line">        feature_ent = feature_ent+temp_ent</span><br><span class="line"></span><br><span class="line">    gain = base_ent - feature_ent  <span class="comment"># 信息增益</span></span><br><span class="line">    <span class="keyword">return</span> gain</span><br></pre></td></tr></table></figure>


<p>在构建决策树（如C4.5决策树算法）时，信息增益比常被用来选择属性，因为它可以减少因属性值数量大而偏向于选择这些属性的倾向，从而选择更有区分能力的属性。</p>
<p>算法C4.5是算法ID3的改进版，它使用了信息增益和增益比两种选择算法，先选出信息增益高于平均水平的属性，然后再在这些属性中选择增益比最高的，作为最优划分属性。这样综合了信息增益和增益比的优点，可以取得较好的效果。</p>
<h2 id="基尼指数-Gini"><a href="#基尼指数-Gini" class="headerlink" title="基尼指数 (Gini)"></a>基尼指数 (Gini)</h2><p>$$<br>{\mathrm{Gini}}(p)&#x3D;\sum_{k&#x3D;1}^{K}p_{k}(1-p_{k})&#x3D;1-\sum_{k&#x3D;1}^{K}p_{k}^{2}<br>$$<br>基尼指数就是在样本集中随机抽出两个样本不同类别的概率。当样本集越不纯的时候，这个概率也就越大，即基尼指数也越大。</p>
<p>用基尼指数计算信息增益的公式<br>$$<br>\text{Gain}(D, a) &#x3D; \text{Gini}(D) - \sum_{i&#x3D;1}^{n} \left( \frac{|D^i|}{|D|} \text{Gini}(D^i) \right)<br>$$<br>和信息增益计算方式类似，就是使用划分前样本集 $D$ 的基尼指数减去划分后子样本集 $D^i$ 的基尼指数加权和。</p>
<p><img data-src="https://images-a2q.pages.dev/file/892f242a8d5bd0498a77a.png"><br>可以看出，基尼指数与信息熵虽然值不同，但是趋势一致。同样的，使用基尼指数来选择最优划分属性也是对比不同属性划分后基尼指数的差值，选择使样本集基尼指数减小最多的属性</p>
<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="ID3 算法"></a>ID3 算法</h2><p><strong>ID3算法</strong>通过递归地选择具有最高信息增益的属性来构建决策树，直至所有实例属于同一类别或没有更多属性可用。</p>
<p>ID3算法的一个局限性在于它倾向于选择具有更多值的特征，这可能会导致决策树的过分生长，并且对噪声敏感。此外，ID3不直接处理连续特征和缺失值，并且生成的树只能用于分类，而不能用于回归问题。</p>
<h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><p><strong>选择最优特征</strong>：从当前样本集合的特征中，选择信息增益最大的特征作为节点特征，用于分支。<br><strong>分支构造子集</strong>：根据选定的最优特征，把当前样本集分割成若干个子集，每个子集包含了数据集中所有在该特征上具有相同值的样本。<br><strong>递归构造</strong>：对每个子集递归重复以上步骤，构造它们的决策子树，直到所有特征的信息增益均很小或者没有特征可以进行分割为止。</p>
<h2 id="C4-5-的生成算法"><a href="#C4-5-的生成算法" class="headerlink" title="C4.5 的生成算法"></a>C4.5 的生成算法</h2><p>C4.5算法是ID3的后续改进版本，它解决了ID3的一些局限性，包括使用信息增益率来选择特征，以及处理连续特征和缺失值的能力。C4.5还加入了剪枝步骤来减少过拟合问题。</p>
<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><p>特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,4)$ 定义为其信息增益 $g(D,4)$ 与训练数据集 $D$ 的经验熵 $H(D)$ 之比:</p>
<p>$$<br>Gain_{-}ratio(D,a)&#x3D;\frac{Gain(D,a)}{IV(a)}<br>$$<br><strong>计算属性A的固有值（Intrinsic Value）</strong>:<br>固有值衡量的是属性A自身的信息熵，而不是属性A带来的信息增益。<br>$$<br>\text{IV}(A) &#x3D; -\sum_{j&#x3D;1}^{n} \frac{|D_j|}{|D|} \log_2 \frac{|D_j|}{|D|}<br>$$</p>
<p>其中，$n$ 是属性 $A$ 的不同值的数目，$D_j$ 是 $D$ 中属性 $A$ 取第 $j$ 个值时的子集，$|D_j|$ 是 $D_j$ 的样本数量，$|D|$ 是 $D$ 的样本总数量。</p>
<p>增益率对取值数目较少的属性会有所偏好，因此，<code>C4.5</code>算法并不是直接选择增益率最大的候选划分属性，而是是用了一个启发式: <strong>先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</strong>。</p>
<h2 id="D3和C4-5算法的Python实现算法流程"><a href="#D3和C4-5算法的Python实现算法流程" class="headerlink" title="D3和C4.5算法的Python实现算法流程"></a>D3和C4.5算法的Python实现算法流程</h2><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ol>
<li><p>决策树流程：<br>获取训练集 $D$ 和特征集 $A$<br>传入数据集D：计算$D$的经验熵<br>计算个特征对数据集的信息增益（比）<br>比较信息增益，选出最大值对应的特征<br>根据特征将数据集 $D$ 划分为$D1、D2、…、Dn$<br>判断$D_i$是否为一类：<br>    若是，则为一类<br>    若非，计算 $D_i$ 经验熵</p>
</li>
<li><p>信息增益比<br> 循环求和：</p>
</li>
</ol>
<ul>
<li>获取属性B1所在行及长度：计算特征B的属性B1占所有数据比</li>
<li>计算数据集属性B1的信息熵</li>
<li>计算这二者的积，或计算每个A1占比*log2A1占比之差（即特征的值的熵）</li>
<li>特征B对数据集的信息增益&#x3D;原数据集-和，或信息增益比&#x3D;信息增益&#x2F;特征的值的熵</li>
<li>对上面循环求解并比较每个特征对数据集的信息增益（比），求出最大值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet1</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创造示例数据/读取数据</span></span><br><span class="line"><span class="string">    @param dataSet: 数据集</span></span><br><span class="line"><span class="string">    @return dataSet labels：数据集 特征集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 数据集</span></span><br><span class="line">    dataSet = [(<span class="string">&#x27;青年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;一般&#x27;</span>, <span class="string">&#x27;不同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;青年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;不同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;青年&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;青年&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;一般&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;青年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;一般&#x27;</span>, <span class="string">&#x27;不同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;中年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;一般&#x27;</span>, <span class="string">&#x27;不同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;中年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;不同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;中年&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;中年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;非常好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;中年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;非常好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;老年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;非常好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;老年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;老年&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;老年&#x27;</span>, <span class="string">&#x27;是&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;非常好&#x27;</span>, <span class="string">&#x27;同意&#x27;</span>),</span><br><span class="line">               (<span class="string">&#x27;老年&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;否&#x27;</span>, <span class="string">&#x27;一般&#x27;</span>, <span class="string">&#x27;不同意&#x27;</span>)]</span><br><span class="line">    <span class="comment"># 特征集</span></span><br><span class="line">    labels = [<span class="string">&#x27;年龄&#x27;</span>, <span class="string">&#x27;有工作&#x27;</span>, <span class="string">&#x27;有房子&#x27;</span>, <span class="string">&#x27;信贷情况&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dataSet,labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算数据的熵(entropy)</span></span><br><span class="line"><span class="string">    @param dataSet: 数据集</span></span><br><span class="line"><span class="string">    @return shannonEnt: 数据集的熵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    numEntries = <span class="built_in">len</span>(dataSet)  <span class="comment"># 数据条数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 循环判断每个样本的类别，统计每个类别的样本总数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 当前样本类型</span></span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]  </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 统计每个样本类型的数量</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">        	labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">        	labelCounts[currentLabel] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 根据公式计算香浓熵</span></span><br><span class="line">    shannonEnt = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key]) / numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet, index, value</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    划分数据集，提取含有某个特征的某个属性的所有数据</span></span><br><span class="line"><span class="string">    @param dataSet: 数据集</span></span><br><span class="line"><span class="string">    @param index: 属性值所对应的特征列</span></span><br><span class="line"><span class="string">    @param value: 某个属性值</span></span><br><span class="line"><span class="string">    @return retDataSet: 含有某个特征的某个属性的数据集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    retDataSet = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果该样本该特征的属性值等于传入的属性值，则去掉该属性然后放入数据集中</span></span><br><span class="line">        <span class="keyword">if</span> featVec[index] == value:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 去掉该属性的当前样本</span></span><br><span class="line">            reducedFeatVec = featVec[:index] + featVec[index+<span class="number">1</span>:] </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># append向末尾追加一个新元素，新元素在元素中格式不变，如数组作为一个值在元素中存在</span></span><br><span class="line">            retDataSet.append(reducedFeatVec) </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    选择最优特征</span></span><br><span class="line"><span class="string">    @param dataSet: 数据集</span></span><br><span class="line"><span class="string">    @return bestFeature: 最优特征所在列</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 特征总数</span></span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 当只有一个特征时</span></span><br><span class="line">    <span class="keyword">if</span> numFeatures == <span class="number">1</span>:  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 数据集的熵</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 最佳信息增益比</span></span><br><span class="line">    bestInfoGainRatio = <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 最优特征所在列</span></span><br><span class="line">    bestFeature = -<span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures): </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 去重，每个属性值唯一</span></span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义按特征分类后的熵</span></span><br><span class="line">        newEntropy = <span class="number">0</span> </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义特征的值的熵</span></span><br><span class="line">        feaEntropy = <span class="number">0</span> </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 依次计算每个特征的值的熵</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            <span class="comment"># 根据该特征属性值分的类</span></span><br><span class="line">            subDataSet = splitDataSet(dataSet,i,value) </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 参数：原数据、循环次数(当前属性值所在列)、当前属性值</span></span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">            feaEntropy -= prob * log(prob, <span class="number">2</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 信息增益比</span></span><br><span class="line">        infoGainRatio = (baseEntropy - newEntropy) / feaEntropy </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> (infoGainRatio &gt; bestInfoGainRatio):</span><br><span class="line">            bestInfoGainRatio = infoGainRatio</span><br><span class="line">            bestFeature = i</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对最后一个特征分类，出现次数最多的类即为该属性类别，比如：最后分类为2男1女，则判定为男</span></span><br><span class="line"><span class="string">    @param classList: 数据集，也是类别集</span></span><br><span class="line"><span class="string">    @return sortedClassCount[0][0]: 该属性的类别</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算每个类别出现次数</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            classCount[vote] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            classCount[vote] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 出现次数最多的类别在首位</span></span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="literal">True</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对第1个参数，按照参数的第1个域来进行排序（第2个参数），然后反序（第3个参数）</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 该属性的类别</span></span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet,labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对最后一个特征分类，按分类后类别数量排序，比如：最后分类为2同意1不同意，则判定为同意</span></span><br><span class="line"><span class="string">    @param dataSet: 数据集</span></span><br><span class="line"><span class="string">    @param labels: 特征集</span></span><br><span class="line"><span class="string">    @return myTree: 决策树</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取每行数据的最后一个值，即每行数据的类别</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当数据集只有一个类别</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 当数据集只剩一列（即类别），即根据最后一个特征分类</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 其他情况</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 选择最优特征（所在列）</span></span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 最优特征</span></span><br><span class="line">    bestFeatLabel = labels[bestFeat] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从特征集中删除当前最优特征</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat]) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 选出最优特征对应属性的唯一值</span></span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分类结果以字典形式保存</span></span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125; </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 深拷贝，拷贝后的值与原值无关（普通复制为浅拷贝，对原值或拷贝后的值的改变互相影响）</span></span><br><span class="line">        subLabels = labels[:] </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 递归调用创建决策树</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创造示列数据</span></span><br><span class="line">	dataSet, labels = createDataSet1()  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出决策树模型结果</span></span><br><span class="line">	<span class="built_in">print</span>(createTree(dataSet, labels))  </span><br></pre></td></tr></table></figure>

<h2 id="ID3算法的Python实现"><a href="#ID3算法的Python实现" class="headerlink" title="ID3算法的Python实现"></a>ID3算法的Python实现</h2><p>根据上面的分析，将chooseBestFeatureToSplit函数中信息增益比的求解修改为信息增益的求解即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">infoGainRatio= baseEntropy - newEntropy <span class="comment"># 信息增益 修改第92行</span></span><br></pre></td></tr></table></figure>

<p>由于infoGainRatio为信息增益比，infoGain才为信息增益</p>
<h1 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h1><h2 id="预剪枝和后剪枝"><a href="#预剪枝和后剪枝" class="headerlink" title="预剪枝和后剪枝"></a>预剪枝和后剪枝</h2><p>在决策树的构建过程中，特别在数据特征非常多时，为了尽可能正确的划分每一个训练样本，结点的划分就会不停的重复，则一棵决策树的分支就非常多。对于训练集而言，拟合出来的模型是非常完美的。但是，这种完美就使得整体模型的复杂度变高，同时对其他数据集的预测能力下降，也就是我们常说的过拟合使得模型的泛化能力变弱。为了避免过拟合问题的出现，在决策树中最常见的两种方法就是预剪枝和后剪枝。</p>
<h3 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h3><p>预剪枝，顾名思义预先减去枝叶，在构建决策树模型的时候，每一次对数据划分之前进行估计，如果当前节点的划分不能带来决策树泛化的提升，则停止划分并将当前节点标记为叶节点。例如前面构造的决策树，按照决策树的构建原则，通过 height 特征进行划分后 &lt; 172分支中又按照 ear stud 特征值进行继续划分。如果应用预剪枝，则当通过 height 进行特征划分之后，对 &lt; 172分支是否进行 ear stud 特征进行划分时计算划分前后的准确度，如果划分后的更高则按照 ear stud 继续划分，如果更低则停止划分。</p>
<h2 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h2><p>跟预剪枝在构建决策树的过程中判断是否继续特征划分所不同的是，后剪枝在决策树构建好之后对树进行修剪。如果说预剪枝是自顶向下的修剪，那么后剪枝就是自底向上进行修剪。后剪枝将最后的分支节点替换为叶节点，判断是否带来决策树泛化的提升，是则进行修剪，并将该分支节点替换为叶节点，否则不进行修剪。例如在前面构建好决策树之后，&gt;172分支的 voice 特征，将其替换为叶节点如（man），计算替换前后划分准确度，如果替换后准确度更高则进行修剪（用叶节点替换分支节点），否则不修剪。</p>
<h1 id="CART-算法"><a href="#CART-算法" class="headerlink" title="CART 算法"></a>CART 算法</h1><h2 id="CART-生成"><a href="#CART-生成" class="headerlink" title="CART 生成"></a>CART 生成</h2><h2 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h2>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/deepLearning/" rel="tag"><i class="fa fa-tag"></i> deepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/posts/a80d0031.html" rel="prev" title="动态规划">
                  <i class="fa fa-angle-left"></i> 动态规划
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/posts/c0ce19bd.html" rel="next" title="Word2Vec">
                  Word2Vec <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><span class="exturl" data-url="aHR0cHM6Ly9iZWlhbi5taWl0Lmdvdi5jbg==">浙ICP备 2023011468号-1 </span>
      <img src="https://images-a2q.pages.dev/file/ab4ebf9b9723073c81a21.png" alt="">
  </div>
  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Joey</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Word count total: </span>
    <span title="Word count total">143k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Reading time total &asymp;</span>
    <span title="Reading time total">8:39</span>
  </span>
</div>
  <div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl" data-url="aHR0cHM6Ly90aGVtZS1uZXh0LmpzLm9yZw==">NexT.Gemini</span>
  </div>

    </div>
  </footer>

  
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.28/fancybox/fancybox.umd.js" integrity="sha256-ytMJGN3toR+a84u7g7NuHm91VIR06Q41kMWDr2pq7Zo=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.6.1/mermaid.min.js","integrity":"sha256-mm3Re3y7xlvh+yCD+l/Zs1d+PU0AEad93MkWvljfm/s="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>


  <script src="/js/third-party/fancybox.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"http://example.com/posts/763a3d0e.html"}</script>
  <script src="/js/third-party/quicklink.js"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":180,"height":300},"mobile":{"show":false},"react":{"opacity":1},"log":false});</script></body>
</html>
